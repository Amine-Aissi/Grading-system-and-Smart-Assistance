{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\amine\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow as keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.regularizers import l2\n",
    "from keras import layers\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Embedding, SimpleRNN, Dense, Dropout, LSTM, Bidirectional, Input\n",
    "from keras.layers import Attention, GlobalAveragePooling1D\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import stanza\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import BatchNormalization, Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*<h3>Load Data<h3>*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_id</th>\n",
       "      <th>answer</th>\n",
       "      <th>grade</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>عقب وفاة عبد المطلب، تولى ابنه مسؤولية رعاية ا...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>عم الرسول صلى الله عليه وسلم هو أبو لهب.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>عم النبي صلى الله عليه وسلم الذي أعتنى به بعد ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>عم النبي صلى الله عليه وسلم هو أبو طالب.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>عم النبي صلى الله عليه وسلم الذي قام برعاية ال...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8</td>\n",
       "      <td>عم النبي صلى الله عليه وسلم هو أبو طالب بن عبد...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>8</td>\n",
       "      <td>عم النبي صلى الله عليه وسلم الذي كفل بتربية ال...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>العم الذي كفل بتربية النبي صلى الله عليه وسلم ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>عم النبي صلى الله عليه وسلم هو أبو طالب الفضل ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8</td>\n",
       "      <td>الجواب هو: بن عبد المطلب.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   question_id                                             answer  grade\n",
       "0            8  عقب وفاة عبد المطلب، تولى ابنه مسؤولية رعاية ا...      1\n",
       "1            8           عم الرسول صلى الله عليه وسلم هو أبو لهب.      0\n",
       "2            8  عم النبي صلى الله عليه وسلم الذي أعتنى به بعد ...      2\n",
       "3            8           عم النبي صلى الله عليه وسلم هو أبو طالب.      2\n",
       "4            8  عم النبي صلى الله عليه وسلم الذي قام برعاية ال...      0\n",
       "5            8  عم النبي صلى الله عليه وسلم هو أبو طالب بن عبد...      2\n",
       "6            8  عم النبي صلى الله عليه وسلم الذي كفل بتربية ال...      0\n",
       "7            8  العم الذي كفل بتربية النبي صلى الله عليه وسلم ...      2\n",
       "8            8  عم النبي صلى الله عليه وسلم هو أبو طالب الفضل ...      2\n",
       "9            8                          الجواب هو: بن عبد المطلب.      1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read csv file into pandas\n",
    "df = pd.read_csv(\"../datasets/shuffled8.csv\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*<h3>EDA<h3>*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100 entries, 0 to 99\n",
      "Data columns (total 3 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   question_id  100 non-null    int64 \n",
      " 1   answer       100 non-null    object\n",
      " 2   grade        100 non-null    int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 2.5+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "grade\n",
       "0    30\n",
       "1    31\n",
       "2    39\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('grade').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(data=df, x='grade')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*<h3>Data Cleaning<h3>*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('question_id', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     عقب وفاة عبد المطلب، تولى ابنه مسؤولية رعاية ا...\n",
      "1              عم الرسول صلى الله عليه وسلم هو أبو لهب.\n",
      "2     عم النبي صلى الله عليه وسلم الذي أعتنى به بعد ...\n",
      "3              عم النبي صلى الله عليه وسلم هو أبو طالب.\n",
      "4     عم النبي صلى الله عليه وسلم الذي قام برعاية ال...\n",
      "                            ...                        \n",
      "95                                     هو زيد بن حارثة.\n",
      "96                     الجواب هو أبو طالب بن عبد المطلب\n",
      "97    العم أبو طالب قام برعاية النبي صلى الله عليه و...\n",
      "98    عم النبي صلى الله عليه وسلم الذي كفل بتربية ال...\n",
      "99                      هو عمه أبو بكر بن أبي قُهَافَة.\n",
      "Name: answer, Length: 100, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df.drop_duplicates(inplace=True)\n",
    "print(df['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*<h3>Data Pre-Preocessing<h3>*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "610da0d1fb20480cba5ea644b73326a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.7.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-09 22:54:53 INFO: Downloading default packages for language: ar (Arabic) ...\n",
      "2024-01-09 22:54:54 INFO: File exists: C:\\Users\\amine\\stanza_resources\\ar\\default.zip\n",
      "2024-01-09 22:54:59 INFO: Finished downloading models and saved to C:\\Users\\amine\\stanza_resources.\n",
      "2024-01-09 22:54:59 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ffeed87b76d4e6e8a0e41fedec84730",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.7.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-09 22:55:01 INFO: Loading these models for language: ar (Arabic):\n",
      "=============================\n",
      "| Processor | Package       |\n",
      "-----------------------------\n",
      "| tokenize  | padt          |\n",
      "| mwt       | padt          |\n",
      "| pos       | padt_charlm   |\n",
      "| lemma     | padt_nocharlm |\n",
      "| depparse  | padt_charlm   |\n",
      "| ner       | aqmar_charlm  |\n",
      "=============================\n",
      "\n",
      "2024-01-09 22:55:01 INFO: Using device: cpu\n",
      "2024-01-09 22:55:01 INFO: Loading: tokenize\n",
      "2024-01-09 22:55:03 INFO: Loading: mwt\n",
      "2024-01-09 22:55:03 INFO: Loading: pos\n",
      "2024-01-09 22:55:03 INFO: Loading: lemma\n",
      "2024-01-09 22:55:03 INFO: Loading: depparse\n",
      "2024-01-09 22:55:04 INFO: Loading: ner\n",
      "2024-01-09 22:55:05 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "df['grade'] = le.fit_transform(df['grade'])\n",
    "\n",
    "stanza.download('ar')\n",
    "nlp = stanza.Pipeline('ar')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    doc = nlp(text)\n",
    "    tokens = [word.lemma for sent in doc.sentences for word in sent.words if word.upos != 'PUNCT']\n",
    "    return tokens\n",
    "\n",
    "df['answer'] = df['answer'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0 ...  19   2  22]\n",
      " [  0   0   0 ...  12  27  34]\n",
      " [  0   0   0 ...   1  12  15]\n",
      " ...\n",
      " [  0   0   0 ...   1   3  10]\n",
      " [  0   4   2 ...  59  11  47]\n",
      " [  0   0   0 ...  11  32 126]]\n",
      "0     [عَقِبَ, وَفَاة, عبد, مَطلَب, تَوَلَّى, اِبن, ...\n",
      "1     [عَمّ, رَسُول, صَلَّى, الله, عَلَى, هُوَ, وَ, ...\n",
      "2     [عَمّ, نَبِيّ, صَلَّى, الله, عَلَى, هُوَ, وَ, ...\n",
      "3     [عَمّ, نَبِيّ, صَلَّى, الله, عَلَى, هُوَ, وَ, ...\n",
      "4     [عَمّ, نَبِيّ, صَلَّى, الله, عَلَى, هُوَ, وَ, ...\n",
      "                            ...                        \n",
      "95                              [هُوَ, زيد, بِن, حارثة]\n",
      "96          [جَوَاب, هُوَ, أبو, طالب, بِن, عبد, مَطلَب]\n",
      "97    [عَمّ, أبو, طالب, قَام, بِ, رِعَايَة, نَبِيّ, ...\n",
      "98    [عَمّ, نَبِيّ, صَلَّى, الله, عَلَى, هُوَ, وَ, ...\n",
      "99     [هُوَ, عَمّ, هُوَ, أبو, بكر, بِن, أَب, قُهَافَة]\n",
      "Name: answer, Length: 100, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Tokenization\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df['answer'])\n",
    "sequences = tokenizer.texts_to_sequences(df['answer'])\n",
    "max_sequence_length = max(len(s) for s in sequences)\n",
    "sequences = pad_sequences(sequences,max_sequence_length)\n",
    "word2idx = tokenizer.word_index\n",
    "vocab_size = len(word2idx) + 1\n",
    "\n",
    "\n",
    "X = pad_sequences(sequences, padding='post', truncating='post', maxlen=max_sequence_length)\n",
    "\n",
    "print(sequences)\n",
    "print(df['answer'])\n",
    "\n",
    "Y = to_categorical(df['grade'], num_classes=3)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.15, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*<h3>build Models<h3>*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*<h4>RNN Model<h4>*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\amine\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:From c:\\Users\\amine\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\amine\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "2/2 [==============================] - 15s 2s/step - loss: 1.3890 - accuracy: 0.2625 - val_loss: 1.2802 - val_accuracy: 0.3000 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "2/2 [==============================] - 0s 163ms/step - loss: 1.2229 - accuracy: 0.4375 - val_loss: 1.2526 - val_accuracy: 0.3000 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "2/2 [==============================] - 0s 154ms/step - loss: 1.1629 - accuracy: 0.4375 - val_loss: 1.2241 - val_accuracy: 0.3000 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "2/2 [==============================] - 0s 106ms/step - loss: 1.1268 - accuracy: 0.5125 - val_loss: 1.2092 - val_accuracy: 0.3500 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "2/2 [==============================] - 0s 98ms/step - loss: 1.1064 - accuracy: 0.4875 - val_loss: 1.1962 - val_accuracy: 0.3500 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 1.1063 - accuracy: 0.5625 - val_loss: 1.1879 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "2/2 [==============================] - 0s 167ms/step - loss: 1.0406 - accuracy: 0.5625 - val_loss: 1.1916 - val_accuracy: 0.3000 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.9693 - accuracy: 0.7656\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "2/2 [==============================] - 0s 173ms/step - loss: 0.9618 - accuracy: 0.7625 - val_loss: 1.1994 - val_accuracy: 0.3000 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.8574 - accuracy: 0.7750 - val_loss: 1.2012 - val_accuracy: 0.3000 - lr: 2.0000e-04\n",
      "Epoch 10/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.8579 - accuracy: 0.7812\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.0001.\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.8523 - accuracy: 0.7875 - val_loss: 1.2029 - val_accuracy: 0.3000 - lr: 2.0000e-04\n",
      "Epoch 11/100\n",
      "2/2 [==============================] - 0s 366ms/step - loss: 0.8872 - accuracy: 0.7625 - val_loss: 1.2039 - val_accuracy: 0.3000 - lr: 1.0000e-04\n",
      "Epoch 12/100\n",
      "2/2 [==============================] - 1s 504ms/step - loss: 0.8312 - accuracy: 0.8000 - val_loss: 1.2048 - val_accuracy: 0.3000 - lr: 1.0000e-04\n",
      "Epoch 13/100\n",
      "2/2 [==============================] - 0s 224ms/step - loss: 0.9132 - accuracy: 0.6875 - val_loss: 1.2058 - val_accuracy: 0.3000 - lr: 1.0000e-04\n",
      "Epoch 14/100\n",
      "2/2 [==============================] - 0s 295ms/step - loss: 0.9081 - accuracy: 0.7000 - val_loss: 1.2062 - val_accuracy: 0.3000 - lr: 1.0000e-04\n",
      "Epoch 15/100\n",
      "2/2 [==============================] - 0s 296ms/step - loss: 0.8603 - accuracy: 0.7875 - val_loss: 1.2065 - val_accuracy: 0.3000 - lr: 1.0000e-04\n",
      "Epoch 16/100\n",
      "2/2 [==============================] - 0s 234ms/step - loss: 0.8293 - accuracy: 0.7750 - val_loss: 1.2064 - val_accuracy: 0.3000 - lr: 1.0000e-04\n",
      "Epoch 17/100\n",
      "2/2 [==============================] - 0s 286ms/step - loss: 0.8499 - accuracy: 0.7750 - val_loss: 1.2053 - val_accuracy: 0.3000 - lr: 1.0000e-04\n",
      "Epoch 18/100\n",
      "2/2 [==============================] - 1s 804ms/step - loss: 0.7726 - accuracy: 0.8250 - val_loss: 1.2034 - val_accuracy: 0.3000 - lr: 1.0000e-04\n",
      "Epoch 19/100\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.7741 - accuracy: 0.8875 - val_loss: 1.2014 - val_accuracy: 0.3000 - lr: 1.0000e-04\n",
      "Epoch 20/100\n",
      "2/2 [==============================] - 1s 754ms/step - loss: 0.7975 - accuracy: 0.8000 - val_loss: 1.1989 - val_accuracy: 0.3000 - lr: 1.0000e-04\n",
      "Epoch 21/100\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.7646 - accuracy: 0.8625 - val_loss: 1.1962 - val_accuracy: 0.3000 - lr: 1.0000e-04\n",
      "Epoch 22/100\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 0.7788 - accuracy: 0.8500 - val_loss: 1.1938 - val_accuracy: 0.3000 - lr: 1.0000e-04\n",
      "Epoch 23/100\n",
      "2/2 [==============================] - 0s 304ms/step - loss: 0.7642 - accuracy: 0.8000 - val_loss: 1.1917 - val_accuracy: 0.3000 - lr: 1.0000e-04\n",
      "Epoch 24/100\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.7695 - accuracy: 0.8500 - val_loss: 1.1892 - val_accuracy: 0.3000 - lr: 1.0000e-04\n",
      "Epoch 25/100\n",
      "2/2 [==============================] - 0s 178ms/step - loss: 0.7525 - accuracy: 0.8500 - val_loss: 1.1870 - val_accuracy: 0.3000 - lr: 1.0000e-04\n",
      "Epoch 26/100\n",
      "2/2 [==============================] - 0s 155ms/step - loss: 0.7431 - accuracy: 0.8750 - val_loss: 1.1856 - val_accuracy: 0.3000 - lr: 1.0000e-04\n",
      "Epoch 27/100\n",
      "2/2 [==============================] - 0s 153ms/step - loss: 0.7319 - accuracy: 0.8875 - val_loss: 1.1840 - val_accuracy: 0.3000 - lr: 1.0000e-04\n",
      "Epoch 28/100\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 0.7332 - accuracy: 0.8500 - val_loss: 1.1826 - val_accuracy: 0.3000 - lr: 1.0000e-04\n",
      "Epoch 29/100\n",
      "2/2 [==============================] - 0s 147ms/step - loss: 0.6951 - accuracy: 0.9000 - val_loss: 1.1816 - val_accuracy: 0.3000 - lr: 1.0000e-04\n",
      "Epoch 30/100\n",
      "2/2 [==============================] - 0s 350ms/step - loss: 0.7152 - accuracy: 0.8875 - val_loss: 1.1796 - val_accuracy: 0.3000 - lr: 1.0000e-04\n",
      "Epoch 31/100\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.6807 - accuracy: 0.9125 - val_loss: 1.1774 - val_accuracy: 0.3000 - lr: 1.0000e-04\n",
      "Epoch 32/100\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.6679 - accuracy: 0.8875 - val_loss: 1.1757 - val_accuracy: 0.3000 - lr: 1.0000e-04\n",
      "Epoch 33/100\n",
      "2/2 [==============================] - 0s 152ms/step - loss: 0.6912 - accuracy: 0.8750 - val_loss: 1.1739 - val_accuracy: 0.3000 - lr: 1.0000e-04\n",
      "Epoch 34/100\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 0.6851 - accuracy: 0.8875 - val_loss: 1.1713 - val_accuracy: 0.3000 - lr: 1.0000e-04\n",
      "Epoch 35/100\n",
      "2/2 [==============================] - 0s 259ms/step - loss: 0.6170 - accuracy: 0.9375 - val_loss: 1.1688 - val_accuracy: 0.3000 - lr: 1.0000e-04\n",
      "Epoch 36/100\n",
      "2/2 [==============================] - 0s 130ms/step - loss: 0.6615 - accuracy: 0.9250 - val_loss: 1.1663 - val_accuracy: 0.3000 - lr: 1.0000e-04\n",
      "Epoch 37/100\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.6676 - accuracy: 0.9000 - val_loss: 1.1634 - val_accuracy: 0.3000 - lr: 1.0000e-04\n",
      "Epoch 38/100\n",
      "2/2 [==============================] - 0s 151ms/step - loss: 0.5970 - accuracy: 0.9625 - val_loss: 1.1606 - val_accuracy: 0.3000 - lr: 1.0000e-04\n",
      "Epoch 39/100\n",
      "2/2 [==============================] - 0s 190ms/step - loss: 0.6093 - accuracy: 0.9000 - val_loss: 1.1576 - val_accuracy: 0.3000 - lr: 1.0000e-04\n",
      "Epoch 40/100\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.6035 - accuracy: 0.9250 - val_loss: 1.1545 - val_accuracy: 0.3000 - lr: 1.0000e-04\n",
      "Epoch 41/100\n",
      "2/2 [==============================] - 0s 153ms/step - loss: 0.5790 - accuracy: 0.9375 - val_loss: 1.1515 - val_accuracy: 0.3000 - lr: 1.0000e-04\n",
      "Epoch 42/100\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.5880 - accuracy: 0.9500 - val_loss: 1.1492 - val_accuracy: 0.3000 - lr: 1.0000e-04\n",
      "Epoch 43/100\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.5771 - accuracy: 0.9625 - val_loss: 1.1479 - val_accuracy: 0.3000 - lr: 1.0000e-04\n",
      "Epoch 44/100\n",
      "2/2 [==============================] - 0s 204ms/step - loss: 0.6151 - accuracy: 0.9125 - val_loss: 1.1460 - val_accuracy: 0.3000 - lr: 1.0000e-04\n",
      "Epoch 45/100\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 0.5645 - accuracy: 0.9750 - val_loss: 1.1436 - val_accuracy: 0.3000 - lr: 1.0000e-04\n",
      "Epoch 46/100\n",
      "2/2 [==============================] - 0s 189ms/step - loss: 0.5939 - accuracy: 0.9875 - val_loss: 1.1415 - val_accuracy: 0.3000 - lr: 1.0000e-04\n",
      "Epoch 47/100\n",
      "2/2 [==============================] - 0s 294ms/step - loss: 0.5434 - accuracy: 0.9625 - val_loss: 1.1397 - val_accuracy: 0.3000 - lr: 1.0000e-04\n",
      "Epoch 48/100\n",
      "2/2 [==============================] - 0s 224ms/step - loss: 0.5509 - accuracy: 0.9375 - val_loss: 1.1384 - val_accuracy: 0.3000 - lr: 1.0000e-04\n",
      "Epoch 49/100\n",
      "2/2 [==============================] - 0s 207ms/step - loss: 0.5246 - accuracy: 0.9875 - val_loss: 1.1371 - val_accuracy: 0.3000 - lr: 1.0000e-04\n",
      "Epoch 50/100\n",
      "2/2 [==============================] - 0s 156ms/step - loss: 0.5367 - accuracy: 0.9875 - val_loss: 1.1355 - val_accuracy: 0.3000 - lr: 1.0000e-04\n",
      "Epoch 51/100\n",
      "2/2 [==============================] - 0s 206ms/step - loss: 0.5017 - accuracy: 0.9625 - val_loss: 1.1337 - val_accuracy: 0.3000 - lr: 1.0000e-04\n",
      "Epoch 52/100\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.5120 - accuracy: 0.9500 - val_loss: 1.1319 - val_accuracy: 0.3000 - lr: 1.0000e-04\n",
      "Epoch 53/100\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.4977 - accuracy: 0.9750 - val_loss: 1.1302 - val_accuracy: 0.3000 - lr: 1.0000e-04\n",
      "Epoch 54/100\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.4749 - accuracy: 1.0000 - val_loss: 1.1293 - val_accuracy: 0.3000 - lr: 1.0000e-04\n",
      "Epoch 55/100\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 0.4832 - accuracy: 0.9750 - val_loss: 1.1282 - val_accuracy: 0.3000 - lr: 1.0000e-04\n",
      "Epoch 56/100\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 0.4767 - accuracy: 0.9750 - val_loss: 1.1262 - val_accuracy: 0.3000 - lr: 1.0000e-04\n",
      "Epoch 57/100\n",
      "2/2 [==============================] - 0s 107ms/step - loss: 0.5085 - accuracy: 0.9625 - val_loss: 1.1234 - val_accuracy: 0.3000 - lr: 1.0000e-04\n",
      "Epoch 58/100\n",
      "2/2 [==============================] - 0s 256ms/step - loss: 0.4699 - accuracy: 0.9875 - val_loss: 1.1204 - val_accuracy: 0.3000 - lr: 1.0000e-04\n",
      "Epoch 59/100\n",
      "2/2 [==============================] - 0s 124ms/step - loss: 0.4353 - accuracy: 1.0000 - val_loss: 1.1186 - val_accuracy: 0.3000 - lr: 1.0000e-04\n",
      "Epoch 60/100\n",
      "2/2 [==============================] - 0s 382ms/step - loss: 0.4447 - accuracy: 1.0000 - val_loss: 1.1170 - val_accuracy: 0.3000 - lr: 1.0000e-04\n",
      "Epoch 61/100\n",
      "2/2 [==============================] - 0s 426ms/step - loss: 0.4510 - accuracy: 1.0000 - val_loss: 1.1138 - val_accuracy: 0.3000 - lr: 1.0000e-04\n",
      "Epoch 62/100\n",
      "2/2 [==============================] - 0s 177ms/step - loss: 0.4291 - accuracy: 1.0000 - val_loss: 1.1090 - val_accuracy: 0.3000 - lr: 1.0000e-04\n",
      "Epoch 63/100\n",
      "2/2 [==============================] - 0s 177ms/step - loss: 0.4340 - accuracy: 1.0000 - val_loss: 1.1037 - val_accuracy: 0.3000 - lr: 1.0000e-04\n",
      "Epoch 64/100\n",
      "2/2 [==============================] - 0s 296ms/step - loss: 0.4353 - accuracy: 0.9875 - val_loss: 1.0976 - val_accuracy: 0.3000 - lr: 1.0000e-04\n",
      "Epoch 65/100\n",
      "2/2 [==============================] - 0s 240ms/step - loss: 0.4100 - accuracy: 1.0000 - val_loss: 1.0907 - val_accuracy: 0.3000 - lr: 1.0000e-04\n",
      "Epoch 66/100\n",
      "2/2 [==============================] - 0s 429ms/step - loss: 0.4216 - accuracy: 0.9875 - val_loss: 1.0846 - val_accuracy: 0.3000 - lr: 1.0000e-04\n",
      "Epoch 67/100\n",
      "2/2 [==============================] - 0s 169ms/step - loss: 0.4246 - accuracy: 0.9875 - val_loss: 1.0799 - val_accuracy: 0.3000 - lr: 1.0000e-04\n",
      "Epoch 68/100\n",
      "2/2 [==============================] - 0s 119ms/step - loss: 0.3997 - accuracy: 1.0000 - val_loss: 1.0767 - val_accuracy: 0.3000 - lr: 1.0000e-04\n",
      "Epoch 69/100\n",
      "2/2 [==============================] - 0s 380ms/step - loss: 0.4191 - accuracy: 0.9875 - val_loss: 1.0737 - val_accuracy: 0.3000 - lr: 1.0000e-04\n",
      "Epoch 70/100\n",
      "2/2 [==============================] - 0s 330ms/step - loss: 0.3769 - accuracy: 0.9875 - val_loss: 1.0690 - val_accuracy: 0.3000 - lr: 1.0000e-04\n",
      "Epoch 71/100\n",
      "2/2 [==============================] - 0s 214ms/step - loss: 0.3891 - accuracy: 1.0000 - val_loss: 1.0629 - val_accuracy: 0.3500 - lr: 1.0000e-04\n",
      "Epoch 72/100\n",
      "2/2 [==============================] - 0s 113ms/step - loss: 0.3817 - accuracy: 1.0000 - val_loss: 1.0557 - val_accuracy: 0.4500 - lr: 1.0000e-04\n",
      "Epoch 73/100\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 0.3925 - accuracy: 1.0000 - val_loss: 1.0490 - val_accuracy: 0.4500 - lr: 1.0000e-04\n",
      "Epoch 74/100\n",
      "2/2 [==============================] - 0s 112ms/step - loss: 0.3798 - accuracy: 0.9875 - val_loss: 1.0449 - val_accuracy: 0.4500 - lr: 1.0000e-04\n",
      "Epoch 75/100\n",
      "2/2 [==============================] - 0s 256ms/step - loss: 0.3609 - accuracy: 1.0000 - val_loss: 1.0439 - val_accuracy: 0.4500 - lr: 1.0000e-04\n",
      "Epoch 76/100\n",
      "2/2 [==============================] - 0s 147ms/step - loss: 0.3661 - accuracy: 1.0000 - val_loss: 1.0444 - val_accuracy: 0.4500 - lr: 1.0000e-04\n",
      "Epoch 77/100\n",
      "2/2 [==============================] - 0s 209ms/step - loss: 0.3558 - accuracy: 1.0000 - val_loss: 1.0420 - val_accuracy: 0.4500 - lr: 1.0000e-04\n",
      "Epoch 78/100\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.3521 - accuracy: 1.0000 - val_loss: 1.0361 - val_accuracy: 0.4500 - lr: 1.0000e-04\n",
      "Epoch 79/100\n",
      "2/2 [==============================] - 0s 171ms/step - loss: 0.3226 - accuracy: 1.0000 - val_loss: 1.0282 - val_accuracy: 0.5000 - lr: 1.0000e-04\n",
      "Epoch 80/100\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 0.3346 - accuracy: 1.0000 - val_loss: 1.0207 - val_accuracy: 0.5500 - lr: 1.0000e-04\n",
      "Epoch 81/100\n",
      "2/2 [==============================] - 0s 229ms/step - loss: 0.3430 - accuracy: 1.0000 - val_loss: 1.0157 - val_accuracy: 0.5500 - lr: 1.0000e-04\n",
      "Epoch 82/100\n",
      "2/2 [==============================] - 0s 147ms/step - loss: 0.3391 - accuracy: 1.0000 - val_loss: 1.0135 - val_accuracy: 0.5500 - lr: 1.0000e-04\n",
      "Epoch 83/100\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.3067 - accuracy: 1.0000 - val_loss: 1.0101 - val_accuracy: 0.5500 - lr: 1.0000e-04\n",
      "Epoch 84/100\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.3366 - accuracy: 1.0000 - val_loss: 1.0044 - val_accuracy: 0.5500 - lr: 1.0000e-04\n",
      "Epoch 85/100\n",
      "2/2 [==============================] - 0s 119ms/step - loss: 0.3379 - accuracy: 1.0000 - val_loss: 0.9992 - val_accuracy: 0.5500 - lr: 1.0000e-04\n",
      "Epoch 86/100\n",
      "2/2 [==============================] - 0s 148ms/step - loss: 0.3058 - accuracy: 1.0000 - val_loss: 0.9958 - val_accuracy: 0.5500 - lr: 1.0000e-04\n",
      "Epoch 87/100\n",
      "2/2 [==============================] - 0s 223ms/step - loss: 0.3025 - accuracy: 1.0000 - val_loss: 0.9924 - val_accuracy: 0.5500 - lr: 1.0000e-04\n",
      "Epoch 88/100\n",
      "2/2 [==============================] - 0s 238ms/step - loss: 0.2933 - accuracy: 1.0000 - val_loss: 0.9871 - val_accuracy: 0.5500 - lr: 1.0000e-04\n",
      "Epoch 89/100\n",
      "2/2 [==============================] - 0s 116ms/step - loss: 0.3050 - accuracy: 1.0000 - val_loss: 0.9797 - val_accuracy: 0.5500 - lr: 1.0000e-04\n",
      "Epoch 90/100\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.2917 - accuracy: 1.0000 - val_loss: 0.9751 - val_accuracy: 0.6000 - lr: 1.0000e-04\n",
      "Epoch 91/100\n",
      "2/2 [==============================] - 0s 117ms/step - loss: 0.3050 - accuracy: 1.0000 - val_loss: 0.9741 - val_accuracy: 0.5500 - lr: 1.0000e-04\n",
      "Epoch 92/100\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.2908 - accuracy: 1.0000 - val_loss: 0.9732 - val_accuracy: 0.5500 - lr: 1.0000e-04\n",
      "Epoch 93/100\n",
      "2/2 [==============================] - 0s 296ms/step - loss: 0.2957 - accuracy: 1.0000 - val_loss: 0.9676 - val_accuracy: 0.5500 - lr: 1.0000e-04\n",
      "Epoch 94/100\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.2925 - accuracy: 1.0000 - val_loss: 0.9582 - val_accuracy: 0.6500 - lr: 1.0000e-04\n",
      "Epoch 95/100\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 0.2759 - accuracy: 1.0000 - val_loss: 0.9490 - val_accuracy: 0.6500 - lr: 1.0000e-04\n",
      "Epoch 96/100\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 0.2850 - accuracy: 1.0000 - val_loss: 0.9457 - val_accuracy: 0.6500 - lr: 1.0000e-04\n",
      "Epoch 97/100\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 0.2749 - accuracy: 1.0000 - val_loss: 0.9465 - val_accuracy: 0.6500 - lr: 1.0000e-04\n",
      "Epoch 98/100\n",
      "2/2 [==============================] - 0s 104ms/step - loss: 0.2703 - accuracy: 1.0000 - val_loss: 0.9443 - val_accuracy: 0.6500 - lr: 1.0000e-04\n",
      "Epoch 99/100\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 0.2860 - accuracy: 1.0000 - val_loss: 0.9364 - val_accuracy: 0.7000 - lr: 1.0000e-04\n",
      "Epoch 100/100\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 0.2568 - accuracy: 1.0000 - val_loss: 0.9287 - val_accuracy: 0.7000 - lr: 1.0000e-04\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 0.7529 - accuracy: 0.9333\n",
      "Evaluation Metrics for RNN:\n",
      "loss: 0.7528685331344604\n",
      "accuracy: 0.9333333373069763\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "\n",
    "X_train_rnn, X_test_rnn, y_train_rnn, y_test_rnn = train_test_split(X, Y, test_size=0.2)\n",
    "\n",
    "def RNN_model(vocab_size, sequence_length):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=vocab_size, output_dim=128, input_length=sequence_length))\n",
    "    model.add(SimpleRNN(units=128, activation='relu', return_sequences=True))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(SimpleRNN(units=64, activation='sigmoid'))\n",
    "    model.add(Dense(256, activation='sigmoid', kernel_regularizer=l2(0.001)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "    optimizer = Adam(learning_rate=0.001)\n",
    "    \n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=0.0001, verbose=1)\n",
    "#EMBEDDING_DIM = 110\n",
    "rnn_model = RNN_model(vocab_size, max_sequence_length)\n",
    "early_stopping_rnn = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "history_updated_rnn = rnn_model.fit(X_train_rnn, y_train_rnn, validation_data=(X_test_rnn, y_test_rnn), epochs=100, batch_size=64, callbacks=[reduce_lr])\n",
    "\n",
    "# Evaluate the RNN model\n",
    "evaluation_metrics_updated_rnn = rnn_model.evaluate(X_test, Y_test)\n",
    "print(\"Evaluation Metrics for RNN:\")\n",
    "for metric_name, metric_value in zip(rnn_model.metrics_names, evaluation_metrics_updated_rnn):\n",
    "    print(f\"{metric_name}: {metric_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*<h4>LSTM Model<h4>*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2/2 [==============================] - 12s 1s/step - loss: 1.1971 - accuracy: 0.4000 - val_loss: 1.1994 - val_accuracy: 0.3000\n",
      "Epoch 2/100\n",
      "2/2 [==============================] - 0s 108ms/step - loss: 1.1432 - accuracy: 0.4875 - val_loss: 1.1976 - val_accuracy: 0.3000\n",
      "Epoch 3/100\n",
      "2/2 [==============================] - 0s 103ms/step - loss: 1.0872 - accuracy: 0.5250 - val_loss: 1.1955 - val_accuracy: 0.3000\n",
      "Epoch 4/100\n",
      "2/2 [==============================] - 0s 114ms/step - loss: 1.0400 - accuracy: 0.5125 - val_loss: 1.1920 - val_accuracy: 0.3000\n",
      "Epoch 5/100\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.9463 - accuracy: 0.6875 - val_loss: 1.1878 - val_accuracy: 0.5500\n",
      "Epoch 6/100\n",
      "2/2 [==============================] - 0s 95ms/step - loss: 0.8102 - accuracy: 0.8500 - val_loss: 1.1828 - val_accuracy: 0.6000\n",
      "Epoch 7/100\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 0.6476 - accuracy: 0.8500 - val_loss: 1.1763 - val_accuracy: 0.5500\n",
      "Epoch 8/100\n",
      "2/2 [==============================] - 0s 112ms/step - loss: 0.4845 - accuracy: 0.8750 - val_loss: 1.1684 - val_accuracy: 0.4500\n",
      "Epoch 9/100\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.5372 - accuracy: 0.9000 - val_loss: 1.1632 - val_accuracy: 0.4000\n",
      "Epoch 10/100\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.3733 - accuracy: 0.9125 - val_loss: 1.1570 - val_accuracy: 0.4000\n",
      "Epoch 11/100\n",
      "2/2 [==============================] - 0s 107ms/step - loss: 0.2751 - accuracy: 0.9125 - val_loss: 1.1496 - val_accuracy: 0.5000\n",
      "Epoch 12/100\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.2409 - accuracy: 0.9500 - val_loss: 1.1408 - val_accuracy: 0.6000\n",
      "Epoch 13/100\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 0.2872 - accuracy: 0.9625 - val_loss: 1.1361 - val_accuracy: 0.5500\n",
      "Epoch 14/100\n",
      "2/2 [==============================] - 0s 104ms/step - loss: 0.1840 - accuracy: 0.9750 - val_loss: 1.1350 - val_accuracy: 0.4000\n",
      "Epoch 15/100\n",
      "2/2 [==============================] - 0s 113ms/step - loss: 0.2377 - accuracy: 0.9500 - val_loss: 1.1293 - val_accuracy: 0.5500\n",
      "Epoch 16/100\n",
      "2/2 [==============================] - 0s 131ms/step - loss: 0.2129 - accuracy: 0.9375 - val_loss: 1.1245 - val_accuracy: 0.5500\n",
      "Epoch 17/100\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.2054 - accuracy: 0.9500 - val_loss: 1.1204 - val_accuracy: 0.5000\n",
      "Epoch 18/100\n",
      "2/2 [==============================] - 0s 95ms/step - loss: 0.1936 - accuracy: 0.9500 - val_loss: 1.1160 - val_accuracy: 0.5500\n",
      "Epoch 19/100\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 0.1385 - accuracy: 0.9875 - val_loss: 1.1129 - val_accuracy: 0.7000\n",
      "Epoch 20/100\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 0.1362 - accuracy: 0.9875 - val_loss: 1.1096 - val_accuracy: 0.6500\n",
      "Epoch 21/100\n",
      "2/2 [==============================] - 0s 121ms/step - loss: 0.1445 - accuracy: 0.9625 - val_loss: 1.1062 - val_accuracy: 0.6500\n",
      "Epoch 22/100\n",
      "2/2 [==============================] - 0s 98ms/step - loss: 0.1381 - accuracy: 0.9750 - val_loss: 1.1022 - val_accuracy: 0.6500\n",
      "Epoch 23/100\n",
      "2/2 [==============================] - 0s 230ms/step - loss: 0.1404 - accuracy: 0.9500 - val_loss: 1.0985 - val_accuracy: 0.6500\n",
      "Epoch 24/100\n",
      "2/2 [==============================] - 0s 236ms/step - loss: 0.1213 - accuracy: 0.9750 - val_loss: 1.0950 - val_accuracy: 0.6500\n",
      "Epoch 25/100\n",
      "2/2 [==============================] - 1s 461ms/step - loss: 0.1118 - accuracy: 0.9875 - val_loss: 1.0915 - val_accuracy: 0.6000\n",
      "Epoch 26/100\n",
      "2/2 [==============================] - 0s 307ms/step - loss: 0.1030 - accuracy: 0.9875 - val_loss: 1.0883 - val_accuracy: 0.5500\n",
      "Epoch 27/100\n",
      "2/2 [==============================] - 0s 268ms/step - loss: 0.1010 - accuracy: 0.9875 - val_loss: 1.0846 - val_accuracy: 0.5500\n",
      "Epoch 28/100\n",
      "2/2 [==============================] - 0s 289ms/step - loss: 0.0930 - accuracy: 0.9875 - val_loss: 1.0805 - val_accuracy: 0.5500\n",
      "Epoch 29/100\n",
      "2/2 [==============================] - 0s 242ms/step - loss: 0.0885 - accuracy: 1.0000 - val_loss: 1.0762 - val_accuracy: 0.5500\n",
      "Epoch 30/100\n",
      "2/2 [==============================] - 0s 421ms/step - loss: 0.0876 - accuracy: 1.0000 - val_loss: 1.0714 - val_accuracy: 0.6000\n",
      "Epoch 31/100\n",
      "2/2 [==============================] - 1s 411ms/step - loss: 0.0846 - accuracy: 1.0000 - val_loss: 1.0668 - val_accuracy: 0.6000\n",
      "Epoch 32/100\n",
      "2/2 [==============================] - 0s 126ms/step - loss: 0.0784 - accuracy: 1.0000 - val_loss: 1.0620 - val_accuracy: 0.6500\n",
      "Epoch 33/100\n",
      "2/2 [==============================] - 1s 482ms/step - loss: 0.0768 - accuracy: 1.0000 - val_loss: 1.0568 - val_accuracy: 0.6500\n",
      "Epoch 34/100\n",
      "2/2 [==============================] - 0s 278ms/step - loss: 0.0736 - accuracy: 1.0000 - val_loss: 1.0521 - val_accuracy: 0.6500\n",
      "Epoch 35/100\n",
      "2/2 [==============================] - 0s 112ms/step - loss: 0.0719 - accuracy: 1.0000 - val_loss: 1.0478 - val_accuracy: 0.6500\n",
      "Epoch 36/100\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 0.0710 - accuracy: 1.0000 - val_loss: 1.0438 - val_accuracy: 0.6500\n",
      "Epoch 37/100\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.0704 - accuracy: 1.0000 - val_loss: 1.0403 - val_accuracy: 0.6500\n",
      "Epoch 38/100\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.0695 - accuracy: 1.0000 - val_loss: 1.0372 - val_accuracy: 0.6000\n",
      "Epoch 39/100\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.0706 - accuracy: 1.0000 - val_loss: 1.0342 - val_accuracy: 0.6000\n",
      "Epoch 40/100\n",
      "2/2 [==============================] - 0s 105ms/step - loss: 0.0673 - accuracy: 1.0000 - val_loss: 1.0309 - val_accuracy: 0.6000\n",
      "Epoch 41/100\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 0.0657 - accuracy: 1.0000 - val_loss: 1.0275 - val_accuracy: 0.6000\n",
      "Epoch 42/100\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 0.0650 - accuracy: 1.0000 - val_loss: 1.0241 - val_accuracy: 0.6000\n",
      "Epoch 43/100\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 0.0639 - accuracy: 1.0000 - val_loss: 1.0208 - val_accuracy: 0.6000\n",
      "Epoch 44/100\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.0634 - accuracy: 1.0000 - val_loss: 1.0176 - val_accuracy: 0.6000\n",
      "Epoch 45/100\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.0630 - accuracy: 1.0000 - val_loss: 1.0143 - val_accuracy: 0.6000\n",
      "Epoch 46/100\n",
      "2/2 [==============================] - 0s 98ms/step - loss: 0.0620 - accuracy: 1.0000 - val_loss: 1.0109 - val_accuracy: 0.6000\n",
      "Epoch 47/100\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.0615 - accuracy: 1.0000 - val_loss: 1.0076 - val_accuracy: 0.6500\n",
      "Epoch 48/100\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.0606 - accuracy: 1.0000 - val_loss: 1.0045 - val_accuracy: 0.6500\n",
      "Epoch 49/100\n",
      "2/2 [==============================] - 0s 154ms/step - loss: 0.0606 - accuracy: 1.0000 - val_loss: 1.0014 - val_accuracy: 0.6500\n",
      "Epoch 50/100\n",
      "2/2 [==============================] - 0s 308ms/step - loss: 0.0596 - accuracy: 1.0000 - val_loss: 0.9983 - val_accuracy: 0.6500\n",
      "Epoch 51/100\n",
      "2/2 [==============================] - 0s 177ms/step - loss: 0.0591 - accuracy: 1.0000 - val_loss: 0.9952 - val_accuracy: 0.6500\n",
      "Epoch 52/100\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 0.0585 - accuracy: 1.0000 - val_loss: 0.9919 - val_accuracy: 0.7000\n",
      "Epoch 53/100\n",
      "2/2 [==============================] - 0s 124ms/step - loss: 0.0584 - accuracy: 1.0000 - val_loss: 0.9885 - val_accuracy: 0.7000\n",
      "Epoch 54/100\n",
      "2/2 [==============================] - 0s 299ms/step - loss: 0.0574 - accuracy: 1.0000 - val_loss: 0.9848 - val_accuracy: 0.7000\n",
      "Epoch 55/100\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.0570 - accuracy: 1.0000 - val_loss: 0.9810 - val_accuracy: 0.7000\n",
      "Epoch 56/100\n",
      "2/2 [==============================] - 0s 111ms/step - loss: 0.0561 - accuracy: 1.0000 - val_loss: 0.9770 - val_accuracy: 0.7000\n",
      "Epoch 57/100\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.0555 - accuracy: 1.0000 - val_loss: 0.9733 - val_accuracy: 0.7000\n",
      "Epoch 58/100\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.0550 - accuracy: 1.0000 - val_loss: 0.9698 - val_accuracy: 0.7000\n",
      "Epoch 59/100\n",
      "2/2 [==============================] - 0s 259ms/step - loss: 0.0545 - accuracy: 1.0000 - val_loss: 0.9666 - val_accuracy: 0.7000\n",
      "Epoch 60/100\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 0.0542 - accuracy: 1.0000 - val_loss: 0.9636 - val_accuracy: 0.7000\n",
      "Epoch 61/100\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.0535 - accuracy: 1.0000 - val_loss: 0.9607 - val_accuracy: 0.7000\n",
      "Epoch 62/100\n",
      "2/2 [==============================] - 0s 112ms/step - loss: 0.0531 - accuracy: 1.0000 - val_loss: 0.9579 - val_accuracy: 0.7000\n",
      "Epoch 63/100\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.0526 - accuracy: 1.0000 - val_loss: 0.9552 - val_accuracy: 0.7000\n",
      "Epoch 64/100\n",
      "2/2 [==============================] - 0s 179ms/step - loss: 0.0522 - accuracy: 1.0000 - val_loss: 0.9527 - val_accuracy: 0.7000\n",
      "Epoch 65/100\n",
      "2/2 [==============================] - 0s 217ms/step - loss: 0.0528 - accuracy: 1.0000 - val_loss: 0.9515 - val_accuracy: 0.7000\n",
      "Epoch 66/100\n",
      "2/2 [==============================] - 0s 205ms/step - loss: 0.0512 - accuracy: 1.0000 - val_loss: 0.9518 - val_accuracy: 0.7000\n",
      "Epoch 67/100\n",
      "2/2 [==============================] - 0s 170ms/step - loss: 0.0508 - accuracy: 1.0000 - val_loss: 0.9518 - val_accuracy: 0.7000\n",
      "Epoch 68/100\n",
      "2/2 [==============================] - 0s 122ms/step - loss: 0.0517 - accuracy: 1.0000 - val_loss: 0.9468 - val_accuracy: 0.7000\n",
      "Epoch 69/100\n",
      "2/2 [==============================] - 0s 324ms/step - loss: 0.0500 - accuracy: 1.0000 - val_loss: 0.9416 - val_accuracy: 0.7500\n",
      "Epoch 70/100\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.0498 - accuracy: 1.0000 - val_loss: 0.9390 - val_accuracy: 0.8000\n",
      "Epoch 71/100\n",
      "2/2 [==============================] - 0s 116ms/step - loss: 0.0521 - accuracy: 1.0000 - val_loss: 0.9368 - val_accuracy: 0.7500\n",
      "Epoch 72/100\n",
      "2/2 [==============================] - 0s 106ms/step - loss: 0.0504 - accuracy: 1.0000 - val_loss: 0.9345 - val_accuracy: 0.7500\n",
      "Epoch 73/100\n",
      "2/2 [==============================] - 0s 113ms/step - loss: 0.0500 - accuracy: 1.0000 - val_loss: 0.9317 - val_accuracy: 0.7500\n",
      "Epoch 74/100\n",
      "2/2 [==============================] - 0s 115ms/step - loss: 0.0481 - accuracy: 1.0000 - val_loss: 0.9291 - val_accuracy: 0.7500\n",
      "Epoch 75/100\n",
      "2/2 [==============================] - 0s 106ms/step - loss: 0.0485 - accuracy: 1.0000 - val_loss: 0.9271 - val_accuracy: 0.7000\n",
      "Epoch 76/100\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.0504 - accuracy: 1.0000 - val_loss: 0.9251 - val_accuracy: 0.7000\n",
      "Epoch 77/100\n",
      "2/2 [==============================] - 0s 151ms/step - loss: 0.0497 - accuracy: 1.0000 - val_loss: 0.9223 - val_accuracy: 0.7000\n",
      "Epoch 78/100\n",
      "2/2 [==============================] - 0s 125ms/step - loss: 0.0470 - accuracy: 1.0000 - val_loss: 0.9190 - val_accuracy: 0.7000\n",
      "Epoch 79/100\n",
      "2/2 [==============================] - 0s 130ms/step - loss: 0.0468 - accuracy: 1.0000 - val_loss: 0.9150 - val_accuracy: 0.7000\n",
      "Epoch 80/100\n",
      "2/2 [==============================] - 0s 126ms/step - loss: 0.0460 - accuracy: 1.0000 - val_loss: 0.9109 - val_accuracy: 0.7000\n",
      "Epoch 81/100\n",
      "2/2 [==============================] - 0s 149ms/step - loss: 0.0607 - accuracy: 0.9875 - val_loss: 0.8781 - val_accuracy: 0.8500\n",
      "Epoch 82/100\n",
      "2/2 [==============================] - 0s 108ms/step - loss: 0.1744 - accuracy: 0.9750 - val_loss: 0.9109 - val_accuracy: 0.6500\n",
      "Epoch 83/100\n",
      "2/2 [==============================] - 0s 111ms/step - loss: 0.1282 - accuracy: 0.9750 - val_loss: 0.9674 - val_accuracy: 0.5000\n",
      "Epoch 84/100\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.4629 - accuracy: 0.9375 - val_loss: 0.9312 - val_accuracy: 0.7500\n",
      "Epoch 85/100\n",
      "2/2 [==============================] - 0s 119ms/step - loss: 0.8015 - accuracy: 0.9375 - val_loss: 0.9625 - val_accuracy: 0.7500\n",
      "Epoch 86/100\n",
      "2/2 [==============================] - 0s 106ms/step - loss: 0.0687 - accuracy: 0.9875 - val_loss: 0.9848 - val_accuracy: 0.7000\n",
      "Epoch 87/100\n",
      "2/2 [==============================] - 0s 104ms/step - loss: 0.1158 - accuracy: 0.9500 - val_loss: 0.9971 - val_accuracy: 0.7000\n",
      "Epoch 88/100\n",
      "2/2 [==============================] - 0s 194ms/step - loss: 0.1161 - accuracy: 0.9625 - val_loss: 0.9965 - val_accuracy: 0.7000\n",
      "Epoch 89/100\n",
      "2/2 [==============================] - 0s 189ms/step - loss: 0.1296 - accuracy: 0.9500 - val_loss: 0.9926 - val_accuracy: 0.8000\n",
      "Epoch 90/100\n",
      "2/2 [==============================] - 0s 178ms/step - loss: 0.1509 - accuracy: 0.9500 - val_loss: 0.9929 - val_accuracy: 0.8000\n",
      "Epoch 91/100\n",
      "2/2 [==============================] - 0s 325ms/step - loss: 0.0963 - accuracy: 0.9875 - val_loss: 1.0002 - val_accuracy: 0.9000\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.7686 - accuracy: 1.0000\n",
      "Evaluation Metrics for LSTM:\n",
      "loss: 0.7686002254486084\n",
      "accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "def LSTM_model(vocab_size, sequence_length):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=vocab_size, output_dim=128, input_length=sequence_length))\n",
    "    model.add(LSTM(units=128, activation='relu', return_sequences=True))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LSTM(units=64, activation='relu'))\n",
    "    model.add(Dense(256, activation='relu', kernel_regularizer=l2(0.001)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "    optimizer = Adam(learning_rate=0.001)\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "#EMBEDDING_DIM = 110\n",
    "lstm_model = LSTM_model(vocab_size, max_sequence_length)\n",
    "early_stopping_rnn = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "history_updated_rnn = lstm_model.fit(X_train_rnn, y_train_rnn, validation_data=(X_test_rnn, y_test_rnn), epochs=100, batch_size=64, callbacks=[early_stopping_rnn])\n",
    "\n",
    "\n",
    "# Evaluate the lstm model\n",
    "evaluation_metrics_updated_lstm = lstm_model.evaluate(X_test, Y_test)\n",
    "print(\"Evaluation Metrics for LSTM:\")\n",
    "for metric_name, metric_value in zip(lstm_model.metrics_names, evaluation_metrics_updated_lstm):\n",
    "    print(f\"{metric_name}: {metric_value}\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*<h4>TRANSFORMER Model<h4>*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, density, rate=0.1, l2_reg=0.01):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            layers.Dense(density, activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(l2_reg)),\n",
    "            layers.Dense(embed_dim, kernel_regularizer=tf.keras.regularizers.l2(l2_reg)),\n",
    "        ])\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "\n",
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions\n",
    "\n",
    "def build_transformer_model(maxlen, vocab_size, embed_dim, num_heads, density, transformer_units, mlp_units, dropout_rate, num_classes):\n",
    "    inputs = layers.Input(shape=(maxlen,))\n",
    "    embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
    "    x = embedding_layer(inputs)\n",
    "    for _ in range(transformer_units):\n",
    "        x = TransformerBlock(embed_dim, num_heads, density, rate=dropout_rate)(x)\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    for dim in mlp_units:\n",
    "        x = layers.Dense(dim, activation=\"relu\")(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "    outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = max_sequence_length\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "embed_dim = 120\n",
    "num_heads = 2\n",
    "density = 3\n",
    "transformer_units = 4\n",
    "mlp_units = [128]\n",
    "dropout_rate = 0.5\n",
    "num_classes = len(df['grade'].unique())\n",
    "\n",
    "transformer_model = build_transformer_model(maxlen, vocab_size, embed_dim, num_heads, density, transformer_units, mlp_units, dropout_rate, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\amine\\anaconda3\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Epoch 1/30\n",
      "3/3 [==============================] - 19s 735ms/step - loss: 1.5708 - accuracy: 0.4235 - val_loss: 1.4220 - val_accuracy: 0.6000\n",
      "Epoch 2/30\n",
      "3/3 [==============================] - 0s 76ms/step - loss: 1.5208 - accuracy: 0.4706 - val_loss: 1.4584 - val_accuracy: 0.4667\n",
      "Epoch 3/30\n",
      "3/3 [==============================] - 0s 72ms/step - loss: 1.6159 - accuracy: 0.3765 - val_loss: 1.3608 - val_accuracy: 0.6000\n",
      "Epoch 4/30\n",
      "3/3 [==============================] - 0s 96ms/step - loss: 1.2630 - accuracy: 0.6706 - val_loss: 1.1778 - val_accuracy: 0.6667\n",
      "Epoch 5/30\n",
      "3/3 [==============================] - 0s 113ms/step - loss: 1.2007 - accuracy: 0.6588 - val_loss: 1.0225 - val_accuracy: 0.8000\n",
      "Epoch 6/30\n",
      "3/3 [==============================] - 0s 149ms/step - loss: 0.9053 - accuracy: 0.8706 - val_loss: 0.8420 - val_accuracy: 0.7333\n",
      "Epoch 7/30\n",
      "3/3 [==============================] - 0s 94ms/step - loss: 0.8173 - accuracy: 0.8941 - val_loss: 0.7085 - val_accuracy: 0.9333\n",
      "Epoch 8/30\n",
      "3/3 [==============================] - 0s 114ms/step - loss: 0.6672 - accuracy: 0.9294 - val_loss: 0.5931 - val_accuracy: 0.9333\n",
      "Epoch 9/30\n",
      "3/3 [==============================] - 0s 110ms/step - loss: 0.6813 - accuracy: 0.9529 - val_loss: 0.5890 - val_accuracy: 0.8667\n",
      "Epoch 10/30\n",
      "3/3 [==============================] - 0s 113ms/step - loss: 0.6306 - accuracy: 0.9294 - val_loss: 0.5073 - val_accuracy: 0.9333\n",
      "Epoch 11/30\n",
      "3/3 [==============================] - 0s 118ms/step - loss: 0.5521 - accuracy: 0.9529 - val_loss: 0.5897 - val_accuracy: 0.9333\n",
      "Epoch 12/30\n",
      "3/3 [==============================] - 0s 107ms/step - loss: 0.5656 - accuracy: 0.9176 - val_loss: 0.4681 - val_accuracy: 1.0000\n",
      "Epoch 13/30\n",
      "3/3 [==============================] - 0s 158ms/step - loss: 0.4740 - accuracy: 0.9647 - val_loss: 0.4444 - val_accuracy: 1.0000\n",
      "Epoch 14/30\n",
      "3/3 [==============================] - 0s 129ms/step - loss: 0.4605 - accuracy: 0.9765 - val_loss: 0.5264 - val_accuracy: 0.9333\n",
      "Epoch 15/30\n",
      "3/3 [==============================] - 0s 114ms/step - loss: 0.4919 - accuracy: 0.9529 - val_loss: 0.5146 - val_accuracy: 0.8667\n",
      "Epoch 16/30\n",
      "3/3 [==============================] - 0s 116ms/step - loss: 0.4412 - accuracy: 0.9765 - val_loss: 0.4690 - val_accuracy: 0.9333\n",
      "Epoch 17/30\n",
      "3/3 [==============================] - 0s 142ms/step - loss: 0.4131 - accuracy: 0.9882 - val_loss: 0.4795 - val_accuracy: 0.9333\n",
      "Epoch 18/30\n",
      "3/3 [==============================] - 0s 137ms/step - loss: 0.3773 - accuracy: 1.0000 - val_loss: 0.5428 - val_accuracy: 0.8667\n",
      "Epoch 19/30\n",
      "3/3 [==============================] - 0s 122ms/step - loss: 0.3658 - accuracy: 1.0000 - val_loss: 0.6670 - val_accuracy: 0.8000\n",
      "Epoch 20/30\n",
      "3/3 [==============================] - 0s 175ms/step - loss: 0.3618 - accuracy: 1.0000 - val_loss: 0.5353 - val_accuracy: 0.9333\n",
      "Epoch 21/30\n",
      "3/3 [==============================] - 0s 147ms/step - loss: 0.3500 - accuracy: 1.0000 - val_loss: 0.6742 - val_accuracy: 0.9333\n",
      "Epoch 22/30\n",
      "3/3 [==============================] - 0s 117ms/step - loss: 0.3654 - accuracy: 0.9882 - val_loss: 0.4847 - val_accuracy: 0.8667\n",
      "Epoch 23/30\n",
      "3/3 [==============================] - 0s 120ms/step - loss: 0.3373 - accuracy: 1.0000 - val_loss: 0.9194 - val_accuracy: 0.8000\n",
      "Epoch 24/30\n",
      "3/3 [==============================] - 0s 167ms/step - loss: 0.3446 - accuracy: 0.9882 - val_loss: 0.8057 - val_accuracy: 0.9333\n",
      "Epoch 25/30\n",
      "3/3 [==============================] - 0s 125ms/step - loss: 0.3272 - accuracy: 1.0000 - val_loss: 0.7952 - val_accuracy: 0.9333\n",
      "Epoch 26/30\n",
      "3/3 [==============================] - 0s 126ms/step - loss: 0.3203 - accuracy: 1.0000 - val_loss: 0.7374 - val_accuracy: 0.9333\n",
      "Epoch 27/30\n",
      "3/3 [==============================] - 0s 211ms/step - loss: 0.3095 - accuracy: 1.0000 - val_loss: 0.7683 - val_accuracy: 0.8667\n",
      "Epoch 28/30\n",
      "3/3 [==============================] - 0s 158ms/step - loss: 0.3033 - accuracy: 1.0000 - val_loss: 1.0296 - val_accuracy: 0.8667\n",
      "Epoch 29/30\n",
      "3/3 [==============================] - 0s 133ms/step - loss: 0.2973 - accuracy: 1.0000 - val_loss: 1.1628 - val_accuracy: 0.8667\n",
      "Epoch 30/30\n",
      "3/3 [==============================] - 0s 109ms/step - loss: 0.2915 - accuracy: 1.0000 - val_loss: 1.2041 - val_accuracy: 0.8000\n",
      "Epoch 1/30\n",
      "3/3 [==============================] - 0s 201ms/step - loss: 0.3742 - accuracy: 0.9882 - val_loss: 0.4950 - val_accuracy: 0.8667\n",
      "Epoch 2/30\n",
      "3/3 [==============================] - 0s 124ms/step - loss: 0.2800 - accuracy: 1.0000 - val_loss: 0.9564 - val_accuracy: 0.9333\n",
      "Epoch 3/30\n",
      "3/3 [==============================] - 0s 113ms/step - loss: 0.2777 - accuracy: 1.0000 - val_loss: 1.0391 - val_accuracy: 0.9333\n",
      "Epoch 4/30\n",
      "3/3 [==============================] - 1s 211ms/step - loss: 0.2702 - accuracy: 1.0000 - val_loss: 1.0392 - val_accuracy: 0.9333\n",
      "Epoch 5/30\n",
      "3/3 [==============================] - 0s 145ms/step - loss: 0.2637 - accuracy: 1.0000 - val_loss: 1.0225 - val_accuracy: 0.9333\n",
      "Epoch 6/30\n",
      "3/3 [==============================] - 0s 124ms/step - loss: 0.2577 - accuracy: 1.0000 - val_loss: 1.0146 - val_accuracy: 0.8667\n",
      "Epoch 7/30\n",
      "3/3 [==============================] - 0s 125ms/step - loss: 0.2536 - accuracy: 1.0000 - val_loss: 1.0061 - val_accuracy: 0.8667\n",
      "Epoch 8/30\n",
      "3/3 [==============================] - 0s 133ms/step - loss: 0.2462 - accuracy: 1.0000 - val_loss: 0.9997 - val_accuracy: 0.8667\n",
      "Epoch 9/30\n",
      "3/3 [==============================] - 0s 135ms/step - loss: 0.2434 - accuracy: 1.0000 - val_loss: 0.9936 - val_accuracy: 0.9333\n",
      "Epoch 10/30\n",
      "3/3 [==============================] - 0s 127ms/step - loss: 0.2367 - accuracy: 1.0000 - val_loss: 0.9860 - val_accuracy: 0.9333\n",
      "Epoch 11/30\n",
      "3/3 [==============================] - 0s 196ms/step - loss: 0.2319 - accuracy: 1.0000 - val_loss: 0.9772 - val_accuracy: 0.9333\n",
      "Epoch 12/30\n",
      "3/3 [==============================] - 0s 115ms/step - loss: 0.2264 - accuracy: 1.0000 - val_loss: 0.9670 - val_accuracy: 0.9333\n",
      "Epoch 13/30\n",
      "3/3 [==============================] - 0s 125ms/step - loss: 0.2210 - accuracy: 1.0000 - val_loss: 0.9560 - val_accuracy: 0.9333\n",
      "Epoch 14/30\n",
      "3/3 [==============================] - 0s 146ms/step - loss: 0.2168 - accuracy: 1.0000 - val_loss: 0.9472 - val_accuracy: 0.9333\n",
      "Epoch 15/30\n",
      "3/3 [==============================] - 0s 141ms/step - loss: 0.2115 - accuracy: 1.0000 - val_loss: 0.9385 - val_accuracy: 0.9333\n",
      "Epoch 16/30\n",
      "3/3 [==============================] - 0s 127ms/step - loss: 0.2073 - accuracy: 1.0000 - val_loss: 0.9327 - val_accuracy: 0.9333\n",
      "Epoch 17/30\n",
      "3/3 [==============================] - 0s 120ms/step - loss: 0.2024 - accuracy: 1.0000 - val_loss: 0.9308 - val_accuracy: 0.9333\n",
      "Epoch 18/30\n",
      "3/3 [==============================] - 0s 132ms/step - loss: 0.1978 - accuracy: 1.0000 - val_loss: 0.9267 - val_accuracy: 0.9333\n",
      "Epoch 19/30\n",
      "3/3 [==============================] - 0s 101ms/step - loss: 0.1936 - accuracy: 1.0000 - val_loss: 0.9225 - val_accuracy: 0.9333\n",
      "Epoch 20/30\n",
      "3/3 [==============================] - 0s 125ms/step - loss: 0.1900 - accuracy: 1.0000 - val_loss: 0.9153 - val_accuracy: 0.9333\n",
      "Epoch 21/30\n",
      "3/3 [==============================] - 0s 150ms/step - loss: 0.2083 - accuracy: 0.9882 - val_loss: 1.2433 - val_accuracy: 0.8000\n",
      "Epoch 22/30\n",
      "3/3 [==============================] - 0s 146ms/step - loss: 0.2757 - accuracy: 0.9882 - val_loss: 0.6384 - val_accuracy: 0.9333\n",
      "Epoch 23/30\n",
      "3/3 [==============================] - 0s 122ms/step - loss: 0.2676 - accuracy: 0.9647 - val_loss: 0.8159 - val_accuracy: 0.8667\n",
      "Epoch 24/30\n",
      "3/3 [==============================] - 0s 134ms/step - loss: 0.1732 - accuracy: 1.0000 - val_loss: 1.2514 - val_accuracy: 0.8000\n",
      "Epoch 25/30\n",
      "3/3 [==============================] - 0s 123ms/step - loss: 0.2609 - accuracy: 0.9647 - val_loss: 0.7401 - val_accuracy: 0.8000\n",
      "Epoch 26/30\n",
      "3/3 [==============================] - 0s 131ms/step - loss: 0.2024 - accuracy: 0.9765 - val_loss: 0.3377 - val_accuracy: 0.9333\n",
      "Epoch 27/30\n",
      "3/3 [==============================] - 1s 226ms/step - loss: 0.1732 - accuracy: 1.0000 - val_loss: 0.3759 - val_accuracy: 0.8667\n",
      "Epoch 28/30\n",
      "3/3 [==============================] - 0s 100ms/step - loss: 0.1939 - accuracy: 0.9882 - val_loss: 0.7606 - val_accuracy: 0.8000\n",
      "Epoch 29/30\n",
      "3/3 [==============================] - 0s 93ms/step - loss: 0.1573 - accuracy: 1.0000 - val_loss: 1.3706 - val_accuracy: 0.8000\n",
      "Epoch 30/30\n",
      "3/3 [==============================] - 0s 91ms/step - loss: 0.1652 - accuracy: 1.0000 - val_loss: 1.2295 - val_accuracy: 0.8000\n"
     ]
    }
   ],
   "source": [
    "transformer_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "transformer_model.fit(X_train, Y_train, epochs=30, batch_size=32, validation_data=(X_test, Y_test))\n",
    "history = transformer_model.fit(X_train, Y_train, epochs=30, batch_size=32, validation_data=(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 69ms/step - loss: 1.2295 - accuracy: 0.8000\n",
      "Evaluation Metrics Transformer:\n",
      "loss: 1.2295254468917847\n",
      "accuracy: 0.800000011920929\n"
     ]
    }
   ],
   "source": [
    "evaluation_metrics_transformer = transformer_model.evaluate(X_test, Y_test)\n",
    "print(\"Evaluation Metrics Transformer:\")\n",
    "for metric_name, metric_value in zip(transformer_model.metrics_names, evaluation_metrics_transformer):\n",
    "    print(f\"{metric_name}: {metric_value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 151ms/step - loss: 1.2295 - accuracy: 0.8000\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1\n",
      "    4   1  12  59  11   5   8]\n",
      " [  0   0   0   0   0   0   0  12  15  11   5   8  23  13  20   2   9   6\n",
      "    7   1   3  10  18  14  16]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   4  28  49\n",
      "   23  13  20   2   1  12  15]\n",
      " [  0   0   0   0   0   0   4  17  21  13  19   2   9   6   7   1   3  10\n",
      "   18  14  16   1  11   5   8]\n",
      " [  0   0   0   0   0   0   0   0   0  54  33  19   2  22  55   4   1  12\n",
      "   15  11   5   8  43  14  16]\n",
      " [  0   0   0   0   0   0   0   0   0   4  17  23  13  20   2   9   6   7\n",
      "    1   3  10   1  12  53  87]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  18  14   5\n",
      "    8  26  24   1  19   2  22]\n",
      " [  0   0   0   0   0   0   0   4   2   9   6   7   1   3  10  17  21  13\n",
      "   19   2   1 118  11   5   8]\n",
      " [  0   0   0   0   0   0  37  29   5   8   3  64  28  22  65   1  38  66\n",
      "    4   1  12  15  11   5   8]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  43  14   5   8\n",
      "   26  24   1  33  19   2  22]\n",
      " [  0   0   0   0   0   0   4   2   9   6   7   1   3  10  17  21  13  20\n",
      "    2  18  14  16   1  12  15]\n",
      " [  0   0   0   0   0   0   4   2   9   6   7   1   3  10  17  23  13  20\n",
      "    2  18  14  16   1  12  15]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  18  14  16\n",
      "    4  12  15  21  13  20   2]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0  82  11   5   8]\n",
      " [  0   0   0   0   0   0   4  17  21  13  19   2   9   6   7   1   3  10\n",
      "   18  14  16   1  12  27  34]]\n",
      "\n",
      "Real and Predicted Values:\n",
      "    Real  Predicted\n",
      "0      0          0\n",
      "1      2          2\n",
      "2      2          2\n",
      "3      1          1\n",
      "4      2          2\n",
      "5      0          0\n",
      "6      1          1\n",
      "7      1          0\n",
      "8      2          2\n",
      "9      1          1\n",
      "10     2          2\n",
      "11     2          2\n",
      "12     2          0\n",
      "13     1          0\n",
      "14     0          0\n"
     ]
    }
   ],
   "source": [
    "evaluation_metrics_transformer = transformer_model.evaluate(X_test, Y_test)\n",
    "\n",
    "predictions = transformer_model.predict(X_test)\n",
    "print(X_test)\n",
    "\n",
    "# Convert one-hot encoded labels to class indices\n",
    "y_true = np.argmax(Y_test, axis=1)\n",
    "\n",
    "# Convert predicted probabilities to class indices\n",
    "y_pred = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Create DataFrame\n",
    "results_df = pd.DataFrame({\"Real\": y_true, \"Predicted\": y_pred})\n",
    "\n",
    "# Display DataFrame\n",
    "print(\"\\nReal and Predicted Values:\")\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy RNN: 0.9333333373069763\n",
      "Accuracy LSTM: 1.0\n",
      "Accuracy Transformer: 0.800000011920929\n"
     ]
    }
   ],
   "source": [
    "# Evaluate RNN model\n",
    "rnn_accuracy = rnn_model.evaluate(X_test, Y_test, verbose=0)[1]\n",
    "print(\"Accuracy RNN:\", rnn_accuracy)\n",
    "\n",
    "# Evaluate LSTM model\n",
    "lstm_accuracy = lstm_model.evaluate(X_test, Y_test, verbose=0)[1]\n",
    "print(\"Accuracy LSTM:\", lstm_accuracy)\n",
    "\n",
    "# Evaluate Transformer model\n",
    "transformer_accuracy = transformer_model.evaluate(X_test, Y_test, verbose=0)[1]\n",
    "print(\"Accuracy Transformer:\", transformer_accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model (LSTM) with accuracy 1.0 has been saved to './savedModels/q8_model.h5'\n"
     ]
    }
   ],
   "source": [
    "# Determine the best model\n",
    "best_model_name, best_model_accuracy = max([('RNN', rnn_accuracy), ('LSTM', lstm_accuracy), ('Transformer', transformer_accuracy)], key=lambda x: x[1])\n",
    "\n",
    "save_path = './savedModels/q8_model.h5'\n",
    "# Save the best model\n",
    "if best_model_name == 'RNN':\n",
    "    rnn_model.save(save_path)\n",
    "elif best_model_name == 'LSTM':\n",
    "    lstm_model.save(save_path)\n",
    "elif best_model_name == 'Transformer':\n",
    "    transformer_model.save(save_path)\n",
    "\n",
    "print(f\"The best model ({best_model_name}) with accuracy {best_model_accuracy} has been saved to '{save_path}'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
