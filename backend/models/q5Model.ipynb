{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\amine\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow as keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.regularizers import l2\n",
    "from keras import layers\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Embedding, SimpleRNN, Dense, Dropout, LSTM, Bidirectional, Input\n",
    "from keras.layers import Attention, GlobalAveragePooling1D\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import stanza\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import BatchNormalization, Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*<h3>Load Data<h3>*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_id</th>\n",
       "      <th>answer</th>\n",
       "      <th>grade</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>خلد التاريخ العبد السابق، كأول من أذن في الإسلام.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>بلال يحكي بصوته قصة نصر الإيمان وأولى نغمات ال...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>أول مؤذن هو عثمان بن عفان رضي الله عنه</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>في سجلات التاريخ، يُحفر اسم بلال بحروف من ذهب،...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>قصة بلال بن رباح تجيب: 'هو أول من أذن للمسلمين'!</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>بلال بن رباح صدح بأول أذان.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>كان بلال بن رباح الحبشي أول مؤذن في الإسلام.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5</td>\n",
       "      <td>بلال بن رباح رضي الله عنه، صاحب الصوت الجميل و...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5</td>\n",
       "      <td>عذوبة صوت بلال لم تكن مجرد نغمات، بل كانت أول ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5</td>\n",
       "      <td>المؤذن الأول في تاريخ الإسلام كان الصحابي بلال...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   question_id                                             answer  grade\n",
       "0            5  خلد التاريخ العبد السابق، كأول من أذن في الإسلام.      1\n",
       "1            5  بلال يحكي بصوته قصة نصر الإيمان وأولى نغمات ال...      2\n",
       "2            5             أول مؤذن هو عثمان بن عفان رضي الله عنه      0\n",
       "3            5  في سجلات التاريخ، يُحفر اسم بلال بحروف من ذهب،...      2\n",
       "4            5   قصة بلال بن رباح تجيب: 'هو أول من أذن للمسلمين'!      2\n",
       "5            5                        بلال بن رباح صدح بأول أذان.      2\n",
       "6            5       كان بلال بن رباح الحبشي أول مؤذن في الإسلام.      2\n",
       "7            5  بلال بن رباح رضي الله عنه، صاحب الصوت الجميل و...      2\n",
       "8            5  عذوبة صوت بلال لم تكن مجرد نغمات، بل كانت أول ...      2\n",
       "9            5  المؤذن الأول في تاريخ الإسلام كان الصحابي بلال...      2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read csv file into pandas\n",
    "df = pd.read_csv(\"../datasets/shuffled5.csv\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*<h3>EDA<h3>*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 99 entries, 0 to 98\n",
      "Data columns (total 3 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   question_id  99 non-null     int64 \n",
      " 1   answer       99 non-null     object\n",
      " 2   grade        99 non-null     int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 2.4+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "grade\n",
       "0    34\n",
       "1    28\n",
       "2    37\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('grade').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(data=df, x='grade')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*<h3>Data Cleaning<h3>*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('question_id', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     خلد التاريخ العبد السابق، كأول من أذن في الإسلام.\n",
      "1     بلال يحكي بصوته قصة نصر الإيمان وأولى نغمات ال...\n",
      "2                أول مؤذن هو عثمان بن عفان رضي الله عنه\n",
      "3     في سجلات التاريخ، يُحفر اسم بلال بحروف من ذهب،...\n",
      "4      قصة بلال بن رباح تجيب: 'هو أول من أذن للمسلمين'!\n",
      "                            ...                        \n",
      "94    عمار بن ياسر (رضي الله عنه) هو الصحابي الذي أد...\n",
      "95    المؤذن الذي نادى بالأذان واقفًا فوق الكعبة كان...\n",
      "96                       أذن صحابي لأول مرة في الإسلام.\n",
      "97    أول أذان في تاريخ الإسلام صدح به بلال بن رباح ...\n",
      "98    الصحابي عمار بن ياسر (رضي الله عنه) كان المؤذن...\n",
      "Name: answer, Length: 99, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df.drop_duplicates(inplace=True)\n",
    "print(df['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*<h3>Data Pre-Preocessing<h3>*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c520b40eaad04d89b77bf7491577d604",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.7.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-09 22:53:10 INFO: Downloading default packages for language: ar (Arabic) ...\n",
      "2024-01-09 22:53:11 INFO: File exists: C:\\Users\\amine\\stanza_resources\\ar\\default.zip\n",
      "2024-01-09 22:53:15 INFO: Finished downloading models and saved to C:\\Users\\amine\\stanza_resources.\n",
      "2024-01-09 22:53:15 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2eb3fa593db0473aa79ef7f335972baa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.7.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-09 22:53:18 INFO: Loading these models for language: ar (Arabic):\n",
      "=============================\n",
      "| Processor | Package       |\n",
      "-----------------------------\n",
      "| tokenize  | padt          |\n",
      "| mwt       | padt          |\n",
      "| pos       | padt_charlm   |\n",
      "| lemma     | padt_nocharlm |\n",
      "| depparse  | padt_charlm   |\n",
      "| ner       | aqmar_charlm  |\n",
      "=============================\n",
      "\n",
      "2024-01-09 22:53:18 INFO: Using device: cpu\n",
      "2024-01-09 22:53:18 INFO: Loading: tokenize\n",
      "2024-01-09 22:53:19 INFO: Loading: mwt\n",
      "2024-01-09 22:53:19 INFO: Loading: pos\n",
      "2024-01-09 22:53:20 INFO: Loading: lemma\n",
      "2024-01-09 22:53:20 INFO: Loading: depparse\n",
      "2024-01-09 22:53:20 INFO: Loading: ner\n",
      "2024-01-09 22:53:21 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "df['grade'] = le.fit_transform(df['grade'])\n",
    "\n",
    "stanza.download('ar')\n",
    "nlp = stanza.Pipeline('ar')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    doc = nlp(text)\n",
    "    tokens = [word.lemma for sent in doc.sentences for word in sent.words if word.upos != 'PUNCT']\n",
    "    return tokens\n",
    "\n",
    "df['answer'] = df['answer'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0 ...  18   6   7]\n",
      " [  0   0   0 ...   2  82  13]\n",
      " [  0   0   0 ...   8   4  46]\n",
      " ...\n",
      " [  0   0   0 ...  73   6   7]\n",
      " [  0   0   0 ...   4   9   1]\n",
      " [  0   0   0 ...  10 273  26]]\n",
      "0     [خَلَد, تَارِيخ, العبد, سَابِق, كَأُول, مِن, أ...\n",
      "1     [بلال, حَكَى, بِ, صُوَّة, هُوَ, قِصَّة, نصر, إ...\n",
      "2     [أَوَّل, مُؤَذَّن, هُوَ, عثمان, بِن, عفان, رضي...\n",
      "3     [فِي, سِجِلّ, تَارِيخ, حَفَر, اِسم, بلال, بِ, ...\n",
      "4     [قِصَّة, بلال, بِن, رباح, أَجَاب, هُوَ, أَوَّل...\n",
      "                            ...                        \n",
      "94    [عمار, بِن, ياسر, رضي, الله, عَن, هُوَ, هُوَ, ...\n",
      "95    [مُؤَذِّن, اَلَّذِي, نَادِي, بِ, أَذَان, وَ, أ...\n",
      "96      [أَذن, صحابي, لِ, أَوَّل, مَرَّة, فِي, إِسلَام]\n",
      "97    [أَوَّل, أُذُن, فِي, تَارِيخ, إِسلَام, صَدَّح,...\n",
      "98    [صَحَابِيّ, عمار, بِن, ياسر, رضي, الله, عَن, ه...\n",
      "Name: answer, Length: 99, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Tokenization\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df['answer'])\n",
    "sequences = tokenizer.texts_to_sequences(df['answer'])\n",
    "max_sequence_length = max(len(s) for s in sequences)\n",
    "sequences = pad_sequences(sequences,max_sequence_length)\n",
    "word2idx = tokenizer.word_index\n",
    "vocab_size = len(word2idx) + 1\n",
    "\n",
    "\n",
    "X = pad_sequences(sequences, padding='post', truncating='post', maxlen=max_sequence_length)\n",
    "\n",
    "print(sequences)\n",
    "print(df['answer'])\n",
    "\n",
    "Y = to_categorical(df['grade'], num_classes=3)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.15, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*<h3>build Models<h3>*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*<h4>RNN Model<h4>*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\amine\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:From c:\\Users\\amine\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\amine\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "2/2 [==============================] - 7s 1s/step - loss: 1.2396 - accuracy: 0.3418 - val_loss: 1.2032 - val_accuracy: 0.3500 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "2/2 [==============================] - 0s 106ms/step - loss: 1.2250 - accuracy: 0.3291 - val_loss: 1.2235 - val_accuracy: 0.3500 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 1.1236 - accuracy: 0.5000\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 1.1496 - accuracy: 0.4810 - val_loss: 1.2080 - val_accuracy: 0.3500 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 1.0893 - accuracy: 0.4684 - val_loss: 1.2050 - val_accuracy: 0.3500 - lr: 2.0000e-04\n",
      "Epoch 5/100\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 1.1575 - accuracy: 0.4937 - val_loss: 1.2031 - val_accuracy: 0.3500 - lr: 2.0000e-04\n",
      "Epoch 6/100\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 1.1224 - accuracy: 0.4557 - val_loss: 1.2008 - val_accuracy: 0.3500 - lr: 2.0000e-04\n",
      "Epoch 7/100\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 1.1524 - accuracy: 0.4557 - val_loss: 1.1988 - val_accuracy: 0.3500 - lr: 2.0000e-04\n",
      "Epoch 8/100\n",
      "2/2 [==============================] - 0s 105ms/step - loss: 1.0657 - accuracy: 0.5823 - val_loss: 1.1975 - val_accuracy: 0.3500 - lr: 2.0000e-04\n",
      "Epoch 9/100\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 1.0356 - accuracy: 0.5696 - val_loss: 1.1966 - val_accuracy: 0.3500 - lr: 2.0000e-04\n",
      "Epoch 10/100\n",
      "2/2 [==============================] - 0s 106ms/step - loss: 1.0627 - accuracy: 0.5443 - val_loss: 1.1957 - val_accuracy: 0.3500 - lr: 2.0000e-04\n",
      "Epoch 11/100\n",
      "2/2 [==============================] - 0s 124ms/step - loss: 1.0808 - accuracy: 0.5316 - val_loss: 1.1948 - val_accuracy: 0.3500 - lr: 2.0000e-04\n",
      "Epoch 12/100\n",
      "2/2 [==============================] - 0s 115ms/step - loss: 1.0344 - accuracy: 0.6076 - val_loss: 1.1942 - val_accuracy: 0.3500 - lr: 2.0000e-04\n",
      "Epoch 13/100\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 0.9964 - accuracy: 0.6203 - val_loss: 1.1935 - val_accuracy: 0.3500 - lr: 2.0000e-04\n",
      "Epoch 14/100\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 1.0276 - accuracy: 0.5696 - val_loss: 1.1929 - val_accuracy: 0.3500 - lr: 2.0000e-04\n",
      "Epoch 15/100\n",
      "2/2 [==============================] - 0s 115ms/step - loss: 1.0100 - accuracy: 0.6076 - val_loss: 1.1923 - val_accuracy: 0.3500 - lr: 2.0000e-04\n",
      "Epoch 16/100\n",
      "2/2 [==============================] - 0s 202ms/step - loss: 0.9411 - accuracy: 0.7342 - val_loss: 1.1922 - val_accuracy: 0.3500 - lr: 2.0000e-04\n",
      "Epoch 17/100\n",
      "2/2 [==============================] - 0s 237ms/step - loss: 0.9828 - accuracy: 0.6582 - val_loss: 1.1919 - val_accuracy: 0.3500 - lr: 2.0000e-04\n",
      "Epoch 18/100\n",
      "2/2 [==============================] - 0s 259ms/step - loss: 0.9384 - accuracy: 0.7595 - val_loss: 1.1917 - val_accuracy: 0.3500 - lr: 2.0000e-04\n",
      "Epoch 19/100\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.8941 - accuracy: 0.7848 - val_loss: 1.1911 - val_accuracy: 0.3500 - lr: 2.0000e-04\n",
      "Epoch 20/100\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 1.0029 - accuracy: 0.5823 - val_loss: 1.1898 - val_accuracy: 0.3500 - lr: 2.0000e-04\n",
      "Epoch 21/100\n",
      "2/2 [==============================] - 0s 191ms/step - loss: 0.8926 - accuracy: 0.7595 - val_loss: 1.1882 - val_accuracy: 0.3500 - lr: 2.0000e-04\n",
      "Epoch 22/100\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 0.9001 - accuracy: 0.7722 - val_loss: 1.1864 - val_accuracy: 0.4500 - lr: 2.0000e-04\n",
      "Epoch 23/100\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.8606 - accuracy: 0.7342 - val_loss: 1.1849 - val_accuracy: 0.6000 - lr: 2.0000e-04\n",
      "Epoch 24/100\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 0.8710 - accuracy: 0.7342 - val_loss: 1.1839 - val_accuracy: 0.4500 - lr: 2.0000e-04\n",
      "Epoch 25/100\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 0.8850 - accuracy: 0.7089 - val_loss: 1.1832 - val_accuracy: 0.3500 - lr: 2.0000e-04\n",
      "Epoch 26/100\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.8619 - accuracy: 0.7848 - val_loss: 1.1827 - val_accuracy: 0.4500 - lr: 2.0000e-04\n",
      "Epoch 27/100\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.8014 - accuracy: 0.8101 - val_loss: 1.1822 - val_accuracy: 0.3500 - lr: 2.0000e-04\n",
      "Epoch 28/100\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.8107 - accuracy: 0.8228 - val_loss: 1.1820 - val_accuracy: 0.3500 - lr: 2.0000e-04\n",
      "Epoch 29/100\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.7893 - accuracy: 0.8101 - val_loss: 1.1819 - val_accuracy: 0.3500 - lr: 2.0000e-04\n",
      "Epoch 30/100\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 0.7141 - accuracy: 0.8987 - val_loss: 1.1814 - val_accuracy: 0.3500 - lr: 2.0000e-04\n",
      "Epoch 31/100\n",
      "2/2 [==============================] - 0s 115ms/step - loss: 0.7025 - accuracy: 0.8861 - val_loss: 1.1803 - val_accuracy: 0.3500 - lr: 2.0000e-04\n",
      "Epoch 32/100\n",
      "2/2 [==============================] - 0s 423ms/step - loss: 0.7058 - accuracy: 0.8861 - val_loss: 1.1787 - val_accuracy: 0.3500 - lr: 2.0000e-04\n",
      "Epoch 33/100\n",
      "2/2 [==============================] - 0s 103ms/step - loss: 0.7502 - accuracy: 0.8608 - val_loss: 1.1774 - val_accuracy: 0.4500 - lr: 2.0000e-04\n",
      "Epoch 34/100\n",
      "2/2 [==============================] - 0s 117ms/step - loss: 0.7154 - accuracy: 0.8608 - val_loss: 1.1755 - val_accuracy: 0.4500 - lr: 2.0000e-04\n",
      "Epoch 35/100\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 0.6746 - accuracy: 0.9114 - val_loss: 1.1730 - val_accuracy: 0.4500 - lr: 2.0000e-04\n",
      "Epoch 36/100\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 0.6539 - accuracy: 0.9494 - val_loss: 1.1700 - val_accuracy: 0.4500 - lr: 2.0000e-04\n",
      "Epoch 37/100\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 0.6336 - accuracy: 0.9114 - val_loss: 1.1673 - val_accuracy: 0.4500 - lr: 2.0000e-04\n",
      "Epoch 38/100\n",
      "2/2 [==============================] - 0s 182ms/step - loss: 0.6222 - accuracy: 0.9494 - val_loss: 1.1651 - val_accuracy: 0.4000 - lr: 2.0000e-04\n",
      "Epoch 39/100\n",
      "2/2 [==============================] - 0s 284ms/step - loss: 0.5887 - accuracy: 0.9241 - val_loss: 1.1630 - val_accuracy: 0.4000 - lr: 2.0000e-04\n",
      "Epoch 40/100\n",
      "2/2 [==============================] - 0s 95ms/step - loss: 0.5846 - accuracy: 0.9367 - val_loss: 1.1602 - val_accuracy: 0.4500 - lr: 2.0000e-04\n",
      "Epoch 41/100\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 0.5546 - accuracy: 0.9494 - val_loss: 1.1562 - val_accuracy: 0.4000 - lr: 2.0000e-04\n",
      "Epoch 42/100\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.5364 - accuracy: 0.9494 - val_loss: 1.1519 - val_accuracy: 0.4500 - lr: 2.0000e-04\n",
      "Epoch 43/100\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.5298 - accuracy: 0.9873 - val_loss: 1.1480 - val_accuracy: 0.4500 - lr: 2.0000e-04\n",
      "Epoch 44/100\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.4880 - accuracy: 0.9494 - val_loss: 1.1454 - val_accuracy: 0.5000 - lr: 2.0000e-04\n",
      "Epoch 45/100\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.5253 - accuracy: 0.9494 - val_loss: 1.1410 - val_accuracy: 0.5000 - lr: 2.0000e-04\n",
      "Epoch 46/100\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 0.4665 - accuracy: 0.9747 - val_loss: 1.1368 - val_accuracy: 0.5500 - lr: 2.0000e-04\n",
      "Epoch 47/100\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.4371 - accuracy: 0.9747 - val_loss: 1.1337 - val_accuracy: 0.5500 - lr: 2.0000e-04\n",
      "Epoch 48/100\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.4746 - accuracy: 0.9747 - val_loss: 1.1309 - val_accuracy: 0.5500 - lr: 2.0000e-04\n",
      "Epoch 49/100\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 0.4134 - accuracy: 0.9747 - val_loss: 1.1243 - val_accuracy: 0.5500 - lr: 2.0000e-04\n",
      "Epoch 50/100\n",
      "2/2 [==============================] - 0s 98ms/step - loss: 0.4107 - accuracy: 1.0000 - val_loss: 1.1183 - val_accuracy: 0.5500 - lr: 2.0000e-04\n",
      "Epoch 51/100\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 0.4089 - accuracy: 0.9747 - val_loss: 1.1164 - val_accuracy: 0.5500 - lr: 2.0000e-04\n",
      "Epoch 52/100\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.4038 - accuracy: 0.9873 - val_loss: 1.1137 - val_accuracy: 0.5500 - lr: 2.0000e-04\n",
      "Epoch 53/100\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.3646 - accuracy: 1.0000 - val_loss: 1.1077 - val_accuracy: 0.6000 - lr: 2.0000e-04\n",
      "Epoch 54/100\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.3886 - accuracy: 1.0000 - val_loss: 1.1035 - val_accuracy: 0.6000 - lr: 2.0000e-04\n",
      "Epoch 55/100\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.3550 - accuracy: 1.0000 - val_loss: 1.0996 - val_accuracy: 0.5500 - lr: 2.0000e-04\n",
      "Epoch 56/100\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.3398 - accuracy: 1.0000 - val_loss: 1.0941 - val_accuracy: 0.5500 - lr: 2.0000e-04\n",
      "Epoch 57/100\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.3299 - accuracy: 1.0000 - val_loss: 1.0899 - val_accuracy: 0.5500 - lr: 2.0000e-04\n",
      "Epoch 58/100\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.3343 - accuracy: 1.0000 - val_loss: 1.0860 - val_accuracy: 0.5500 - lr: 2.0000e-04\n",
      "Epoch 59/100\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.3140 - accuracy: 1.0000 - val_loss: 1.0816 - val_accuracy: 0.6000 - lr: 2.0000e-04\n",
      "Epoch 60/100\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.3113 - accuracy: 1.0000 - val_loss: 1.0769 - val_accuracy: 0.6000 - lr: 2.0000e-04\n",
      "Epoch 61/100\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 0.3133 - accuracy: 1.0000 - val_loss: 1.0723 - val_accuracy: 0.6000 - lr: 2.0000e-04\n",
      "Epoch 62/100\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.2870 - accuracy: 1.0000 - val_loss: 1.0644 - val_accuracy: 0.6000 - lr: 2.0000e-04\n",
      "Epoch 63/100\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.2786 - accuracy: 1.0000 - val_loss: 1.0599 - val_accuracy: 0.6000 - lr: 2.0000e-04\n",
      "Epoch 64/100\n",
      "2/2 [==============================] - 0s 128ms/step - loss: 0.2817 - accuracy: 1.0000 - val_loss: 1.0585 - val_accuracy: 0.5500 - lr: 2.0000e-04\n",
      "Epoch 65/100\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.2749 - accuracy: 1.0000 - val_loss: 1.0534 - val_accuracy: 0.5500 - lr: 2.0000e-04\n",
      "Epoch 66/100\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.2758 - accuracy: 1.0000 - val_loss: 1.0481 - val_accuracy: 0.5500 - lr: 2.0000e-04\n",
      "Epoch 67/100\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.2627 - accuracy: 1.0000 - val_loss: 1.0461 - val_accuracy: 0.5500 - lr: 2.0000e-04\n",
      "Epoch 68/100\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 0.2592 - accuracy: 1.0000 - val_loss: 1.0450 - val_accuracy: 0.5500 - lr: 2.0000e-04\n",
      "Epoch 69/100\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.2539 - accuracy: 1.0000 - val_loss: 1.0392 - val_accuracy: 0.5500 - lr: 2.0000e-04\n",
      "Epoch 70/100\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.2543 - accuracy: 1.0000 - val_loss: 1.0374 - val_accuracy: 0.5500 - lr: 2.0000e-04\n",
      "Epoch 71/100\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.2465 - accuracy: 1.0000 - val_loss: 1.0308 - val_accuracy: 0.5500 - lr: 2.0000e-04\n",
      "Epoch 72/100\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.2447 - accuracy: 1.0000 - val_loss: 1.0195 - val_accuracy: 0.5500 - lr: 2.0000e-04\n",
      "Epoch 73/100\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 0.2354 - accuracy: 1.0000 - val_loss: 1.0213 - val_accuracy: 0.5500 - lr: 2.0000e-04\n",
      "Epoch 74/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.2431 - accuracy: 1.0000\n",
      "Epoch 74: ReduceLROnPlateau reducing learning rate to 0.0001.\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.2429 - accuracy: 1.0000 - val_loss: 1.0230 - val_accuracy: 0.5500 - lr: 2.0000e-04\n",
      "Epoch 75/100\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.2289 - accuracy: 1.0000 - val_loss: 1.0137 - val_accuracy: 0.5500 - lr: 1.0000e-04\n",
      "Epoch 76/100\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.2288 - accuracy: 1.0000 - val_loss: 1.0067 - val_accuracy: 0.6000 - lr: 1.0000e-04\n",
      "Epoch 77/100\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.2427 - accuracy: 1.0000 - val_loss: 1.0035 - val_accuracy: 0.6000 - lr: 1.0000e-04\n",
      "Epoch 78/100\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 0.2188 - accuracy: 1.0000 - val_loss: 1.0022 - val_accuracy: 0.6000 - lr: 1.0000e-04\n",
      "Epoch 79/100\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.2315 - accuracy: 1.0000 - val_loss: 1.0027 - val_accuracy: 0.5500 - lr: 1.0000e-04\n",
      "Epoch 80/100\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.2175 - accuracy: 1.0000 - val_loss: 0.9992 - val_accuracy: 0.6000 - lr: 1.0000e-04\n",
      "Epoch 81/100\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 0.2081 - accuracy: 1.0000 - val_loss: 0.9913 - val_accuracy: 0.6000 - lr: 1.0000e-04\n",
      "Epoch 82/100\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 0.2213 - accuracy: 1.0000 - val_loss: 0.9834 - val_accuracy: 0.6000 - lr: 1.0000e-04\n",
      "Epoch 83/100\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 0.2067 - accuracy: 1.0000 - val_loss: 0.9788 - val_accuracy: 0.6500 - lr: 1.0000e-04\n",
      "Epoch 84/100\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.2213 - accuracy: 1.0000 - val_loss: 0.9785 - val_accuracy: 0.6000 - lr: 1.0000e-04\n",
      "Epoch 85/100\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.2130 - accuracy: 1.0000 - val_loss: 0.9820 - val_accuracy: 0.6000 - lr: 1.0000e-04\n",
      "Epoch 86/100\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 0.2151 - accuracy: 1.0000 - val_loss: 0.9807 - val_accuracy: 0.6000 - lr: 1.0000e-04\n",
      "Epoch 87/100\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.2053 - accuracy: 1.0000 - val_loss: 0.9721 - val_accuracy: 0.6000 - lr: 1.0000e-04\n",
      "Epoch 88/100\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 0.2183 - accuracy: 1.0000 - val_loss: 0.9654 - val_accuracy: 0.6500 - lr: 1.0000e-04\n",
      "Epoch 89/100\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.2085 - accuracy: 1.0000 - val_loss: 0.9620 - val_accuracy: 0.6500 - lr: 1.0000e-04\n",
      "Epoch 90/100\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.1942 - accuracy: 1.0000 - val_loss: 0.9611 - val_accuracy: 0.6500 - lr: 1.0000e-04\n",
      "Epoch 91/100\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.2060 - accuracy: 1.0000 - val_loss: 0.9609 - val_accuracy: 0.6000 - lr: 1.0000e-04\n",
      "Epoch 92/100\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.2111 - accuracy: 1.0000 - val_loss: 0.9560 - val_accuracy: 0.6500 - lr: 1.0000e-04\n",
      "Epoch 93/100\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 0.1972 - accuracy: 1.0000 - val_loss: 0.9487 - val_accuracy: 0.6500 - lr: 1.0000e-04\n",
      "Epoch 94/100\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.1997 - accuracy: 1.0000 - val_loss: 0.9428 - val_accuracy: 0.6500 - lr: 1.0000e-04\n",
      "Epoch 95/100\n",
      "2/2 [==============================] - 0s 173ms/step - loss: 0.1977 - accuracy: 1.0000 - val_loss: 0.9397 - val_accuracy: 0.6500 - lr: 1.0000e-04\n",
      "Epoch 96/100\n",
      "2/2 [==============================] - 0s 110ms/step - loss: 0.2057 - accuracy: 1.0000 - val_loss: 0.9379 - val_accuracy: 0.6500 - lr: 1.0000e-04\n",
      "Epoch 97/100\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 0.1973 - accuracy: 1.0000 - val_loss: 0.9364 - val_accuracy: 0.6500 - lr: 1.0000e-04\n",
      "Epoch 98/100\n",
      "2/2 [==============================] - 0s 225ms/step - loss: 0.1937 - accuracy: 1.0000 - val_loss: 0.9322 - val_accuracy: 0.6500 - lr: 1.0000e-04\n",
      "Epoch 99/100\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.2000 - accuracy: 1.0000 - val_loss: 0.9270 - val_accuracy: 0.7000 - lr: 1.0000e-04\n",
      "Epoch 100/100\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 0.1908 - accuracy: 1.0000 - val_loss: 0.9224 - val_accuracy: 0.7000 - lr: 1.0000e-04\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.6105 - accuracy: 0.9333\n",
      "Evaluation Metrics for RNN:\n",
      "loss: 0.6104791164398193\n",
      "accuracy: 0.9333333373069763\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "\n",
    "X_train_rnn, X_test_rnn, y_train_rnn, y_test_rnn = train_test_split(X, Y, test_size=0.2)\n",
    "\n",
    "def RNN_model(vocab_size, sequence_length):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=vocab_size, output_dim=128, input_length=sequence_length))\n",
    "    model.add(SimpleRNN(units=128, activation='relu', return_sequences=True))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(SimpleRNN(units=64, activation='sigmoid'))\n",
    "    model.add(Dense(256, activation='sigmoid', kernel_regularizer=l2(0.001)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "    optimizer = Adam(learning_rate=0.001)\n",
    "    \n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=0.0001, verbose=1)\n",
    "#EMBEDDING_DIM = 110\n",
    "rnn_model = RNN_model(vocab_size, max_sequence_length)\n",
    "early_stopping_rnn = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "history_updated_rnn = rnn_model.fit(X_train_rnn, y_train_rnn, validation_data=(X_test_rnn, y_test_rnn), epochs=100, batch_size=64, callbacks=[reduce_lr])\n",
    "\n",
    "# Evaluate the RNN model\n",
    "evaluation_metrics_updated_rnn = rnn_model.evaluate(X_test, Y_test)\n",
    "print(\"Evaluation Metrics for RNN:\")\n",
    "for metric_name, metric_value in zip(rnn_model.metrics_names, evaluation_metrics_updated_rnn):\n",
    "    print(f\"{metric_name}: {metric_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*<h4>LSTM Model<h4>*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2/2 [==============================] - 8s 1s/step - loss: 1.2027 - accuracy: 0.3165 - val_loss: 1.1981 - val_accuracy: 0.5000\n",
      "Epoch 2/100\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 1.1477 - accuracy: 0.6329 - val_loss: 1.1957 - val_accuracy: 0.3500\n",
      "Epoch 3/100\n",
      "2/2 [==============================] - 0s 105ms/step - loss: 1.0940 - accuracy: 0.7468 - val_loss: 1.1929 - val_accuracy: 0.3500\n",
      "Epoch 4/100\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.9821 - accuracy: 0.8101 - val_loss: 1.1900 - val_accuracy: 0.3500\n",
      "Epoch 5/100\n",
      "2/2 [==============================] - 0s 98ms/step - loss: 0.8217 - accuracy: 0.8354 - val_loss: 1.1868 - val_accuracy: 0.3500\n",
      "Epoch 6/100\n",
      "2/2 [==============================] - 0s 107ms/step - loss: 0.5892 - accuracy: 0.9241 - val_loss: 1.1829 - val_accuracy: 0.4000\n",
      "Epoch 7/100\n",
      "2/2 [==============================] - 0s 128ms/step - loss: 0.3470 - accuracy: 0.9620 - val_loss: 1.1776 - val_accuracy: 0.6000\n",
      "Epoch 8/100\n",
      "2/2 [==============================] - 0s 152ms/step - loss: 0.2739 - accuracy: 0.9114 - val_loss: 1.1686 - val_accuracy: 0.5500\n",
      "Epoch 9/100\n",
      "2/2 [==============================] - 0s 162ms/step - loss: 0.1781 - accuracy: 0.9873 - val_loss: 1.1574 - val_accuracy: 0.8000\n",
      "Epoch 10/100\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 0.1755 - accuracy: 0.9620 - val_loss: 1.1479 - val_accuracy: 0.7500\n",
      "Epoch 11/100\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.1330 - accuracy: 0.9747 - val_loss: 1.1395 - val_accuracy: 0.7500\n",
      "Epoch 12/100\n",
      "2/2 [==============================] - 0s 111ms/step - loss: 0.1200 - accuracy: 1.0000 - val_loss: 1.1321 - val_accuracy: 0.7500\n",
      "Epoch 13/100\n",
      "2/2 [==============================] - 0s 106ms/step - loss: 0.1027 - accuracy: 1.0000 - val_loss: 1.1255 - val_accuracy: 0.7500\n",
      "Epoch 14/100\n",
      "2/2 [==============================] - 0s 115ms/step - loss: 0.1196 - accuracy: 0.9747 - val_loss: 1.1193 - val_accuracy: 0.7500\n",
      "Epoch 15/100\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 0.0885 - accuracy: 1.0000 - val_loss: 1.1148 - val_accuracy: 0.8000\n",
      "Epoch 16/100\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 0.0877 - accuracy: 1.0000 - val_loss: 1.1110 - val_accuracy: 0.8000\n",
      "Epoch 17/100\n",
      "2/2 [==============================] - 0s 98ms/step - loss: 0.0883 - accuracy: 1.0000 - val_loss: 1.1076 - val_accuracy: 0.8000\n",
      "Epoch 18/100\n",
      "2/2 [==============================] - 0s 98ms/step - loss: 0.0887 - accuracy: 1.0000 - val_loss: 1.1047 - val_accuracy: 0.8000\n",
      "Epoch 19/100\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.0918 - accuracy: 0.9873 - val_loss: 1.1034 - val_accuracy: 0.8000\n",
      "Epoch 20/100\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.0871 - accuracy: 1.0000 - val_loss: 1.1018 - val_accuracy: 0.8000\n",
      "Epoch 21/100\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.0837 - accuracy: 1.0000 - val_loss: 1.0967 - val_accuracy: 0.8000\n",
      "Epoch 22/100\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 0.0818 - accuracy: 1.0000 - val_loss: 1.0916 - val_accuracy: 0.8000\n",
      "Epoch 23/100\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.0764 - accuracy: 1.0000 - val_loss: 1.0890 - val_accuracy: 0.8000\n",
      "Epoch 24/100\n",
      "2/2 [==============================] - 0s 104ms/step - loss: 0.0798 - accuracy: 1.0000 - val_loss: 1.0870 - val_accuracy: 0.8000\n",
      "Epoch 25/100\n",
      "2/2 [==============================] - 0s 130ms/step - loss: 0.0747 - accuracy: 1.0000 - val_loss: 1.0856 - val_accuracy: 0.8000\n",
      "Epoch 26/100\n",
      "2/2 [==============================] - 0s 115ms/step - loss: 0.1333 - accuracy: 0.9873 - val_loss: 1.0849 - val_accuracy: 0.8000\n",
      "Epoch 27/100\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.0728 - accuracy: 1.0000 - val_loss: 1.0837 - val_accuracy: 0.8000\n",
      "Epoch 28/100\n",
      "2/2 [==============================] - 0s 107ms/step - loss: 0.0986 - accuracy: 0.9873 - val_loss: 1.0820 - val_accuracy: 0.8000\n",
      "Epoch 29/100\n",
      "2/2 [==============================] - 0s 117ms/step - loss: 0.1123 - accuracy: 0.9873 - val_loss: 1.0799 - val_accuracy: 0.8000\n",
      "Epoch 30/100\n",
      "2/2 [==============================] - 0s 275ms/step - loss: 0.0795 - accuracy: 1.0000 - val_loss: 1.0777 - val_accuracy: 0.8000\n",
      "Epoch 31/100\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.0735 - accuracy: 1.0000 - val_loss: 1.0755 - val_accuracy: 0.8000\n",
      "Epoch 32/100\n",
      "2/2 [==============================] - 0s 105ms/step - loss: 0.0783 - accuracy: 1.0000 - val_loss: 1.0721 - val_accuracy: 0.8000\n",
      "Epoch 33/100\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 0.0692 - accuracy: 1.0000 - val_loss: 1.0681 - val_accuracy: 0.8000\n",
      "Epoch 34/100\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.0692 - accuracy: 1.0000 - val_loss: 1.0645 - val_accuracy: 0.8000\n",
      "Epoch 35/100\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 0.0683 - accuracy: 1.0000 - val_loss: 1.0610 - val_accuracy: 0.8000\n",
      "Epoch 36/100\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.0667 - accuracy: 1.0000 - val_loss: 1.0577 - val_accuracy: 0.8000\n",
      "Epoch 37/100\n",
      "2/2 [==============================] - 0s 162ms/step - loss: 0.0659 - accuracy: 1.0000 - val_loss: 1.0546 - val_accuracy: 0.8000\n",
      "Epoch 38/100\n",
      "2/2 [==============================] - 0s 120ms/step - loss: 0.0656 - accuracy: 1.0000 - val_loss: 1.0516 - val_accuracy: 0.8000\n",
      "Epoch 39/100\n",
      "2/2 [==============================] - 0s 120ms/step - loss: 0.0644 - accuracy: 1.0000 - val_loss: 1.0488 - val_accuracy: 0.8000\n",
      "Epoch 40/100\n",
      "2/2 [==============================] - 0s 110ms/step - loss: 0.0634 - accuracy: 1.0000 - val_loss: 1.0459 - val_accuracy: 0.8000\n",
      "Epoch 41/100\n",
      "2/2 [==============================] - 0s 114ms/step - loss: 0.0625 - accuracy: 1.0000 - val_loss: 1.0432 - val_accuracy: 0.8000\n",
      "Epoch 42/100\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.0618 - accuracy: 1.0000 - val_loss: 1.0405 - val_accuracy: 0.8000\n",
      "Epoch 43/100\n",
      "2/2 [==============================] - 0s 125ms/step - loss: 0.0611 - accuracy: 1.0000 - val_loss: 1.0379 - val_accuracy: 0.8000\n",
      "Epoch 44/100\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.0604 - accuracy: 1.0000 - val_loss: 1.0353 - val_accuracy: 0.8000\n",
      "Epoch 45/100\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.0599 - accuracy: 1.0000 - val_loss: 1.0328 - val_accuracy: 0.8000\n",
      "Epoch 46/100\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.0590 - accuracy: 1.0000 - val_loss: 1.0305 - val_accuracy: 0.8000\n",
      "Epoch 47/100\n",
      "2/2 [==============================] - 0s 98ms/step - loss: 0.0584 - accuracy: 1.0000 - val_loss: 1.0282 - val_accuracy: 0.8000\n",
      "Epoch 48/100\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.0578 - accuracy: 1.0000 - val_loss: 1.0259 - val_accuracy: 0.8000\n",
      "Epoch 49/100\n",
      "2/2 [==============================] - 0s 190ms/step - loss: 0.0571 - accuracy: 1.0000 - val_loss: 1.0238 - val_accuracy: 0.8000\n",
      "Epoch 50/100\n",
      "2/2 [==============================] - 0s 120ms/step - loss: 0.0566 - accuracy: 1.0000 - val_loss: 1.0218 - val_accuracy: 0.8000\n",
      "Epoch 51/100\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.0561 - accuracy: 1.0000 - val_loss: 1.0196 - val_accuracy: 0.8000\n",
      "Epoch 52/100\n",
      "2/2 [==============================] - 0s 124ms/step - loss: 0.0557 - accuracy: 1.0000 - val_loss: 1.0175 - val_accuracy: 0.8000\n",
      "Epoch 53/100\n",
      "2/2 [==============================] - 0s 109ms/step - loss: 0.0549 - accuracy: 1.0000 - val_loss: 1.0154 - val_accuracy: 0.8000\n",
      "Epoch 54/100\n",
      "2/2 [==============================] - 0s 119ms/step - loss: 0.0543 - accuracy: 1.0000 - val_loss: 1.0133 - val_accuracy: 0.8000\n",
      "Epoch 55/100\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.0538 - accuracy: 1.0000 - val_loss: 1.0113 - val_accuracy: 0.8000\n",
      "Epoch 56/100\n",
      "2/2 [==============================] - 0s 156ms/step - loss: 0.0534 - accuracy: 1.0000 - val_loss: 1.0093 - val_accuracy: 0.8000\n",
      "Epoch 57/100\n",
      "2/2 [==============================] - 0s 122ms/step - loss: 0.0528 - accuracy: 1.0000 - val_loss: 1.0072 - val_accuracy: 0.8000\n",
      "Epoch 58/100\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 0.0523 - accuracy: 1.0000 - val_loss: 1.0052 - val_accuracy: 0.8000\n",
      "Epoch 59/100\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.0518 - accuracy: 1.0000 - val_loss: 1.0033 - val_accuracy: 0.8000\n",
      "Epoch 60/100\n",
      "2/2 [==============================] - 0s 123ms/step - loss: 0.0512 - accuracy: 1.0000 - val_loss: 1.0014 - val_accuracy: 0.8000\n",
      "Epoch 61/100\n",
      "2/2 [==============================] - 0s 252ms/step - loss: 0.0508 - accuracy: 1.0000 - val_loss: 0.9996 - val_accuracy: 0.8000\n",
      "Epoch 62/100\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.0503 - accuracy: 1.0000 - val_loss: 0.9977 - val_accuracy: 0.8000\n",
      "Epoch 63/100\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.0498 - accuracy: 1.0000 - val_loss: 0.9958 - val_accuracy: 0.8000\n",
      "Epoch 64/100\n",
      "2/2 [==============================] - 0s 131ms/step - loss: 0.0494 - accuracy: 1.0000 - val_loss: 0.9937 - val_accuracy: 0.8000\n",
      "Epoch 65/100\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 0.0489 - accuracy: 1.0000 - val_loss: 0.9913 - val_accuracy: 0.8000\n",
      "Epoch 66/100\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.0485 - accuracy: 1.0000 - val_loss: 0.9890 - val_accuracy: 0.8000\n",
      "Epoch 67/100\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.0481 - accuracy: 1.0000 - val_loss: 0.9865 - val_accuracy: 0.8000\n",
      "Epoch 68/100\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 0.0476 - accuracy: 1.0000 - val_loss: 0.9836 - val_accuracy: 0.8000\n",
      "Epoch 69/100\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 0.0471 - accuracy: 1.0000 - val_loss: 0.9809 - val_accuracy: 0.8000\n",
      "Epoch 70/100\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 0.0467 - accuracy: 1.0000 - val_loss: 0.9785 - val_accuracy: 0.8000\n",
      "Epoch 71/100\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 0.0463 - accuracy: 1.0000 - val_loss: 0.9762 - val_accuracy: 0.8000\n",
      "Epoch 72/100\n",
      "2/2 [==============================] - 0s 105ms/step - loss: 0.0460 - accuracy: 1.0000 - val_loss: 0.9739 - val_accuracy: 0.8000\n",
      "Epoch 73/100\n",
      "2/2 [==============================] - 0s 118ms/step - loss: 0.0455 - accuracy: 1.0000 - val_loss: 0.9718 - val_accuracy: 0.8000\n",
      "Epoch 74/100\n",
      "2/2 [==============================] - 0s 120ms/step - loss: 0.0452 - accuracy: 1.0000 - val_loss: 0.9698 - val_accuracy: 0.8000\n",
      "Epoch 75/100\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.0447 - accuracy: 1.0000 - val_loss: 0.9679 - val_accuracy: 0.8000\n",
      "Epoch 76/100\n",
      "2/2 [==============================] - 0s 186ms/step - loss: 0.0443 - accuracy: 1.0000 - val_loss: 0.9661 - val_accuracy: 0.8000\n",
      "Epoch 77/100\n",
      "2/2 [==============================] - 0s 106ms/step - loss: 0.0440 - accuracy: 1.0000 - val_loss: 0.9642 - val_accuracy: 0.8000\n",
      "Epoch 78/100\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 0.0436 - accuracy: 1.0000 - val_loss: 0.9623 - val_accuracy: 0.8000\n",
      "Epoch 79/100\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 0.0432 - accuracy: 1.0000 - val_loss: 0.9605 - val_accuracy: 0.8000\n",
      "Epoch 80/100\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 0.0428 - accuracy: 1.0000 - val_loss: 0.9587 - val_accuracy: 0.8000\n",
      "Epoch 81/100\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.0425 - accuracy: 1.0000 - val_loss: 0.9568 - val_accuracy: 0.8000\n",
      "Epoch 82/100\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 0.0421 - accuracy: 1.0000 - val_loss: 0.9550 - val_accuracy: 0.8000\n",
      "Epoch 83/100\n",
      "2/2 [==============================] - 0s 103ms/step - loss: 0.0418 - accuracy: 1.0000 - val_loss: 0.9530 - val_accuracy: 0.8000\n",
      "Epoch 84/100\n",
      "2/2 [==============================] - 0s 121ms/step - loss: 0.0414 - accuracy: 1.0000 - val_loss: 0.9510 - val_accuracy: 0.8000\n",
      "Epoch 85/100\n",
      "2/2 [==============================] - 0s 127ms/step - loss: 0.0411 - accuracy: 1.0000 - val_loss: 0.9490 - val_accuracy: 0.8000\n",
      "Epoch 86/100\n",
      "2/2 [==============================] - 0s 117ms/step - loss: 0.0408 - accuracy: 1.0000 - val_loss: 0.9469 - val_accuracy: 0.8000\n",
      "Epoch 87/100\n",
      "2/2 [==============================] - 0s 105ms/step - loss: 0.0405 - accuracy: 1.0000 - val_loss: 0.9447 - val_accuracy: 0.8000\n",
      "Epoch 88/100\n",
      "2/2 [==============================] - 0s 110ms/step - loss: 0.0401 - accuracy: 1.0000 - val_loss: 0.9424 - val_accuracy: 0.8000\n",
      "Epoch 89/100\n",
      "2/2 [==============================] - 0s 122ms/step - loss: 0.0398 - accuracy: 1.0000 - val_loss: 0.9401 - val_accuracy: 0.8000\n",
      "Epoch 90/100\n",
      "2/2 [==============================] - 0s 121ms/step - loss: 0.0394 - accuracy: 1.0000 - val_loss: 0.9379 - val_accuracy: 0.8000\n",
      "Epoch 91/100\n",
      "2/2 [==============================] - 0s 121ms/step - loss: 0.0391 - accuracy: 1.0000 - val_loss: 0.9358 - val_accuracy: 0.8000\n",
      "Epoch 92/100\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.0388 - accuracy: 1.0000 - val_loss: 0.9337 - val_accuracy: 0.8000\n",
      "Epoch 93/100\n",
      "2/2 [==============================] - 0s 159ms/step - loss: 0.0385 - accuracy: 1.0000 - val_loss: 0.9316 - val_accuracy: 0.8000\n",
      "Epoch 94/100\n",
      "2/2 [==============================] - 0s 116ms/step - loss: 0.0382 - accuracy: 1.0000 - val_loss: 0.9296 - val_accuracy: 0.8000\n",
      "Epoch 95/100\n",
      "2/2 [==============================] - 0s 223ms/step - loss: 0.0380 - accuracy: 1.0000 - val_loss: 0.9275 - val_accuracy: 0.8000\n",
      "Epoch 96/100\n",
      "2/2 [==============================] - 0s 105ms/step - loss: 0.0376 - accuracy: 1.0000 - val_loss: 0.9255 - val_accuracy: 0.8000\n",
      "Epoch 97/100\n",
      "2/2 [==============================] - 0s 114ms/step - loss: 0.0373 - accuracy: 1.0000 - val_loss: 0.9236 - val_accuracy: 0.8000\n",
      "Epoch 98/100\n",
      "2/2 [==============================] - 0s 107ms/step - loss: 0.0370 - accuracy: 1.0000 - val_loss: 0.9216 - val_accuracy: 0.8000\n",
      "Epoch 99/100\n",
      "2/2 [==============================] - 0s 103ms/step - loss: 0.0367 - accuracy: 1.0000 - val_loss: 0.9197 - val_accuracy: 0.8000\n",
      "Epoch 100/100\n",
      "2/2 [==============================] - 0s 110ms/step - loss: 0.0364 - accuracy: 1.0000 - val_loss: 0.9176 - val_accuracy: 0.8000\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.8246 - accuracy: 1.0000\n",
      "Evaluation Metrics for LSTM:\n",
      "loss: 0.8246227502822876\n",
      "accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "def LSTM_model(vocab_size, sequence_length):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=vocab_size, output_dim=128, input_length=sequence_length))\n",
    "    model.add(LSTM(units=128, activation='relu', return_sequences=True))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LSTM(units=64, activation='relu'))\n",
    "    model.add(Dense(256, activation='relu', kernel_regularizer=l2(0.001)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "    optimizer = Adam(learning_rate=0.001)\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "#EMBEDDING_DIM = 110\n",
    "lstm_model = LSTM_model(vocab_size, max_sequence_length)\n",
    "early_stopping_rnn = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "history_updated_rnn = lstm_model.fit(X_train_rnn, y_train_rnn, validation_data=(X_test_rnn, y_test_rnn), epochs=100, batch_size=64, callbacks=[early_stopping_rnn])\n",
    "\n",
    "\n",
    "# Evaluate the lstm model\n",
    "evaluation_metrics_updated_lstm = lstm_model.evaluate(X_test, Y_test)\n",
    "print(\"Evaluation Metrics for LSTM:\")\n",
    "for metric_name, metric_value in zip(lstm_model.metrics_names, evaluation_metrics_updated_lstm):\n",
    "    print(f\"{metric_name}: {metric_value}\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*<h4>TRANSFORMER Model<h4>*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, density, rate=0.1, l2_reg=0.01):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            layers.Dense(density, activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(l2_reg)),\n",
    "            layers.Dense(embed_dim, kernel_regularizer=tf.keras.regularizers.l2(l2_reg)),\n",
    "        ])\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "\n",
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions\n",
    "\n",
    "def build_transformer_model(maxlen, vocab_size, embed_dim, num_heads, density, transformer_units, mlp_units, dropout_rate, num_classes):\n",
    "    inputs = layers.Input(shape=(maxlen,))\n",
    "    embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
    "    x = embedding_layer(inputs)\n",
    "    for _ in range(transformer_units):\n",
    "        x = TransformerBlock(embed_dim, num_heads, density, rate=dropout_rate)(x)\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    for dim in mlp_units:\n",
    "        x = layers.Dense(dim, activation=\"relu\")(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "    outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = max_sequence_length\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "embed_dim = 120\n",
    "num_heads = 2\n",
    "density = 3\n",
    "transformer_units = 4\n",
    "mlp_units = [128]\n",
    "dropout_rate = 0.5\n",
    "num_classes = len(df['grade'].unique())\n",
    "\n",
    "transformer_model = build_transformer_model(maxlen, vocab_size, embed_dim, num_heads, density, transformer_units, mlp_units, dropout_rate, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\amine\\anaconda3\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Epoch 1/30\n",
      "3/3 [==============================] - 18s 910ms/step - loss: 1.6185 - accuracy: 0.4167 - val_loss: 2.1556 - val_accuracy: 0.1333\n",
      "Epoch 2/30\n",
      "3/3 [==============================] - 0s 139ms/step - loss: 1.6443 - accuracy: 0.5000 - val_loss: 1.3530 - val_accuracy: 0.5333\n",
      "Epoch 3/30\n",
      "3/3 [==============================] - 0s 93ms/step - loss: 1.5313 - accuracy: 0.4405 - val_loss: 1.3489 - val_accuracy: 0.6667\n",
      "Epoch 4/30\n",
      "3/3 [==============================] - 0s 91ms/step - loss: 1.4224 - accuracy: 0.5000 - val_loss: 1.4477 - val_accuracy: 0.5333\n",
      "Epoch 5/30\n",
      "3/3 [==============================] - 0s 88ms/step - loss: 1.1548 - accuracy: 0.7381 - val_loss: 1.0106 - val_accuracy: 0.9333\n",
      "Epoch 6/30\n",
      "3/3 [==============================] - 0s 104ms/step - loss: 0.9527 - accuracy: 0.8452 - val_loss: 0.9532 - val_accuracy: 0.8667\n",
      "Epoch 7/30\n",
      "3/3 [==============================] - 0s 116ms/step - loss: 0.7285 - accuracy: 0.9286 - val_loss: 1.0678 - val_accuracy: 0.8667\n",
      "Epoch 8/30\n",
      "3/3 [==============================] - 0s 128ms/step - loss: 0.6057 - accuracy: 0.9167 - val_loss: 0.9261 - val_accuracy: 0.9333\n",
      "Epoch 9/30\n",
      "3/3 [==============================] - 0s 169ms/step - loss: 0.4755 - accuracy: 1.0000 - val_loss: 0.8481 - val_accuracy: 0.9333\n",
      "Epoch 10/30\n",
      "3/3 [==============================] - 0s 95ms/step - loss: 0.4742 - accuracy: 0.9762 - val_loss: 0.8710 - val_accuracy: 0.9333\n",
      "Epoch 11/30\n",
      "3/3 [==============================] - 0s 105ms/step - loss: 0.4237 - accuracy: 1.0000 - val_loss: 0.7419 - val_accuracy: 0.9333\n",
      "Epoch 12/30\n",
      "3/3 [==============================] - 0s 109ms/step - loss: 0.4119 - accuracy: 0.9881 - val_loss: 0.4404 - val_accuracy: 1.0000\n",
      "Epoch 13/30\n",
      "3/3 [==============================] - 0s 109ms/step - loss: 0.3918 - accuracy: 1.0000 - val_loss: 0.4122 - val_accuracy: 1.0000\n",
      "Epoch 14/30\n",
      "3/3 [==============================] - 0s 107ms/step - loss: 0.3807 - accuracy: 1.0000 - val_loss: 0.3712 - val_accuracy: 1.0000\n",
      "Epoch 15/30\n",
      "3/3 [==============================] - 0s 98ms/step - loss: 0.3672 - accuracy: 1.0000 - val_loss: 0.3595 - val_accuracy: 1.0000\n",
      "Epoch 16/30\n",
      "3/3 [==============================] - 0s 113ms/step - loss: 0.3601 - accuracy: 1.0000 - val_loss: 0.3512 - val_accuracy: 1.0000\n",
      "Epoch 17/30\n",
      "3/3 [==============================] - 0s 99ms/step - loss: 0.3566 - accuracy: 1.0000 - val_loss: 0.3430 - val_accuracy: 1.0000\n",
      "Epoch 18/30\n",
      "3/3 [==============================] - 0s 154ms/step - loss: 0.3426 - accuracy: 1.0000 - val_loss: 0.3350 - val_accuracy: 1.0000\n",
      "Epoch 19/30\n",
      "3/3 [==============================] - 0s 110ms/step - loss: 0.3412 - accuracy: 1.0000 - val_loss: 0.3271 - val_accuracy: 1.0000\n",
      "Epoch 20/30\n",
      "3/3 [==============================] - 0s 108ms/step - loss: 0.3269 - accuracy: 1.0000 - val_loss: 0.3194 - val_accuracy: 1.0000\n",
      "Epoch 21/30\n",
      "3/3 [==============================] - 0s 116ms/step - loss: 0.3189 - accuracy: 1.0000 - val_loss: 0.3125 - val_accuracy: 1.0000\n",
      "Epoch 22/30\n",
      "3/3 [==============================] - 0s 111ms/step - loss: 0.3103 - accuracy: 1.0000 - val_loss: 0.3066 - val_accuracy: 1.0000\n",
      "Epoch 23/30\n",
      "3/3 [==============================] - 0s 117ms/step - loss: 0.3053 - accuracy: 1.0000 - val_loss: 0.2986 - val_accuracy: 1.0000\n",
      "Epoch 24/30\n",
      "3/3 [==============================] - 0s 116ms/step - loss: 0.2953 - accuracy: 1.0000 - val_loss: 0.2964 - val_accuracy: 1.0000\n",
      "Epoch 25/30\n",
      "3/3 [==============================] - 0s 112ms/step - loss: 0.2878 - accuracy: 1.0000 - val_loss: 0.3147 - val_accuracy: 1.0000\n",
      "Epoch 26/30\n",
      "3/3 [==============================] - 0s 108ms/step - loss: 0.2850 - accuracy: 1.0000 - val_loss: 0.2902 - val_accuracy: 1.0000\n",
      "Epoch 27/30\n",
      "3/3 [==============================] - 0s 100ms/step - loss: 0.2906 - accuracy: 0.9881 - val_loss: 0.3629 - val_accuracy: 0.9333\n",
      "Epoch 28/30\n",
      "3/3 [==============================] - 0s 107ms/step - loss: 0.2686 - accuracy: 1.0000 - val_loss: 0.4531 - val_accuracy: 0.9333\n",
      "Epoch 29/30\n",
      "3/3 [==============================] - 0s 108ms/step - loss: 0.2612 - accuracy: 1.0000 - val_loss: 0.3432 - val_accuracy: 0.9333\n",
      "Epoch 30/30\n",
      "3/3 [==============================] - 0s 114ms/step - loss: 0.2557 - accuracy: 1.0000 - val_loss: 0.2731 - val_accuracy: 1.0000\n",
      "Epoch 1/30\n",
      "3/3 [==============================] - 0s 145ms/step - loss: 0.2499 - accuracy: 1.0000 - val_loss: 0.2411 - val_accuracy: 1.0000\n",
      "Epoch 2/30\n",
      "3/3 [==============================] - 0s 102ms/step - loss: 0.2402 - accuracy: 1.0000 - val_loss: 0.2342 - val_accuracy: 1.0000\n",
      "Epoch 3/30\n",
      "3/3 [==============================] - 0s 91ms/step - loss: 0.2343 - accuracy: 1.0000 - val_loss: 0.2279 - val_accuracy: 1.0000\n",
      "Epoch 4/30\n",
      "3/3 [==============================] - 0s 130ms/step - loss: 0.2270 - accuracy: 1.0000 - val_loss: 0.2218 - val_accuracy: 1.0000\n",
      "Epoch 5/30\n",
      "3/3 [==============================] - 0s 113ms/step - loss: 0.2203 - accuracy: 1.0000 - val_loss: 0.2158 - val_accuracy: 1.0000\n",
      "Epoch 6/30\n",
      "3/3 [==============================] - 0s 164ms/step - loss: 0.2148 - accuracy: 1.0000 - val_loss: 0.2098 - val_accuracy: 1.0000\n",
      "Epoch 7/30\n",
      "3/3 [==============================] - 0s 118ms/step - loss: 0.2086 - accuracy: 1.0000 - val_loss: 0.2041 - val_accuracy: 1.0000\n",
      "Epoch 8/30\n",
      "3/3 [==============================] - 0s 138ms/step - loss: 0.2028 - accuracy: 1.0000 - val_loss: 0.1984 - val_accuracy: 1.0000\n",
      "Epoch 9/30\n",
      "3/3 [==============================] - 0s 169ms/step - loss: 0.1979 - accuracy: 1.0000 - val_loss: 0.1928 - val_accuracy: 1.0000\n",
      "Epoch 10/30\n",
      "3/3 [==============================] - 0s 142ms/step - loss: 0.1914 - accuracy: 1.0000 - val_loss: 0.1874 - val_accuracy: 1.0000\n",
      "Epoch 11/30\n",
      "3/3 [==============================] - 0s 122ms/step - loss: 0.1859 - accuracy: 1.0000 - val_loss: 0.1822 - val_accuracy: 1.0000\n",
      "Epoch 12/30\n",
      "3/3 [==============================] - 0s 139ms/step - loss: 0.1808 - accuracy: 1.0000 - val_loss: 0.1770 - val_accuracy: 1.0000\n",
      "Epoch 13/30\n",
      "3/3 [==============================] - 0s 187ms/step - loss: 0.1764 - accuracy: 1.0000 - val_loss: 0.1720 - val_accuracy: 1.0000\n",
      "Epoch 14/30\n",
      "3/3 [==============================] - 0s 107ms/step - loss: 0.1706 - accuracy: 1.0000 - val_loss: 0.1672 - val_accuracy: 1.0000\n",
      "Epoch 15/30\n",
      "3/3 [==============================] - 0s 101ms/step - loss: 0.1657 - accuracy: 1.0000 - val_loss: 0.1626 - val_accuracy: 1.0000\n",
      "Epoch 16/30\n",
      "3/3 [==============================] - 0s 103ms/step - loss: 0.1627 - accuracy: 1.0000 - val_loss: 0.1583 - val_accuracy: 1.0000\n",
      "Epoch 17/30\n",
      "3/3 [==============================] - 0s 107ms/step - loss: 0.1559 - accuracy: 1.0000 - val_loss: 0.1544 - val_accuracy: 1.0000\n",
      "Epoch 18/30\n",
      "3/3 [==============================] - 0s 102ms/step - loss: 0.1517 - accuracy: 1.0000 - val_loss: 0.1512 - val_accuracy: 1.0000\n",
      "Epoch 19/30\n",
      "3/3 [==============================] - 0s 102ms/step - loss: 0.1473 - accuracy: 1.0000 - val_loss: 0.1482 - val_accuracy: 1.0000\n",
      "Epoch 20/30\n",
      "3/3 [==============================] - 0s 112ms/step - loss: 0.1430 - accuracy: 1.0000 - val_loss: 0.1448 - val_accuracy: 1.0000\n",
      "Epoch 21/30\n",
      "3/3 [==============================] - 0s 155ms/step - loss: 0.1390 - accuracy: 1.0000 - val_loss: 0.1399 - val_accuracy: 1.0000\n",
      "Epoch 22/30\n",
      "3/3 [==============================] - 0s 107ms/step - loss: 0.1353 - accuracy: 1.0000 - val_loss: 0.1350 - val_accuracy: 1.0000\n",
      "Epoch 23/30\n",
      "3/3 [==============================] - 0s 92ms/step - loss: 0.1306 - accuracy: 1.0000 - val_loss: 0.1304 - val_accuracy: 1.0000\n",
      "Epoch 24/30\n",
      "3/3 [==============================] - 0s 94ms/step - loss: 0.1271 - accuracy: 1.0000 - val_loss: 0.1263 - val_accuracy: 1.0000\n",
      "Epoch 25/30\n",
      "3/3 [==============================] - 0s 95ms/step - loss: 0.1228 - accuracy: 1.0000 - val_loss: 0.1221 - val_accuracy: 1.0000\n",
      "Epoch 26/30\n",
      "3/3 [==============================] - 0s 107ms/step - loss: 0.1189 - accuracy: 1.0000 - val_loss: 0.1181 - val_accuracy: 1.0000\n",
      "Epoch 27/30\n",
      "3/3 [==============================] - 0s 136ms/step - loss: 0.1153 - accuracy: 1.0000 - val_loss: 0.1143 - val_accuracy: 1.0000\n",
      "Epoch 28/30\n",
      "3/3 [==============================] - 0s 116ms/step - loss: 0.1118 - accuracy: 1.0000 - val_loss: 0.1107 - val_accuracy: 1.0000\n",
      "Epoch 29/30\n",
      "3/3 [==============================] - 0s 102ms/step - loss: 0.1092 - accuracy: 1.0000 - val_loss: 0.1073 - val_accuracy: 1.0000\n",
      "Epoch 30/30\n",
      "3/3 [==============================] - 0s 100ms/step - loss: 0.1055 - accuracy: 1.0000 - val_loss: 0.1039 - val_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "transformer_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "transformer_model.fit(X_train, Y_train, epochs=30, batch_size=32, validation_data=(X_test, Y_test))\n",
    "history = transformer_model.fit(X_train, Y_train, epochs=30, batch_size=32, validation_data=(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 99ms/step - loss: 0.1039 - accuracy: 1.0000\n",
      "Evaluation Metrics Transformer:\n",
      "loss: 0.10390578210353851\n",
      "accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "evaluation_metrics_transformer = transformer_model.evaluate(X_test, Y_test)\n",
    "print(\"Evaluation Metrics Transformer:\")\n",
    "for metric_name, metric_value in zip(transformer_model.metrics_names, evaluation_metrics_transformer):\n",
    "    print(f\"{metric_name}: {metric_value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 97ms/step - loss: 0.1039 - accuracy: 1.0000\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   5\n",
      "   15  12   1  65  50   2  16   6   7]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0  11   5  15   8   4   9\n",
      "    1  14   2  21  54 195  27   6   7]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  27  41  44   3\n",
      "   13  12  79  31  26  14  42   3  60]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  38   5\n",
      "   39   8   4   9   1  14   2  10  18]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   2  22   6  19   7  47\n",
      "    3   1  11   5  15   8   4   9   1]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  85 253  28\n",
      "   24  23 254 255  23  11  16   7   2]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0  62  48   7 225  20  29  25\n",
      "   30  45  21  47   3  13  64  40  89]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0 196  20  29  25  30\n",
      "    3 197 198 199  17  76  13   6   7]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0  23  42  37   3  13  17   2  73]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   80  19  49  50  45  10  18   6   7]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   2  16   6   7\n",
      "    1  11   5  15  53   8   4   9   1]\n",
      " [  0   0  43  38   5  39  14   2  21  44   3  13  12  79  31  26   3 112\n",
      "   10  28  24  33   4  34   1  12  35]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0 208  11   5  15   8   4   9   1]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0  11   5  15 180 181 182\n",
      "  183   4   9   1 184   2  16   6   7]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0  37  23  49  41 200\n",
      "    3  71  17  63   2  16  68   3  13]]\n",
      "\n",
      "Real and Predicted Values:\n",
      "    Real  Predicted\n",
      "0      1          1\n",
      "1      2          2\n",
      "2      2          2\n",
      "3      0          0\n",
      "4      2          2\n",
      "5      2          2\n",
      "6      1          1\n",
      "7      1          1\n",
      "8      1          1\n",
      "9      1          1\n",
      "10     2          2\n",
      "11     0          0\n",
      "12     2          2\n",
      "13     2          2\n",
      "14     1          1\n"
     ]
    }
   ],
   "source": [
    "evaluation_metrics_transformer = transformer_model.evaluate(X_test, Y_test)\n",
    "\n",
    "predictions = transformer_model.predict(X_test)\n",
    "print(X_test)\n",
    "\n",
    "# Convert one-hot encoded labels to class indices\n",
    "y_true = np.argmax(Y_test, axis=1)\n",
    "\n",
    "# Convert predicted probabilities to class indices\n",
    "y_pred = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Create DataFrame\n",
    "results_df = pd.DataFrame({\"Real\": y_true, \"Predicted\": y_pred})\n",
    "\n",
    "# Display DataFrame\n",
    "print(\"\\nReal and Predicted Values:\")\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy RNN: 0.9333333373069763\n",
      "Accuracy LSTM: 1.0\n",
      "Accuracy Transformer: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Evaluate RNN model\n",
    "rnn_accuracy = rnn_model.evaluate(X_test, Y_test, verbose=0)[1]\n",
    "print(\"Accuracy RNN:\", rnn_accuracy)\n",
    "\n",
    "# Evaluate LSTM model\n",
    "lstm_accuracy = lstm_model.evaluate(X_test, Y_test, verbose=0)[1]\n",
    "print(\"Accuracy LSTM:\", lstm_accuracy)\n",
    "\n",
    "# Evaluate Transformer model\n",
    "transformer_accuracy = transformer_model.evaluate(X_test, Y_test, verbose=0)[1]\n",
    "print(\"Accuracy Transformer:\", transformer_accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model (LSTM) with accuracy 1.0 has been saved to './savedModels/q5_model.h5'\n"
     ]
    }
   ],
   "source": [
    "# Determine the best model\n",
    "best_model_name, best_model_accuracy = max([('RNN', rnn_accuracy), ('LSTM', lstm_accuracy), ('Transformer', transformer_accuracy)], key=lambda x: x[1])\n",
    "\n",
    "save_path = './savedModels/q5_model.h5'\n",
    "# Save the best model\n",
    "if best_model_name == 'RNN':\n",
    "    rnn_model.save(save_path)\n",
    "elif best_model_name == 'LSTM':\n",
    "    lstm_model.save(save_path)\n",
    "elif best_model_name == 'Transformer':\n",
    "    transformer_model.save(save_path)\n",
    "\n",
    "print(f\"The best model ({best_model_name}) with accuracy {best_model_accuracy} has been saved to '{save_path}'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
