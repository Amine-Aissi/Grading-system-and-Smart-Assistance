{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\amine\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow as keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.regularizers import l2\n",
    "from keras import layers\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Embedding, SimpleRNN, Dense, Dropout, LSTM, Bidirectional, Input\n",
    "from keras.layers import Attention, GlobalAveragePooling1D\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import stanza\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import BatchNormalization, Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*<h3>Load Data<h3>*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_id</th>\n",
       "      <th>answer</th>\n",
       "      <th>grade</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>زوجة الرسول الكريم.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>خديجة بنت خويلد.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>زوجة النبي.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>المرأة الأولى التي أسلمت هي خديجة بنت خويلد.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>خديجة الكبرى هي أول امرأة آمنت بالنبي محمد صلى...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>أول امرأة أسلمت كانت خديجة بنت خويلد (رضي الله...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>أول من آمنت برسالة النبي محمد هي خديجة بنت خويلد.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6</td>\n",
       "      <td>الأمرأة الأولى التي أسلمت هي خديجة بنت خويلد (...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>6</td>\n",
       "      <td>خديجة بنت خويلد هي المرأة الأولى التي أعلنت إس...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>6</td>\n",
       "      <td>خديجة بنت خويلد هي أول امرأة أعلنت إسلامها.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   question_id                                             answer  grade\n",
       "0            6                                زوجة الرسول الكريم.      1\n",
       "1            6                                   خديجة بنت خويلد.      2\n",
       "2            6                                        زوجة النبي.      1\n",
       "3            6       المرأة الأولى التي أسلمت هي خديجة بنت خويلد.      2\n",
       "4            6  خديجة الكبرى هي أول امرأة آمنت بالنبي محمد صلى...      2\n",
       "5            6  أول امرأة أسلمت كانت خديجة بنت خويلد (رضي الله...      2\n",
       "6            6  أول من آمنت برسالة النبي محمد هي خديجة بنت خويلد.      2\n",
       "7            6  الأمرأة الأولى التي أسلمت هي خديجة بنت خويلد (...      2\n",
       "8            6  خديجة بنت خويلد هي المرأة الأولى التي أعلنت إس...      2\n",
       "9            6        خديجة بنت خويلد هي أول امرأة أعلنت إسلامها.      2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read csv file into pandas\n",
    "df = pd.read_csv(\"../datasets/shuffled6.csv\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*<h3>EDA<h3>*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100 entries, 0 to 99\n",
      "Data columns (total 3 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   question_id  100 non-null    int64 \n",
      " 1   answer       100 non-null    object\n",
      " 2   grade        100 non-null    int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 2.5+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "grade\n",
       "0    28\n",
       "1    33\n",
       "2    39\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('grade').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(data=df, x='grade')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*<h3>Data Cleaning<h3>*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('question_id', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                                   زوجة الرسول الكريم.\n",
      "1                                      خديجة بنت خويلد.\n",
      "2                                           زوجة النبي.\n",
      "3          المرأة الأولى التي أسلمت هي خديجة بنت خويلد.\n",
      "4     خديجة الكبرى هي أول امرأة آمنت بالنبي محمد صلى...\n",
      "                            ...                        \n",
      "95                       زينب بنت خزيمة -رضي الله عنها-\n",
      "96                                          أم المؤمنين\n",
      "97                                           بنت خويلد.\n",
      "98    أول امرأة قبلت الإسلام كانت خديجة بنت خويلد رض...\n",
      "99                        سودة بنت زمعة -رضي الله عنها-\n",
      "Name: answer, Length: 99, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df.drop_duplicates(inplace=True)\n",
    "print(df['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*<h3>Data Pre-Preocessing<h3>*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e477eb6e9d141f599d68d69c2dbc3f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.7.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-09 22:54:02 INFO: Downloading default packages for language: ar (Arabic) ...\n",
      "2024-01-09 22:54:03 INFO: File exists: C:\\Users\\amine\\stanza_resources\\ar\\default.zip\n",
      "2024-01-09 22:54:08 INFO: Finished downloading models and saved to C:\\Users\\amine\\stanza_resources.\n",
      "2024-01-09 22:54:08 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fec6078f45194704b239928783ec4ef6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.7.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-09 22:54:10 INFO: Loading these models for language: ar (Arabic):\n",
      "=============================\n",
      "| Processor | Package       |\n",
      "-----------------------------\n",
      "| tokenize  | padt          |\n",
      "| mwt       | padt          |\n",
      "| pos       | padt_charlm   |\n",
      "| lemma     | padt_nocharlm |\n",
      "| depparse  | padt_charlm   |\n",
      "| ner       | aqmar_charlm  |\n",
      "=============================\n",
      "\n",
      "2024-01-09 22:54:10 INFO: Using device: cpu\n",
      "2024-01-09 22:54:10 INFO: Loading: tokenize\n",
      "2024-01-09 22:54:12 INFO: Loading: mwt\n",
      "2024-01-09 22:54:12 INFO: Loading: pos\n",
      "2024-01-09 22:54:12 INFO: Loading: lemma\n",
      "2024-01-09 22:54:12 INFO: Loading: depparse\n",
      "2024-01-09 22:54:13 INFO: Loading: ner\n",
      "2024-01-09 22:54:14 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "df['grade'] = le.fit_transform(df['grade'])\n",
    "\n",
    "stanza.download('ar')\n",
    "nlp = stanza.Pipeline('ar')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    doc = nlp(text)\n",
    "    tokens = [word.lemma for sent in doc.sentences for word in sent.words if word.upos != 'PUNCT']\n",
    "    return tokens\n",
    "\n",
    "df['answer'] = df['answer'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0 ... 10 18 26]\n",
      " [ 0  0  0 ...  5  2  8]\n",
      " [ 0  0  0 ...  0 10 11]\n",
      " ...\n",
      " [ 0  0  0 ...  0  2  8]\n",
      " [ 0  0  0 ...  4  7  1]\n",
      " [ 0  0  0 ...  4  7  1]]\n",
      "0                              [زَوجَة, رَسُول, كَرِيم]\n",
      "1                                   [خديجة, بنت, خويلد]\n",
      "2                                      [زَوجَة, نَبِيّ]\n",
      "3     [اِمرَأَة, أَوَّل, اَلَّذِي, أَسلَم, هُوَ, خدي...\n",
      "4     [خديجة, الكبرى, هُوَ, أَوَّل, اِمرَأَة, أَمَّن...\n",
      "                            ...                        \n",
      "95             [زينب, بنت, خزيمة, رضي, الله, عَن, هُوَ]\n",
      "96                                      [أَم, مُؤَمِّن]\n",
      "97                                         [بنت, خويلد]\n",
      "98    [أَوَّل, اِمرَأَة, قَبِل, إِسلَام, كَان, خديجة...\n",
      "99              [سودة, بنت, زمعة, رضي, الله, عَن, هُوَ]\n",
      "Name: answer, Length: 99, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Tokenization\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df['answer'])\n",
    "sequences = tokenizer.texts_to_sequences(df['answer'])\n",
    "max_sequence_length = max(len(s) for s in sequences)\n",
    "sequences = pad_sequences(sequences,max_sequence_length)\n",
    "word2idx = tokenizer.word_index\n",
    "vocab_size = len(word2idx) + 1\n",
    "\n",
    "\n",
    "X = pad_sequences(sequences, padding='post', truncating='post', maxlen=max_sequence_length)\n",
    "\n",
    "print(sequences)\n",
    "print(df['answer'])\n",
    "\n",
    "Y = to_categorical(df['grade'], num_classes=3)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.15, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*<h3>build Models<h3>*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*<h4>RNN Model<h4>*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\amine\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:From c:\\Users\\amine\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\amine\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "2/2 [==============================] - 7s 950ms/step - loss: 1.2667 - accuracy: 0.3924 - val_loss: 1.1821 - val_accuracy: 0.4500 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 1.1789 - accuracy: 0.4304 - val_loss: 1.1711 - val_accuracy: 0.4500 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 1.2132 - accuracy: 0.3544 - val_loss: 1.1689 - val_accuracy: 0.4500 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 1.0870 - accuracy: 0.5570 - val_loss: 1.1662 - val_accuracy: 0.4500 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 1.1004 - accuracy: 0.4177 - val_loss: 1.1620 - val_accuracy: 0.4500 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 1.0096 - accuracy: 0.5443 - val_loss: 1.1583 - val_accuracy: 0.4500 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 0.9882 - accuracy: 0.5316 - val_loss: 1.1532 - val_accuracy: 0.4500 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.8949 - accuracy: 0.7089 - val_loss: 1.1472 - val_accuracy: 0.4500 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.8475 - accuracy: 0.7722 - val_loss: 1.1384 - val_accuracy: 0.4500 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 0.7852 - accuracy: 0.7722 - val_loss: 1.1230 - val_accuracy: 0.4500 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.7006 - accuracy: 0.8734 - val_loss: 1.1084 - val_accuracy: 0.4500 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 0.6344 - accuracy: 0.9114 - val_loss: 1.0948 - val_accuracy: 0.4500 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.5709 - accuracy: 0.9114 - val_loss: 1.0788 - val_accuracy: 0.4500 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 0.5297 - accuracy: 0.9494 - val_loss: 1.0583 - val_accuracy: 0.5500 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.4748 - accuracy: 0.9494 - val_loss: 1.0312 - val_accuracy: 0.5500 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.4006 - accuracy: 0.9747 - val_loss: 1.0162 - val_accuracy: 0.6000 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.3567 - accuracy: 0.9873 - val_loss: 0.9993 - val_accuracy: 0.7000 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.3204 - accuracy: 0.9873 - val_loss: 0.9889 - val_accuracy: 0.7000 - lr: 0.0010\n",
      "Epoch 19/100\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.2814 - accuracy: 0.9873 - val_loss: 0.9970 - val_accuracy: 0.7000 - lr: 0.0010\n",
      "Epoch 20/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.2924 - accuracy: 0.9844\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 0.2891 - accuracy: 0.9873 - val_loss: 0.9926 - val_accuracy: 0.7000 - lr: 0.0010\n",
      "Epoch 21/100\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 0.2372 - accuracy: 0.9873 - val_loss: 0.9900 - val_accuracy: 0.7000 - lr: 2.0000e-04\n",
      "Epoch 22/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.2238 - accuracy: 1.0000\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 0.0001.\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 0.2231 - accuracy: 1.0000 - val_loss: 0.9912 - val_accuracy: 0.7000 - lr: 2.0000e-04\n",
      "Epoch 23/100\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.2216 - accuracy: 0.9873 - val_loss: 0.9897 - val_accuracy: 0.7000 - lr: 1.0000e-04\n",
      "Epoch 24/100\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.2232 - accuracy: 0.9873 - val_loss: 0.9871 - val_accuracy: 0.7000 - lr: 1.0000e-04\n",
      "Epoch 25/100\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 0.2296 - accuracy: 0.9873 - val_loss: 0.9834 - val_accuracy: 0.7000 - lr: 1.0000e-04\n",
      "Epoch 26/100\n",
      "2/2 [==============================] - 0s 167ms/step - loss: 0.2233 - accuracy: 0.9873 - val_loss: 0.9790 - val_accuracy: 0.7000 - lr: 1.0000e-04\n",
      "Epoch 27/100\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.2169 - accuracy: 0.9873 - val_loss: 0.9741 - val_accuracy: 0.7000 - lr: 1.0000e-04\n",
      "Epoch 28/100\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.1984 - accuracy: 1.0000 - val_loss: 0.9683 - val_accuracy: 0.7000 - lr: 1.0000e-04\n",
      "Epoch 29/100\n",
      "2/2 [==============================] - 0s 98ms/step - loss: 0.2136 - accuracy: 1.0000 - val_loss: 0.9621 - val_accuracy: 0.7000 - lr: 1.0000e-04\n",
      "Epoch 30/100\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.2134 - accuracy: 1.0000 - val_loss: 0.9562 - val_accuracy: 0.7000 - lr: 1.0000e-04\n",
      "Epoch 31/100\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 0.2138 - accuracy: 1.0000 - val_loss: 0.9504 - val_accuracy: 0.7500 - lr: 1.0000e-04\n",
      "Epoch 32/100\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 0.2060 - accuracy: 1.0000 - val_loss: 0.9447 - val_accuracy: 0.7500 - lr: 1.0000e-04\n",
      "Epoch 33/100\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 0.2094 - accuracy: 1.0000 - val_loss: 0.9388 - val_accuracy: 0.7000 - lr: 1.0000e-04\n",
      "Epoch 34/100\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 0.2157 - accuracy: 0.9873 - val_loss: 0.9322 - val_accuracy: 0.7000 - lr: 1.0000e-04\n",
      "Epoch 35/100\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.2021 - accuracy: 1.0000 - val_loss: 0.9252 - val_accuracy: 0.7000 - lr: 1.0000e-04\n",
      "Epoch 36/100\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 0.2019 - accuracy: 1.0000 - val_loss: 0.9185 - val_accuracy: 0.7000 - lr: 1.0000e-04\n",
      "Epoch 37/100\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.1991 - accuracy: 1.0000 - val_loss: 0.9123 - val_accuracy: 0.7000 - lr: 1.0000e-04\n",
      "Epoch 38/100\n",
      "2/2 [==============================] - 0s 213ms/step - loss: 0.1880 - accuracy: 1.0000 - val_loss: 0.9061 - val_accuracy: 0.7000 - lr: 1.0000e-04\n",
      "Epoch 39/100\n",
      "2/2 [==============================] - 0s 203ms/step - loss: 0.2038 - accuracy: 0.9873 - val_loss: 0.9007 - val_accuracy: 0.7000 - lr: 1.0000e-04\n",
      "Epoch 40/100\n",
      "2/2 [==============================] - 0s 110ms/step - loss: 0.1939 - accuracy: 1.0000 - val_loss: 0.8960 - val_accuracy: 0.7000 - lr: 1.0000e-04\n",
      "Epoch 41/100\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.2004 - accuracy: 1.0000 - val_loss: 0.8913 - val_accuracy: 0.7000 - lr: 1.0000e-04\n",
      "Epoch 42/100\n",
      "2/2 [==============================] - 0s 115ms/step - loss: 0.1957 - accuracy: 1.0000 - val_loss: 0.8864 - val_accuracy: 0.7000 - lr: 1.0000e-04\n",
      "Epoch 43/100\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 0.1912 - accuracy: 1.0000 - val_loss: 0.8814 - val_accuracy: 0.7000 - lr: 1.0000e-04\n",
      "Epoch 44/100\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.1931 - accuracy: 1.0000 - val_loss: 0.8753 - val_accuracy: 0.7000 - lr: 1.0000e-04\n",
      "Epoch 45/100\n",
      "2/2 [==============================] - 0s 110ms/step - loss: 0.1861 - accuracy: 1.0000 - val_loss: 0.8688 - val_accuracy: 0.7000 - lr: 1.0000e-04\n",
      "Epoch 46/100\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 0.1906 - accuracy: 1.0000 - val_loss: 0.8624 - val_accuracy: 0.7000 - lr: 1.0000e-04\n",
      "Epoch 47/100\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.1772 - accuracy: 1.0000 - val_loss: 0.8565 - val_accuracy: 0.7000 - lr: 1.0000e-04\n",
      "Epoch 48/100\n",
      "2/2 [==============================] - 0s 104ms/step - loss: 0.1882 - accuracy: 1.0000 - val_loss: 0.8508 - val_accuracy: 0.7000 - lr: 1.0000e-04\n",
      "Epoch 49/100\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 0.1738 - accuracy: 1.0000 - val_loss: 0.8452 - val_accuracy: 0.7000 - lr: 1.0000e-04\n",
      "Epoch 50/100\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 0.1822 - accuracy: 1.0000 - val_loss: 0.8399 - val_accuracy: 0.7000 - lr: 1.0000e-04\n",
      "Epoch 51/100\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.1853 - accuracy: 1.0000 - val_loss: 0.8348 - val_accuracy: 0.7000 - lr: 1.0000e-04\n",
      "Epoch 52/100\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 0.1718 - accuracy: 1.0000 - val_loss: 0.8297 - val_accuracy: 0.7500 - lr: 1.0000e-04\n",
      "Epoch 53/100\n",
      "2/2 [==============================] - 0s 181ms/step - loss: 0.1737 - accuracy: 1.0000 - val_loss: 0.8244 - val_accuracy: 0.7500 - lr: 1.0000e-04\n",
      "Epoch 54/100\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 0.1730 - accuracy: 1.0000 - val_loss: 0.8191 - val_accuracy: 0.7500 - lr: 1.0000e-04\n",
      "Epoch 55/100\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 0.1722 - accuracy: 1.0000 - val_loss: 0.8137 - val_accuracy: 0.7500 - lr: 1.0000e-04\n",
      "Epoch 56/100\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 0.1798 - accuracy: 1.0000 - val_loss: 0.8083 - val_accuracy: 0.7500 - lr: 1.0000e-04\n",
      "Epoch 57/100\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.1766 - accuracy: 1.0000 - val_loss: 0.8029 - val_accuracy: 0.7500 - lr: 1.0000e-04\n",
      "Epoch 58/100\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 0.1741 - accuracy: 1.0000 - val_loss: 0.7977 - val_accuracy: 0.7500 - lr: 1.0000e-04\n",
      "Epoch 59/100\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 0.1742 - accuracy: 1.0000 - val_loss: 0.7921 - val_accuracy: 0.7500 - lr: 1.0000e-04\n",
      "Epoch 60/100\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.1752 - accuracy: 1.0000 - val_loss: 0.7870 - val_accuracy: 0.7500 - lr: 1.0000e-04\n",
      "Epoch 61/100\n",
      "2/2 [==============================] - 0s 103ms/step - loss: 0.1713 - accuracy: 1.0000 - val_loss: 0.7827 - val_accuracy: 0.7500 - lr: 1.0000e-04\n",
      "Epoch 62/100\n",
      "2/2 [==============================] - 0s 124ms/step - loss: 0.1723 - accuracy: 1.0000 - val_loss: 0.7783 - val_accuracy: 0.7500 - lr: 1.0000e-04\n",
      "Epoch 63/100\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 0.1655 - accuracy: 1.0000 - val_loss: 0.7737 - val_accuracy: 0.7500 - lr: 1.0000e-04\n",
      "Epoch 64/100\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.1668 - accuracy: 1.0000 - val_loss: 0.7692 - val_accuracy: 0.7500 - lr: 1.0000e-04\n",
      "Epoch 65/100\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 0.1732 - accuracy: 1.0000 - val_loss: 0.7645 - val_accuracy: 0.8000 - lr: 1.0000e-04\n",
      "Epoch 66/100\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.1694 - accuracy: 1.0000 - val_loss: 0.7594 - val_accuracy: 0.8000 - lr: 1.0000e-04\n",
      "Epoch 67/100\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 0.1727 - accuracy: 1.0000 - val_loss: 0.7539 - val_accuracy: 0.8000 - lr: 1.0000e-04\n",
      "Epoch 68/100\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 0.1666 - accuracy: 1.0000 - val_loss: 0.7484 - val_accuracy: 0.8000 - lr: 1.0000e-04\n",
      "Epoch 69/100\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.1735 - accuracy: 1.0000 - val_loss: 0.7428 - val_accuracy: 0.8000 - lr: 1.0000e-04\n",
      "Epoch 70/100\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.1628 - accuracy: 1.0000 - val_loss: 0.7374 - val_accuracy: 0.8000 - lr: 1.0000e-04\n",
      "Epoch 71/100\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.1643 - accuracy: 1.0000 - val_loss: 0.7318 - val_accuracy: 0.8000 - lr: 1.0000e-04\n",
      "Epoch 72/100\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 0.1609 - accuracy: 1.0000 - val_loss: 0.7262 - val_accuracy: 0.8000 - lr: 1.0000e-04\n",
      "Epoch 73/100\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 0.1623 - accuracy: 1.0000 - val_loss: 0.7205 - val_accuracy: 0.8000 - lr: 1.0000e-04\n",
      "Epoch 74/100\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.1598 - accuracy: 1.0000 - val_loss: 0.7152 - val_accuracy: 0.8000 - lr: 1.0000e-04\n",
      "Epoch 75/100\n",
      "2/2 [==============================] - 0s 190ms/step - loss: 0.1612 - accuracy: 1.0000 - val_loss: 0.7106 - val_accuracy: 0.8000 - lr: 1.0000e-04\n",
      "Epoch 76/100\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.1615 - accuracy: 1.0000 - val_loss: 0.7064 - val_accuracy: 0.8000 - lr: 1.0000e-04\n",
      "Epoch 77/100\n",
      "2/2 [==============================] - 0s 106ms/step - loss: 0.1565 - accuracy: 1.0000 - val_loss: 0.7022 - val_accuracy: 0.8000 - lr: 1.0000e-04\n",
      "Epoch 78/100\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 0.1563 - accuracy: 1.0000 - val_loss: 0.6979 - val_accuracy: 0.8000 - lr: 1.0000e-04\n",
      "Epoch 79/100\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 0.1635 - accuracy: 1.0000 - val_loss: 0.6927 - val_accuracy: 0.8000 - lr: 1.0000e-04\n",
      "Epoch 80/100\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.1630 - accuracy: 1.0000 - val_loss: 0.6867 - val_accuracy: 0.8000 - lr: 1.0000e-04\n",
      "Epoch 81/100\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.1608 - accuracy: 1.0000 - val_loss: 0.6808 - val_accuracy: 0.8000 - lr: 1.0000e-04\n",
      "Epoch 82/100\n",
      "2/2 [==============================] - 0s 103ms/step - loss: 0.1589 - accuracy: 1.0000 - val_loss: 0.6753 - val_accuracy: 0.8000 - lr: 1.0000e-04\n",
      "Epoch 83/100\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.1584 - accuracy: 1.0000 - val_loss: 0.6701 - val_accuracy: 0.8000 - lr: 1.0000e-04\n",
      "Epoch 84/100\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.1550 - accuracy: 1.0000 - val_loss: 0.6647 - val_accuracy: 0.8000 - lr: 1.0000e-04\n",
      "Epoch 85/100\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.1521 - accuracy: 1.0000 - val_loss: 0.6591 - val_accuracy: 0.8000 - lr: 1.0000e-04\n",
      "Epoch 86/100\n",
      "2/2 [==============================] - 0s 127ms/step - loss: 0.1523 - accuracy: 1.0000 - val_loss: 0.6536 - val_accuracy: 0.8000 - lr: 1.0000e-04\n",
      "Epoch 87/100\n",
      "2/2 [==============================] - 0s 370ms/step - loss: 0.1535 - accuracy: 1.0000 - val_loss: 0.6483 - val_accuracy: 0.8000 - lr: 1.0000e-04\n",
      "Epoch 88/100\n",
      "2/2 [==============================] - 0s 160ms/step - loss: 0.1616 - accuracy: 1.0000 - val_loss: 0.6425 - val_accuracy: 0.8000 - lr: 1.0000e-04\n",
      "Epoch 89/100\n",
      "2/2 [==============================] - 0s 113ms/step - loss: 0.1511 - accuracy: 1.0000 - val_loss: 0.6362 - val_accuracy: 0.8000 - lr: 1.0000e-04\n",
      "Epoch 90/100\n",
      "2/2 [==============================] - 0s 113ms/step - loss: 0.1527 - accuracy: 1.0000 - val_loss: 0.6299 - val_accuracy: 0.8000 - lr: 1.0000e-04\n",
      "Epoch 91/100\n",
      "2/2 [==============================] - 0s 151ms/step - loss: 0.1588 - accuracy: 1.0000 - val_loss: 0.6235 - val_accuracy: 0.8000 - lr: 1.0000e-04\n",
      "Epoch 92/100\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.1549 - accuracy: 1.0000 - val_loss: 0.6173 - val_accuracy: 0.8000 - lr: 1.0000e-04\n",
      "Epoch 93/100\n",
      "2/2 [==============================] - 0s 120ms/step - loss: 0.1605 - accuracy: 1.0000 - val_loss: 0.6114 - val_accuracy: 0.8000 - lr: 1.0000e-04\n",
      "Epoch 94/100\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.1460 - accuracy: 1.0000 - val_loss: 0.6058 - val_accuracy: 0.8000 - lr: 1.0000e-04\n",
      "Epoch 95/100\n",
      "2/2 [==============================] - 0s 107ms/step - loss: 0.1525 - accuracy: 1.0000 - val_loss: 0.6002 - val_accuracy: 0.8000 - lr: 1.0000e-04\n",
      "Epoch 96/100\n",
      "2/2 [==============================] - 0s 109ms/step - loss: 0.1436 - accuracy: 1.0000 - val_loss: 0.5950 - val_accuracy: 0.8000 - lr: 1.0000e-04\n",
      "Epoch 97/100\n",
      "2/2 [==============================] - 0s 125ms/step - loss: 0.1443 - accuracy: 1.0000 - val_loss: 0.5901 - val_accuracy: 0.8000 - lr: 1.0000e-04\n",
      "Epoch 98/100\n",
      "2/2 [==============================] - 0s 229ms/step - loss: 0.1526 - accuracy: 1.0000 - val_loss: 0.5862 - val_accuracy: 0.8000 - lr: 1.0000e-04\n",
      "Epoch 99/100\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 0.1407 - accuracy: 1.0000 - val_loss: 0.5827 - val_accuracy: 0.8000 - lr: 1.0000e-04\n",
      "Epoch 100/100\n",
      "2/2 [==============================] - 0s 103ms/step - loss: 0.1465 - accuracy: 1.0000 - val_loss: 0.5790 - val_accuracy: 0.8000 - lr: 1.0000e-04\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.4137 - accuracy: 1.0000\n",
      "Evaluation Metrics for RNN:\n",
      "loss: 0.4137016832828522\n",
      "accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "\n",
    "X_train_rnn, X_test_rnn, y_train_rnn, y_test_rnn = train_test_split(X, Y, test_size=0.2)\n",
    "\n",
    "def RNN_model(vocab_size, sequence_length):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=vocab_size, output_dim=128, input_length=sequence_length))\n",
    "    model.add(SimpleRNN(units=128, activation='relu', return_sequences=True))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(SimpleRNN(units=64, activation='sigmoid'))\n",
    "    model.add(Dense(256, activation='sigmoid', kernel_regularizer=l2(0.001)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "    optimizer = Adam(learning_rate=0.001)\n",
    "    \n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=0.0001, verbose=1)\n",
    "#EMBEDDING_DIM = 110\n",
    "rnn_model = RNN_model(vocab_size, max_sequence_length)\n",
    "early_stopping_rnn = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "history_updated_rnn = rnn_model.fit(X_train_rnn, y_train_rnn, validation_data=(X_test_rnn, y_test_rnn), epochs=100, batch_size=64, callbacks=[reduce_lr])\n",
    "\n",
    "# Evaluate the RNN model\n",
    "evaluation_metrics_updated_rnn = rnn_model.evaluate(X_test, Y_test)\n",
    "print(\"Evaluation Metrics for RNN:\")\n",
    "for metric_name, metric_value in zip(rnn_model.metrics_names, evaluation_metrics_updated_rnn):\n",
    "    print(f\"{metric_name}: {metric_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*<h4>LSTM Model<h4>*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2/2 [==============================] - 9s 1s/step - loss: 1.2000 - accuracy: 0.3544 - val_loss: 1.1951 - val_accuracy: 0.6000\n",
      "Epoch 2/100\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 1.1187 - accuracy: 0.6329 - val_loss: 1.1894 - val_accuracy: 0.6000\n",
      "Epoch 3/100\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 1.0206 - accuracy: 0.5823 - val_loss: 1.1819 - val_accuracy: 0.6000\n",
      "Epoch 4/100\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 0.9299 - accuracy: 0.5823 - val_loss: 1.1779 - val_accuracy: 0.4500\n",
      "Epoch 5/100\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.8281 - accuracy: 0.6962 - val_loss: 1.1788 - val_accuracy: 0.3500\n",
      "Epoch 6/100\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 0.7597 - accuracy: 0.7342 - val_loss: 1.1790 - val_accuracy: 0.3000\n",
      "Epoch 7/100\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.6696 - accuracy: 0.7595 - val_loss: 1.1764 - val_accuracy: 0.3000\n",
      "Epoch 8/100\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 0.5419 - accuracy: 0.8101 - val_loss: 1.1720 - val_accuracy: 0.5000\n",
      "Epoch 9/100\n",
      "2/2 [==============================] - 0s 105ms/step - loss: 0.4965 - accuracy: 0.8608 - val_loss: 1.1605 - val_accuracy: 0.6500\n",
      "Epoch 10/100\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 0.4239 - accuracy: 0.8861 - val_loss: 1.1476 - val_accuracy: 0.7000\n",
      "Epoch 11/100\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 0.3482 - accuracy: 0.9114 - val_loss: 1.1366 - val_accuracy: 0.8000\n",
      "Epoch 12/100\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 0.2991 - accuracy: 0.9367 - val_loss: 1.1250 - val_accuracy: 0.8000\n",
      "Epoch 13/100\n",
      "2/2 [==============================] - 0s 103ms/step - loss: 0.2306 - accuracy: 0.9494 - val_loss: 1.1192 - val_accuracy: 0.6000\n",
      "Epoch 14/100\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 0.1660 - accuracy: 0.9620 - val_loss: 1.1195 - val_accuracy: 0.4500\n",
      "Epoch 15/100\n",
      "2/2 [==============================] - 0s 103ms/step - loss: 0.1512 - accuracy: 0.9747 - val_loss: 1.1137 - val_accuracy: 0.4500\n",
      "Epoch 16/100\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.1437 - accuracy: 0.9747 - val_loss: 1.1000 - val_accuracy: 0.6500\n",
      "Epoch 17/100\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 0.1120 - accuracy: 1.0000 - val_loss: 1.0874 - val_accuracy: 0.8500\n",
      "Epoch 18/100\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 0.1242 - accuracy: 0.9873 - val_loss: 1.0759 - val_accuracy: 0.7500\n",
      "Epoch 19/100\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 0.1010 - accuracy: 0.9873 - val_loss: 1.0663 - val_accuracy: 0.7500\n",
      "Epoch 20/100\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 0.1692 - accuracy: 0.9747 - val_loss: 1.0557 - val_accuracy: 0.7500\n",
      "Epoch 21/100\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.1098 - accuracy: 0.9873 - val_loss: 1.0500 - val_accuracy: 0.8000\n",
      "Epoch 22/100\n",
      "2/2 [==============================] - 0s 103ms/step - loss: 0.1129 - accuracy: 0.9873 - val_loss: 1.0470 - val_accuracy: 0.7500\n",
      "Epoch 23/100\n",
      "2/2 [==============================] - 0s 109ms/step - loss: 0.1128 - accuracy: 0.9873 - val_loss: 1.0451 - val_accuracy: 0.8000\n",
      "Epoch 24/100\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.0994 - accuracy: 0.9873 - val_loss: 1.0445 - val_accuracy: 0.7500\n",
      "Epoch 25/100\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 0.0899 - accuracy: 1.0000 - val_loss: 1.0445 - val_accuracy: 0.7500\n",
      "Epoch 26/100\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 0.0848 - accuracy: 1.0000 - val_loss: 1.0439 - val_accuracy: 0.8000\n",
      "Epoch 27/100\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.0883 - accuracy: 1.0000 - val_loss: 1.0457 - val_accuracy: 0.8000\n",
      "Epoch 28/100\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.0806 - accuracy: 1.0000 - val_loss: 1.0495 - val_accuracy: 0.8000\n",
      "Epoch 29/100\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 0.0805 - accuracy: 1.0000 - val_loss: 1.0523 - val_accuracy: 0.5500\n",
      "Epoch 30/100\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.0790 - accuracy: 1.0000 - val_loss: 1.0535 - val_accuracy: 0.5500\n",
      "Epoch 31/100\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.0780 - accuracy: 1.0000 - val_loss: 1.0533 - val_accuracy: 0.5500\n",
      "Epoch 32/100\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 0.0768 - accuracy: 1.0000 - val_loss: 1.0523 - val_accuracy: 0.5500\n",
      "Epoch 33/100\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 0.0759 - accuracy: 1.0000 - val_loss: 1.0501 - val_accuracy: 0.5000\n",
      "Epoch 34/100\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 0.0748 - accuracy: 1.0000 - val_loss: 1.0467 - val_accuracy: 0.5000\n",
      "Epoch 35/100\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 0.0740 - accuracy: 1.0000 - val_loss: 1.0426 - val_accuracy: 0.5000\n",
      "Epoch 36/100\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 0.0724 - accuracy: 1.0000 - val_loss: 1.0384 - val_accuracy: 0.5000\n",
      "Epoch 37/100\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 0.0711 - accuracy: 1.0000 - val_loss: 1.0340 - val_accuracy: 0.5500\n",
      "Epoch 38/100\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 0.0710 - accuracy: 1.0000 - val_loss: 1.0293 - val_accuracy: 0.5500\n",
      "Epoch 39/100\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 0.0706 - accuracy: 1.0000 - val_loss: 1.0242 - val_accuracy: 0.5500\n",
      "Epoch 40/100\n",
      "2/2 [==============================] - 0s 109ms/step - loss: 0.0688 - accuracy: 1.0000 - val_loss: 1.0194 - val_accuracy: 0.5500\n",
      "Epoch 41/100\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 0.0687 - accuracy: 1.0000 - val_loss: 1.0146 - val_accuracy: 0.5500\n",
      "Epoch 42/100\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.0678 - accuracy: 1.0000 - val_loss: 1.0097 - val_accuracy: 0.6000\n",
      "Epoch 43/100\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 0.0668 - accuracy: 1.0000 - val_loss: 1.0046 - val_accuracy: 0.6500\n",
      "Epoch 44/100\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 0.0660 - accuracy: 1.0000 - val_loss: 0.9999 - val_accuracy: 0.6500\n",
      "Epoch 45/100\n",
      "2/2 [==============================] - 0s 180ms/step - loss: 0.0657 - accuracy: 1.0000 - val_loss: 0.9955 - val_accuracy: 0.6500\n",
      "Epoch 46/100\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.0652 - accuracy: 1.0000 - val_loss: 0.9910 - val_accuracy: 0.6500\n",
      "Epoch 47/100\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 0.0642 - accuracy: 1.0000 - val_loss: 0.9869 - val_accuracy: 0.6500\n",
      "Epoch 48/100\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 0.0636 - accuracy: 1.0000 - val_loss: 0.9830 - val_accuracy: 0.6500\n",
      "Epoch 49/100\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.0631 - accuracy: 1.0000 - val_loss: 0.9792 - val_accuracy: 0.6500\n",
      "Epoch 50/100\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 0.0625 - accuracy: 1.0000 - val_loss: 0.9756 - val_accuracy: 0.6500\n",
      "Epoch 51/100\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.0619 - accuracy: 1.0000 - val_loss: 0.9721 - val_accuracy: 0.7000\n",
      "Epoch 52/100\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 0.0614 - accuracy: 1.0000 - val_loss: 0.9689 - val_accuracy: 0.7000\n",
      "Epoch 53/100\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.0608 - accuracy: 1.0000 - val_loss: 0.9656 - val_accuracy: 0.7000\n",
      "Epoch 54/100\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 0.0602 - accuracy: 1.0000 - val_loss: 0.9624 - val_accuracy: 0.7500\n",
      "Epoch 55/100\n",
      "2/2 [==============================] - 0s 104ms/step - loss: 0.0597 - accuracy: 1.0000 - val_loss: 0.9594 - val_accuracy: 0.7500\n",
      "Epoch 56/100\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 0.0592 - accuracy: 1.0000 - val_loss: 0.9565 - val_accuracy: 0.7500\n",
      "Epoch 57/100\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.0593 - accuracy: 1.0000 - val_loss: 0.9536 - val_accuracy: 0.7500\n",
      "Epoch 58/100\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 0.0582 - accuracy: 1.0000 - val_loss: 0.9509 - val_accuracy: 0.7500\n",
      "Epoch 59/100\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 0.0577 - accuracy: 1.0000 - val_loss: 0.9482 - val_accuracy: 0.7500\n",
      "Epoch 60/100\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 0.0573 - accuracy: 1.0000 - val_loss: 0.9454 - val_accuracy: 0.7500\n",
      "Epoch 61/100\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.0567 - accuracy: 1.0000 - val_loss: 0.9428 - val_accuracy: 0.8000\n",
      "Epoch 62/100\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.0563 - accuracy: 1.0000 - val_loss: 0.9400 - val_accuracy: 0.8000\n",
      "Epoch 63/100\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.0557 - accuracy: 1.0000 - val_loss: 0.9375 - val_accuracy: 0.8000\n",
      "Epoch 64/100\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 0.0552 - accuracy: 1.0000 - val_loss: 0.9349 - val_accuracy: 0.8000\n",
      "Epoch 65/100\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 0.0548 - accuracy: 1.0000 - val_loss: 0.9323 - val_accuracy: 0.8000\n",
      "Epoch 66/100\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 0.0546 - accuracy: 1.0000 - val_loss: 0.9294 - val_accuracy: 0.8000\n",
      "Epoch 67/100\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 0.0539 - accuracy: 1.0000 - val_loss: 0.9262 - val_accuracy: 0.8000\n",
      "Epoch 68/100\n",
      "2/2 [==============================] - 0s 104ms/step - loss: 0.0535 - accuracy: 1.0000 - val_loss: 0.9230 - val_accuracy: 0.8000\n",
      "Epoch 69/100\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 0.0530 - accuracy: 1.0000 - val_loss: 0.9199 - val_accuracy: 0.8000\n",
      "Epoch 70/100\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 0.0526 - accuracy: 1.0000 - val_loss: 0.9168 - val_accuracy: 0.8000\n",
      "Epoch 71/100\n",
      "2/2 [==============================] - 0s 98ms/step - loss: 0.0522 - accuracy: 1.0000 - val_loss: 0.9140 - val_accuracy: 0.8000\n",
      "Epoch 72/100\n",
      "2/2 [==============================] - 0s 98ms/step - loss: 0.0517 - accuracy: 1.0000 - val_loss: 0.9111 - val_accuracy: 0.8000\n",
      "Epoch 73/100\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.0513 - accuracy: 1.0000 - val_loss: 0.9082 - val_accuracy: 0.8000\n",
      "Epoch 74/100\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 0.0509 - accuracy: 1.0000 - val_loss: 0.9054 - val_accuracy: 0.8000\n",
      "Epoch 75/100\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.0504 - accuracy: 1.0000 - val_loss: 0.9027 - val_accuracy: 0.8000\n",
      "Epoch 76/100\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 0.0501 - accuracy: 1.0000 - val_loss: 0.8999 - val_accuracy: 0.8000\n",
      "Epoch 77/100\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 0.0496 - accuracy: 1.0000 - val_loss: 0.8971 - val_accuracy: 0.8000\n",
      "Epoch 78/100\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 0.0493 - accuracy: 1.0000 - val_loss: 0.8943 - val_accuracy: 0.8000\n",
      "Epoch 79/100\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 0.0489 - accuracy: 1.0000 - val_loss: 0.8914 - val_accuracy: 0.8000\n",
      "Epoch 80/100\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 0.0485 - accuracy: 1.0000 - val_loss: 0.8887 - val_accuracy: 0.8000\n",
      "Epoch 81/100\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.0481 - accuracy: 1.0000 - val_loss: 0.8859 - val_accuracy: 0.8000\n",
      "Epoch 82/100\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 0.0477 - accuracy: 1.0000 - val_loss: 0.8831 - val_accuracy: 0.8000\n",
      "Epoch 83/100\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 0.0473 - accuracy: 1.0000 - val_loss: 0.8803 - val_accuracy: 0.8000\n",
      "Epoch 84/100\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.0470 - accuracy: 1.0000 - val_loss: 0.8776 - val_accuracy: 0.8000\n",
      "Epoch 85/100\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 0.0466 - accuracy: 1.0000 - val_loss: 0.8749 - val_accuracy: 0.8000\n",
      "Epoch 86/100\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.0462 - accuracy: 1.0000 - val_loss: 0.8720 - val_accuracy: 0.8000\n",
      "Epoch 87/100\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.0459 - accuracy: 1.0000 - val_loss: 0.8690 - val_accuracy: 0.8000\n",
      "Epoch 88/100\n",
      "2/2 [==============================] - 0s 108ms/step - loss: 0.0455 - accuracy: 1.0000 - val_loss: 0.8662 - val_accuracy: 0.8000\n",
      "Epoch 89/100\n",
      "2/2 [==============================] - 0s 121ms/step - loss: 0.0452 - accuracy: 1.0000 - val_loss: 0.8632 - val_accuracy: 0.8000\n",
      "Epoch 90/100\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.0448 - accuracy: 1.0000 - val_loss: 0.8600 - val_accuracy: 0.8000\n",
      "Epoch 91/100\n",
      "2/2 [==============================] - 0s 316ms/step - loss: 0.0445 - accuracy: 1.0000 - val_loss: 0.8570 - val_accuracy: 0.8000\n",
      "Epoch 92/100\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 0.0441 - accuracy: 1.0000 - val_loss: 0.8541 - val_accuracy: 0.8000\n",
      "Epoch 93/100\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.0438 - accuracy: 1.0000 - val_loss: 0.8512 - val_accuracy: 0.8000\n",
      "Epoch 94/100\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.0435 - accuracy: 1.0000 - val_loss: 0.8481 - val_accuracy: 0.8000\n",
      "Epoch 95/100\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.0431 - accuracy: 1.0000 - val_loss: 0.8451 - val_accuracy: 0.8000\n",
      "Epoch 96/100\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.0428 - accuracy: 1.0000 - val_loss: 0.8420 - val_accuracy: 0.8000\n",
      "Epoch 97/100\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 0.0424 - accuracy: 1.0000 - val_loss: 0.8389 - val_accuracy: 0.8000\n",
      "Epoch 98/100\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.0421 - accuracy: 1.0000 - val_loss: 0.8358 - val_accuracy: 0.8000\n",
      "Epoch 99/100\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.0418 - accuracy: 1.0000 - val_loss: 0.8326 - val_accuracy: 0.8000\n",
      "Epoch 100/100\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.0415 - accuracy: 1.0000 - val_loss: 0.8294 - val_accuracy: 0.8000\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.7148 - accuracy: 1.0000\n",
      "Evaluation Metrics for LSTM:\n",
      "loss: 0.7148203253746033\n",
      "accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "def LSTM_model(vocab_size, sequence_length):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=vocab_size, output_dim=128, input_length=sequence_length))\n",
    "    model.add(LSTM(units=128, activation='relu', return_sequences=True))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LSTM(units=64, activation='relu'))\n",
    "    model.add(Dense(256, activation='relu', kernel_regularizer=l2(0.001)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "    optimizer = Adam(learning_rate=0.001)\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "#EMBEDDING_DIM = 110\n",
    "lstm_model = LSTM_model(vocab_size, max_sequence_length)\n",
    "early_stopping_rnn = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "history_updated_rnn = lstm_model.fit(X_train_rnn, y_train_rnn, validation_data=(X_test_rnn, y_test_rnn), epochs=100, batch_size=64, callbacks=[early_stopping_rnn])\n",
    "\n",
    "\n",
    "# Evaluate the lstm model\n",
    "evaluation_metrics_updated_lstm = lstm_model.evaluate(X_test, Y_test)\n",
    "print(\"Evaluation Metrics for LSTM:\")\n",
    "for metric_name, metric_value in zip(lstm_model.metrics_names, evaluation_metrics_updated_lstm):\n",
    "    print(f\"{metric_name}: {metric_value}\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*<h4>TRANSFORMER Model<h4>*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, density, rate=0.1, l2_reg=0.01):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            layers.Dense(density, activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(l2_reg)),\n",
    "            layers.Dense(embed_dim, kernel_regularizer=tf.keras.regularizers.l2(l2_reg)),\n",
    "        ])\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "\n",
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions\n",
    "\n",
    "def build_transformer_model(maxlen, vocab_size, embed_dim, num_heads, density, transformer_units, mlp_units, dropout_rate, num_classes):\n",
    "    inputs = layers.Input(shape=(maxlen,))\n",
    "    embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
    "    x = embedding_layer(inputs)\n",
    "    for _ in range(transformer_units):\n",
    "        x = TransformerBlock(embed_dim, num_heads, density, rate=dropout_rate)(x)\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    for dim in mlp_units:\n",
    "        x = layers.Dense(dim, activation=\"relu\")(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "    outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = max_sequence_length\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "embed_dim = 120\n",
    "num_heads = 2\n",
    "density = 3\n",
    "transformer_units = 4\n",
    "mlp_units = [128]\n",
    "dropout_rate = 0.5\n",
    "num_classes = len(df['grade'].unique())\n",
    "\n",
    "transformer_model = build_transformer_model(maxlen, vocab_size, embed_dim, num_heads, density, transformer_units, mlp_units, dropout_rate, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\amine\\anaconda3\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Epoch 1/30\n",
      "3/3 [==============================] - 22s 781ms/step - loss: 1.7744 - accuracy: 0.3333 - val_loss: 1.4864 - val_accuracy: 0.3333\n",
      "Epoch 2/30\n",
      "3/3 [==============================] - 0s 67ms/step - loss: 1.3756 - accuracy: 0.5952 - val_loss: 1.2999 - val_accuracy: 0.7333\n",
      "Epoch 3/30\n",
      "3/3 [==============================] - 0s 63ms/step - loss: 1.3551 - accuracy: 0.5952 - val_loss: 1.4481 - val_accuracy: 0.6667\n",
      "Epoch 4/30\n",
      "3/3 [==============================] - 0s 102ms/step - loss: 1.3146 - accuracy: 0.5833 - val_loss: 1.2339 - val_accuracy: 0.6667\n",
      "Epoch 5/30\n",
      "3/3 [==============================] - 0s 73ms/step - loss: 1.1499 - accuracy: 0.6905 - val_loss: 1.1792 - val_accuracy: 0.6667\n",
      "Epoch 6/30\n",
      "3/3 [==============================] - 0s 69ms/step - loss: 1.0174 - accuracy: 0.7738 - val_loss: 1.0950 - val_accuracy: 0.7333\n",
      "Epoch 7/30\n",
      "3/3 [==============================] - 0s 76ms/step - loss: 0.8243 - accuracy: 0.7976 - val_loss: 0.9661 - val_accuracy: 0.8000\n",
      "Epoch 8/30\n",
      "3/3 [==============================] - 0s 82ms/step - loss: 0.6863 - accuracy: 0.9286 - val_loss: 0.8815 - val_accuracy: 0.9333\n",
      "Epoch 9/30\n",
      "3/3 [==============================] - 0s 94ms/step - loss: 0.6079 - accuracy: 0.9524 - val_loss: 0.7946 - val_accuracy: 0.9333\n",
      "Epoch 10/30\n",
      "3/3 [==============================] - 0s 83ms/step - loss: 0.5790 - accuracy: 0.9643 - val_loss: 0.8896 - val_accuracy: 0.8667\n",
      "Epoch 11/30\n",
      "3/3 [==============================] - 0s 70ms/step - loss: 0.5155 - accuracy: 0.9524 - val_loss: 1.2414 - val_accuracy: 0.8667\n",
      "Epoch 12/30\n",
      "3/3 [==============================] - 0s 72ms/step - loss: 0.4700 - accuracy: 0.9762 - val_loss: 1.2226 - val_accuracy: 0.8667\n",
      "Epoch 13/30\n",
      "3/3 [==============================] - 0s 146ms/step - loss: 0.4250 - accuracy: 1.0000 - val_loss: 0.8474 - val_accuracy: 0.9333\n",
      "Epoch 14/30\n",
      "3/3 [==============================] - 0s 96ms/step - loss: 0.4217 - accuracy: 0.9762 - val_loss: 0.6878 - val_accuracy: 0.9333\n",
      "Epoch 15/30\n",
      "3/3 [==============================] - 0s 93ms/step - loss: 0.4233 - accuracy: 0.9881 - val_loss: 0.8419 - val_accuracy: 0.9333\n",
      "Epoch 16/30\n",
      "3/3 [==============================] - 0s 130ms/step - loss: 0.3762 - accuracy: 1.0000 - val_loss: 0.9705 - val_accuracy: 0.8667\n",
      "Epoch 17/30\n",
      "3/3 [==============================] - 0s 68ms/step - loss: 0.3836 - accuracy: 0.9881 - val_loss: 1.1596 - val_accuracy: 0.8667\n",
      "Epoch 18/30\n",
      "3/3 [==============================] - 0s 72ms/step - loss: 0.3749 - accuracy: 0.9881 - val_loss: 1.1826 - val_accuracy: 0.8667\n",
      "Epoch 19/30\n",
      "3/3 [==============================] - 0s 81ms/step - loss: 0.3541 - accuracy: 1.0000 - val_loss: 0.8434 - val_accuracy: 0.8667\n",
      "Epoch 20/30\n",
      "3/3 [==============================] - 0s 87ms/step - loss: 0.3392 - accuracy: 1.0000 - val_loss: 0.6900 - val_accuracy: 0.9333\n",
      "Epoch 21/30\n",
      "3/3 [==============================] - 0s 76ms/step - loss: 0.3351 - accuracy: 1.0000 - val_loss: 0.6837 - val_accuracy: 0.9333\n",
      "Epoch 22/30\n",
      "3/3 [==============================] - 0s 107ms/step - loss: 0.3417 - accuracy: 0.9881 - val_loss: 0.7398 - val_accuracy: 0.8667\n",
      "Epoch 23/30\n",
      "3/3 [==============================] - 0s 83ms/step - loss: 0.3212 - accuracy: 1.0000 - val_loss: 0.8208 - val_accuracy: 0.8667\n",
      "Epoch 24/30\n",
      "3/3 [==============================] - 0s 128ms/step - loss: 0.3149 - accuracy: 1.0000 - val_loss: 0.9209 - val_accuracy: 0.8667\n",
      "Epoch 25/30\n",
      "3/3 [==============================] - 0s 79ms/step - loss: 0.3086 - accuracy: 1.0000 - val_loss: 0.9920 - val_accuracy: 0.8667\n",
      "Epoch 26/30\n",
      "3/3 [==============================] - 0s 73ms/step - loss: 0.3022 - accuracy: 1.0000 - val_loss: 1.0209 - val_accuracy: 0.8667\n",
      "Epoch 27/30\n",
      "3/3 [==============================] - 0s 66ms/step - loss: 0.3157 - accuracy: 0.9881 - val_loss: 0.8194 - val_accuracy: 0.9333\n",
      "Epoch 28/30\n",
      "3/3 [==============================] - 0s 76ms/step - loss: 0.2866 - accuracy: 1.0000 - val_loss: 0.8453 - val_accuracy: 0.9333\n",
      "Epoch 29/30\n",
      "3/3 [==============================] - 0s 71ms/step - loss: 0.2973 - accuracy: 0.9881 - val_loss: 0.7023 - val_accuracy: 0.9333\n",
      "Epoch 30/30\n",
      "3/3 [==============================] - 0s 63ms/step - loss: 0.2740 - accuracy: 1.0000 - val_loss: 0.8528 - val_accuracy: 0.8667\n",
      "Epoch 1/30\n",
      "3/3 [==============================] - 0s 98ms/step - loss: 0.3342 - accuracy: 0.9643 - val_loss: 1.2850 - val_accuracy: 0.8667\n",
      "Epoch 2/30\n",
      "3/3 [==============================] - 0s 67ms/step - loss: 0.2618 - accuracy: 1.0000 - val_loss: 1.1466 - val_accuracy: 0.8667\n",
      "Epoch 3/30\n",
      "3/3 [==============================] - 0s 61ms/step - loss: 0.2577 - accuracy: 1.0000 - val_loss: 1.1553 - val_accuracy: 0.8667\n",
      "Epoch 4/30\n",
      "3/3 [==============================] - 0s 61ms/step - loss: 0.2578 - accuracy: 1.0000 - val_loss: 1.2559 - val_accuracy: 0.8667\n",
      "Epoch 5/30\n",
      "3/3 [==============================] - 0s 62ms/step - loss: 0.2467 - accuracy: 1.0000 - val_loss: 1.3021 - val_accuracy: 0.8667\n",
      "Epoch 6/30\n",
      "3/3 [==============================] - 0s 66ms/step - loss: 0.2439 - accuracy: 1.0000 - val_loss: 1.3000 - val_accuracy: 0.8667\n",
      "Epoch 7/30\n",
      "3/3 [==============================] - 0s 62ms/step - loss: 0.2346 - accuracy: 1.0000 - val_loss: 1.2747 - val_accuracy: 0.8667\n",
      "Epoch 8/30\n",
      "3/3 [==============================] - 0s 70ms/step - loss: 0.2306 - accuracy: 1.0000 - val_loss: 1.2316 - val_accuracy: 0.8667\n",
      "Epoch 9/30\n",
      "3/3 [==============================] - 0s 62ms/step - loss: 0.2248 - accuracy: 1.0000 - val_loss: 1.1870 - val_accuracy: 0.8667\n",
      "Epoch 10/30\n",
      "3/3 [==============================] - 0s 60ms/step - loss: 0.2187 - accuracy: 1.0000 - val_loss: 1.1432 - val_accuracy: 0.8667\n",
      "Epoch 11/30\n",
      "3/3 [==============================] - 0s 61ms/step - loss: 0.2147 - accuracy: 1.0000 - val_loss: 1.1044 - val_accuracy: 0.8667\n",
      "Epoch 12/30\n",
      "3/3 [==============================] - 0s 62ms/step - loss: 0.2101 - accuracy: 1.0000 - val_loss: 1.0650 - val_accuracy: 0.8667\n",
      "Epoch 13/30\n",
      "3/3 [==============================] - 0s 58ms/step - loss: 0.2038 - accuracy: 1.0000 - val_loss: 1.0361 - val_accuracy: 0.8667\n",
      "Epoch 14/30\n",
      "3/3 [==============================] - 0s 62ms/step - loss: 0.1992 - accuracy: 1.0000 - val_loss: 1.0129 - val_accuracy: 0.8667\n",
      "Epoch 15/30\n",
      "3/3 [==============================] - 0s 67ms/step - loss: 0.1948 - accuracy: 1.0000 - val_loss: 1.0003 - val_accuracy: 0.8667\n",
      "Epoch 16/30\n",
      "3/3 [==============================] - 0s 60ms/step - loss: 0.1893 - accuracy: 1.0000 - val_loss: 0.9894 - val_accuracy: 0.8667\n",
      "Epoch 17/30\n",
      "3/3 [==============================] - 0s 56ms/step - loss: 0.1846 - accuracy: 1.0000 - val_loss: 0.9779 - val_accuracy: 0.8667\n",
      "Epoch 18/30\n",
      "3/3 [==============================] - 0s 64ms/step - loss: 0.1804 - accuracy: 1.0000 - val_loss: 0.9654 - val_accuracy: 0.8667\n",
      "Epoch 19/30\n",
      "3/3 [==============================] - 0s 62ms/step - loss: 0.1762 - accuracy: 1.0000 - val_loss: 0.9510 - val_accuracy: 0.8667\n",
      "Epoch 20/30\n",
      "3/3 [==============================] - 0s 58ms/step - loss: 0.1714 - accuracy: 1.0000 - val_loss: 0.9391 - val_accuracy: 0.8667\n",
      "Epoch 21/30\n",
      "3/3 [==============================] - 0s 60ms/step - loss: 0.1672 - accuracy: 1.0000 - val_loss: 0.9270 - val_accuracy: 0.8667\n",
      "Epoch 22/30\n",
      "3/3 [==============================] - 0s 67ms/step - loss: 0.1632 - accuracy: 1.0000 - val_loss: 0.9168 - val_accuracy: 0.8667\n",
      "Epoch 23/30\n",
      "3/3 [==============================] - 0s 63ms/step - loss: 0.1594 - accuracy: 1.0000 - val_loss: 0.9072 - val_accuracy: 0.8667\n",
      "Epoch 24/30\n",
      "3/3 [==============================] - 0s 62ms/step - loss: 0.1553 - accuracy: 1.0000 - val_loss: 0.9013 - val_accuracy: 0.8667\n",
      "Epoch 25/30\n",
      "3/3 [==============================] - 0s 57ms/step - loss: 0.1514 - accuracy: 1.0000 - val_loss: 0.8973 - val_accuracy: 0.8667\n",
      "Epoch 26/30\n",
      "3/3 [==============================] - 0s 58ms/step - loss: 0.1477 - accuracy: 1.0000 - val_loss: 0.8918 - val_accuracy: 0.8667\n",
      "Epoch 27/30\n",
      "3/3 [==============================] - 0s 62ms/step - loss: 0.1445 - accuracy: 1.0000 - val_loss: 0.8841 - val_accuracy: 0.8667\n",
      "Epoch 28/30\n",
      "3/3 [==============================] - 0s 60ms/step - loss: 0.1402 - accuracy: 1.0000 - val_loss: 0.8760 - val_accuracy: 0.8667\n",
      "Epoch 29/30\n",
      "3/3 [==============================] - 0s 60ms/step - loss: 0.1365 - accuracy: 1.0000 - val_loss: 0.8680 - val_accuracy: 0.8667\n",
      "Epoch 30/30\n",
      "3/3 [==============================] - 0s 65ms/step - loss: 0.1334 - accuracy: 1.0000 - val_loss: 0.8487 - val_accuracy: 0.8667\n"
     ]
    }
   ],
   "source": [
    "transformer_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "transformer_model.fit(X_train, Y_train, epochs=30, batch_size=32, validation_data=(X_test, Y_test))\n",
    "history = transformer_model.fit(X_train, Y_train, epochs=30, batch_size=32, validation_data=(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 132ms/step - loss: 0.8487 - accuracy: 0.8667\n",
      "Evaluation Metrics Transformer:\n",
      "loss: 0.8487389087677002\n",
      "accuracy: 0.8666666746139526\n"
     ]
    }
   ],
   "source": [
    "evaluation_metrics_transformer = transformer_model.evaluate(X_test, Y_test)\n",
    "print(\"Evaluation Metrics Transformer:\")\n",
    "for metric_name, metric_value in zip(transformer_model.metrics_names, evaluation_metrics_transformer):\n",
    "    print(f\"{metric_name}: {metric_value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 70ms/step - loss: 0.8487 - accuracy: 0.8667\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "[[ 0  0  0  0  0  0  0  0  0  0  0  0 10 11 26]\n",
      " [ 0  0  0  0  0  0  0  0  0  5 10 11  1 50  3]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0 14 25]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0 61  2 16 62]\n",
      " [ 0  0  0  3  9 87 13 23  5  2  8  6  4  7  1]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  5  6  4  7  1]\n",
      " [ 0  0  0  0  0 10  3 36 11 20  4 21  1 16 22]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0 10 11 37]\n",
      " [ 0  0  0  0  0  0  0  0  0  0 24 41 28 57 58]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0 10 18 26]\n",
      " [ 0  0  0  0  0  0  0 67  3 15 12  1  5  2  8]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  5 10 11]\n",
      " [ 0  9  3 15 17 13  1 69 11  1  5  6  4  7  1]\n",
      " [ 0  0  0  0  0  0  0  0  9  3 15 17 13  1 46]\n",
      " [ 0  0  0  0  0  0  0  0  0  0 10 11 37 28 13]]\n",
      "\n",
      "Real and Predicted Values:\n",
      "    Real  Predicted\n",
      "0      1          1\n",
      "1      2          2\n",
      "2      1          1\n",
      "3      0          1\n",
      "4      2          2\n",
      "5      2          2\n",
      "6      1          1\n",
      "7      1          1\n",
      "8      1          1\n",
      "9      1          1\n",
      "10     2          2\n",
      "11     2          2\n",
      "12     2          2\n",
      "13     0          2\n",
      "14     1          1\n"
     ]
    }
   ],
   "source": [
    "evaluation_metrics_transformer = transformer_model.evaluate(X_test, Y_test)\n",
    "\n",
    "predictions = transformer_model.predict(X_test)\n",
    "print(X_test)\n",
    "\n",
    "# Convert one-hot encoded labels to class indices\n",
    "y_true = np.argmax(Y_test, axis=1)\n",
    "\n",
    "# Convert predicted probabilities to class indices\n",
    "y_pred = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Create DataFrame\n",
    "results_df = pd.DataFrame({\"Real\": y_true, \"Predicted\": y_pred})\n",
    "\n",
    "# Display DataFrame\n",
    "print(\"\\nReal and Predicted Values:\")\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy RNN: 1.0\n",
      "Accuracy LSTM: 1.0\n",
      "Accuracy Transformer: 0.8666666746139526\n"
     ]
    }
   ],
   "source": [
    "# Evaluate RNN model\n",
    "rnn_accuracy = rnn_model.evaluate(X_test, Y_test, verbose=0)[1]\n",
    "print(\"Accuracy RNN:\", rnn_accuracy)\n",
    "\n",
    "# Evaluate LSTM model\n",
    "lstm_accuracy = lstm_model.evaluate(X_test, Y_test, verbose=0)[1]\n",
    "print(\"Accuracy LSTM:\", lstm_accuracy)\n",
    "\n",
    "# Evaluate Transformer model\n",
    "transformer_accuracy = transformer_model.evaluate(X_test, Y_test, verbose=0)[1]\n",
    "print(\"Accuracy Transformer:\", transformer_accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model (RNN) with accuracy 1.0 has been saved to './savedModels/q6_model.h5'\n"
     ]
    }
   ],
   "source": [
    "# Determine the best model\n",
    "best_model_name, best_model_accuracy = max([('RNN', rnn_accuracy), ('LSTM', lstm_accuracy), ('Transformer', transformer_accuracy)], key=lambda x: x[1])\n",
    "\n",
    "save_path = './savedModels/q6_model.h5'\n",
    "# Save the best model\n",
    "if best_model_name == 'RNN':\n",
    "    rnn_model.save(save_path)\n",
    "elif best_model_name == 'LSTM':\n",
    "    lstm_model.save(save_path)\n",
    "elif best_model_name == 'Transformer':\n",
    "    transformer_model.save(save_path)\n",
    "\n",
    "print(f\"The best model ({best_model_name}) with accuracy {best_model_accuracy} has been saved to '{save_path}'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
