{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow as keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.regularizers import l2\n",
    "from keras import layers\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Embedding, SimpleRNN, Dense, Dropout, LSTM, Bidirectional, Input\n",
    "from keras.layers import Attention, GlobalAveragePooling1D\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import stanza\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import BatchNormalization, Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*<h3>Load Data<h3>*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_id</th>\n",
       "      <th>answer</th>\n",
       "      <th>grade</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>من خزائن العطاء الإلهي، انبلج سيلان: الأول حب ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>السيدة فاطمة هي الأم الزكية للنبي</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>في ظلال الكعبة المشرفة، رأت آمنة بنت وهب فجر ا...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>الرسول محمد والدته ابنة وهب.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>عائشة هي الأم الصالحة لنبي الإسلام</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>عائشة أم الرسول</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>على دروب مكة المكرمة، خطت آمنة بنت وهب خطواتها...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3</td>\n",
       "      <td>من بطنها انبثق النور، ومن حنانها نهل كل روح، ب...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3</td>\n",
       "      <td>خديجةُ نموذجٌ للمرأةِ المسلمةِ المثاليةِ، التي...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3</td>\n",
       "      <td>بصوتٍ حنونٍ كصوت المطر، أرضعت آمنة بنت وهب محم...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   question_id                                             answer  grade\n",
       "0            3  من خزائن العطاء الإلهي، انبلج سيلان: الأول حب ...      2\n",
       "1            3                  السيدة فاطمة هي الأم الزكية للنبي      0\n",
       "2            3  في ظلال الكعبة المشرفة، رأت آمنة بنت وهب فجر ا...      2\n",
       "3            3                       الرسول محمد والدته ابنة وهب.      1\n",
       "4            3                 عائشة هي الأم الصالحة لنبي الإسلام      0\n",
       "5            3                                    عائشة أم الرسول      0\n",
       "6            3  على دروب مكة المكرمة، خطت آمنة بنت وهب خطواتها...      2\n",
       "7            3  من بطنها انبثق النور، ومن حنانها نهل كل روح، ب...      1\n",
       "8            3  خديجةُ نموذجٌ للمرأةِ المسلمةِ المثاليةِ، التي...      0\n",
       "9            3  بصوتٍ حنونٍ كصوت المطر، أرضعت آمنة بنت وهب محم...      2"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read csv file into pandas\n",
    "df = pd.read_csv(\"../datasets/shuffled3.csv\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*<h3>EDA<h3>*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 78 entries, 0 to 77\n",
      "Data columns (total 3 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   question_id  78 non-null     int64 \n",
      " 1   answer       78 non-null     object\n",
      " 2   grade        78 non-null     int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 2.0+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "grade\n",
       "0    25\n",
       "1    18\n",
       "2    35\n",
       "dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('grade').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(data=df, x='grade')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*<h3>Data Cleaning<h3>*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('question_id', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     من خزائن العطاء الإلهي، انبلج سيلان: الأول حب ...\n",
      "1                     السيدة فاطمة هي الأم الزكية للنبي\n",
      "2     في ظلال الكعبة المشرفة، رأت آمنة بنت وهب فجر ا...\n",
      "3                          الرسول محمد والدته ابنة وهب.\n",
      "4                    عائشة هي الأم الصالحة لنبي الإسلام\n",
      "                            ...                        \n",
      "73                                      آمنة، أم النبي.\n",
      "74    تحتضن السيرة العطرة اسم آمنة بنت وهب، أم رسول ...\n",
      "75                       الأم الكريمة عائشة هي أم النبي\n",
      "76    جذور النبوة امتزجت في بنت وهب، لتمنحنا خير الأ...\n",
      "77                                    ولّدت آمنة النبي.\n",
      "Name: answer, Length: 78, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df.drop_duplicates(inplace=True)\n",
    "print(df['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*<h3>Data Pre-Preocessing<h3>*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "347c7b2367654933984feb04a5c01811",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.7.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-09 22:44:05 INFO: Downloading default packages for language: ar (Arabic) ...\n",
      "2024-01-09 22:44:07 INFO: File exists: C:\\Users\\amine\\stanza_resources\\ar\\default.zip\n",
      "2024-01-09 22:44:10 INFO: Finished downloading models and saved to C:\\Users\\amine\\stanza_resources.\n",
      "2024-01-09 22:44:10 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5ebafaa84014a658144e1c9e5138b68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.7.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-09 22:44:13 INFO: Loading these models for language: ar (Arabic):\n",
      "=============================\n",
      "| Processor | Package       |\n",
      "-----------------------------\n",
      "| tokenize  | padt          |\n",
      "| mwt       | padt          |\n",
      "| pos       | padt_charlm   |\n",
      "| lemma     | padt_nocharlm |\n",
      "| depparse  | padt_charlm   |\n",
      "| ner       | aqmar_charlm  |\n",
      "=============================\n",
      "\n",
      "2024-01-09 22:44:13 INFO: Using device: cpu\n",
      "2024-01-09 22:44:13 INFO: Loading: tokenize\n",
      "2024-01-09 22:44:13 INFO: Loading: mwt\n",
      "2024-01-09 22:44:13 INFO: Loading: pos\n",
      "2024-01-09 22:44:13 INFO: Loading: lemma\n",
      "2024-01-09 22:44:13 INFO: Loading: depparse\n",
      "2024-01-09 22:44:14 INFO: Loading: ner\n",
      "2024-01-09 22:44:14 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "df['grade'] = le.fit_transform(df['grade'])\n",
    "\n",
    "stanza.download('ar')\n",
    "nlp = stanza.Pipeline('ar')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    doc = nlp(text)\n",
    "    tokens = [word.lemma for sent in doc.sentences for word in sent.words if word.upos != 'PUNCT']\n",
    "    return tokens\n",
    "\n",
    "df['answer'] = df['answer'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0 ...   4   1  13]\n",
      " [  0   0   0 ...   8 110  11]\n",
      " [  0   0   0 ...  35 115   2]\n",
      " ...\n",
      " [  0   0   0 ...   2   8  11]\n",
      " [  0   0   0 ... 310  87  17]\n",
      " [  0   0   0 ... 311   3  11]]\n",
      "0     [مِن, خَزِينَة, عَطَاء, إِلٰهِيّ, اِنبَلَج, سَ...\n",
      "1         [سَيِّدَة, فاطمة, هُوَ, أُمّ, زَكِيّ, نَبِيّ]\n",
      "2     [فِي, ظِلَال, كُعبَة, مُشَرَّف, رَأَى, آمِن, ب...\n",
      "3      [رَسُول, محمد, وَالِدَة, هُوَ, اِبنَة, وَ, هَبّ]\n",
      "4      [عائشة, هُوَ, أُمّ, صَالِح, لِ, نَبِيّ, إِسلَام]\n",
      "                            ...                        \n",
      "73                                  [آمِن, أَم, نَبِيّ]\n",
      "74    [اِحتَضَن, سِيرَة, عُطر, اِسم, آمِن, بنت, وَ, ...\n",
      "75            [أُمّ, كَرِيم, عائشة, هُوَ, أُمّ, نَبِيّ]\n",
      "76    [جِذر, نُبُوَّة, اِمتَزَج, فِي, بنت, وَ, هُبّ,...\n",
      "77                           [وَ, لَدَّى, آمِن, نَبِيّ]\n",
      "Name: answer, Length: 78, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Tokenization\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df['answer'])\n",
    "sequences = tokenizer.texts_to_sequences(df['answer'])\n",
    "max_sequence_length = max(len(s) for s in sequences)\n",
    "sequences = pad_sequences(sequences,max_sequence_length)\n",
    "word2idx = tokenizer.word_index\n",
    "vocab_size = len(word2idx) + 1\n",
    "\n",
    "\n",
    "X = pad_sequences(sequences, padding='post', truncating='post', maxlen=max_sequence_length)\n",
    "\n",
    "print(sequences)\n",
    "print(df['answer'])\n",
    "\n",
    "Y = to_categorical(df['grade'], num_classes=3)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.15, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*<h3>build Models<h3>*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*<h4>RNN Model<h4>*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1/1 [==============================] - 5s 5s/step - loss: 1.4001 - accuracy: 0.2742 - val_loss: 1.2497 - val_accuracy: 0.3125 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 1.2663 - accuracy: 0.2903 - val_loss: 1.2060 - val_accuracy: 0.3125 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 1.1740 - accuracy: 0.4032 - val_loss: 1.1975 - val_accuracy: 0.3750 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 1.0476 - accuracy: 0.6290 - val_loss: 1.2187 - val_accuracy: 0.3750 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.2052 - accuracy: 0.4032\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 1.2052 - accuracy: 0.4032 - val_loss: 1.2540 - val_accuracy: 0.3750 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 1.0930 - accuracy: 0.5000 - val_loss: 1.2600 - val_accuracy: 0.3750 - lr: 2.0000e-04\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.1520 - accuracy: 0.4677\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0001.\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 1.1520 - accuracy: 0.4677 - val_loss: 1.2638 - val_accuracy: 0.3750 - lr: 2.0000e-04\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 1.1180 - accuracy: 0.4677 - val_loss: 1.2649 - val_accuracy: 0.3750 - lr: 1.0000e-04\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 1.0852 - accuracy: 0.5000 - val_loss: 1.2653 - val_accuracy: 0.3750 - lr: 1.0000e-04\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 1.0729 - accuracy: 0.4677 - val_loss: 1.2652 - val_accuracy: 0.3750 - lr: 1.0000e-04\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 1.0772 - accuracy: 0.4839 - val_loss: 1.2646 - val_accuracy: 0.3750 - lr: 1.0000e-04\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 1.0317 - accuracy: 0.5484 - val_loss: 1.2636 - val_accuracy: 0.3750 - lr: 1.0000e-04\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 1.1440 - accuracy: 0.4516 - val_loss: 1.2623 - val_accuracy: 0.3750 - lr: 1.0000e-04\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 1.0746 - accuracy: 0.4839 - val_loss: 1.2608 - val_accuracy: 0.3750 - lr: 1.0000e-04\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 1.0822 - accuracy: 0.5000 - val_loss: 1.2590 - val_accuracy: 0.3750 - lr: 1.0000e-04\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 1.0795 - accuracy: 0.5484 - val_loss: 1.2571 - val_accuracy: 0.3750 - lr: 1.0000e-04\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 1.1082 - accuracy: 0.4677 - val_loss: 1.2551 - val_accuracy: 0.3750 - lr: 1.0000e-04\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 1.0623 - accuracy: 0.5161 - val_loss: 1.2529 - val_accuracy: 0.3750 - lr: 1.0000e-04\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 1.0472 - accuracy: 0.5645 - val_loss: 1.2508 - val_accuracy: 0.3750 - lr: 1.0000e-04\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 1.0557 - accuracy: 0.5161 - val_loss: 1.2487 - val_accuracy: 0.3750 - lr: 1.0000e-04\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 1.0464 - accuracy: 0.5806 - val_loss: 1.2465 - val_accuracy: 0.3750 - lr: 1.0000e-04\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 1.0663 - accuracy: 0.5161 - val_loss: 1.2444 - val_accuracy: 0.3750 - lr: 1.0000e-04\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 0.9684 - accuracy: 0.6129 - val_loss: 1.2425 - val_accuracy: 0.3750 - lr: 1.0000e-04\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 1.0032 - accuracy: 0.5806 - val_loss: 1.2405 - val_accuracy: 0.3750 - lr: 1.0000e-04\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 1.0031 - accuracy: 0.5806 - val_loss: 1.2386 - val_accuracy: 0.3750 - lr: 1.0000e-04\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 1.0539 - accuracy: 0.6290 - val_loss: 1.2368 - val_accuracy: 0.3750 - lr: 1.0000e-04\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 1.0681 - accuracy: 0.5323 - val_loss: 1.2351 - val_accuracy: 0.3750 - lr: 1.0000e-04\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 1.0255 - accuracy: 0.5484 - val_loss: 1.2333 - val_accuracy: 0.3750 - lr: 1.0000e-04\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 1.0654 - accuracy: 0.5806 - val_loss: 1.2316 - val_accuracy: 0.3750 - lr: 1.0000e-04\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 1.0034 - accuracy: 0.5806 - val_loss: 1.2301 - val_accuracy: 0.3750 - lr: 1.0000e-04\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.9979 - accuracy: 0.5323 - val_loss: 1.2286 - val_accuracy: 0.3750 - lr: 1.0000e-04\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.9732 - accuracy: 0.5968 - val_loss: 1.2272 - val_accuracy: 0.3750 - lr: 1.0000e-04\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.9842 - accuracy: 0.5806 - val_loss: 1.2258 - val_accuracy: 0.3750 - lr: 1.0000e-04\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.9879 - accuracy: 0.5968 - val_loss: 1.2246 - val_accuracy: 0.3750 - lr: 1.0000e-04\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.9875 - accuracy: 0.6452 - val_loss: 1.2234 - val_accuracy: 0.3750 - lr: 1.0000e-04\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 1.0300 - accuracy: 0.5645 - val_loss: 1.2224 - val_accuracy: 0.3750 - lr: 1.0000e-04\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.9662 - accuracy: 0.6129 - val_loss: 1.2214 - val_accuracy: 0.3750 - lr: 1.0000e-04\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.9491 - accuracy: 0.6774 - val_loss: 1.2204 - val_accuracy: 0.3750 - lr: 1.0000e-04\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.9498 - accuracy: 0.6613 - val_loss: 1.2196 - val_accuracy: 0.3750 - lr: 1.0000e-04\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.9723 - accuracy: 0.6290 - val_loss: 1.2189 - val_accuracy: 0.3750 - lr: 1.0000e-04\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.9650 - accuracy: 0.6129 - val_loss: 1.2183 - val_accuracy: 0.3750 - lr: 1.0000e-04\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.9928 - accuracy: 0.6613 - val_loss: 1.2177 - val_accuracy: 0.3750 - lr: 1.0000e-04\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 218ms/step - loss: 0.9694 - accuracy: 0.6452 - val_loss: 1.2172 - val_accuracy: 0.3750 - lr: 1.0000e-04\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.9221 - accuracy: 0.7097 - val_loss: 1.2169 - val_accuracy: 0.3750 - lr: 1.0000e-04\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.9376 - accuracy: 0.7258 - val_loss: 1.2166 - val_accuracy: 0.3750 - lr: 1.0000e-04\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.8864 - accuracy: 0.6774 - val_loss: 1.2163 - val_accuracy: 0.3750 - lr: 1.0000e-04\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.8977 - accuracy: 0.6935 - val_loss: 1.2161 - val_accuracy: 0.3750 - lr: 1.0000e-04\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.8887 - accuracy: 0.6452 - val_loss: 1.2160 - val_accuracy: 0.3750 - lr: 1.0000e-04\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.9139 - accuracy: 0.5645 - val_loss: 1.2159 - val_accuracy: 0.3750 - lr: 1.0000e-04\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.8952 - accuracy: 0.6452 - val_loss: 1.2158 - val_accuracy: 0.3750 - lr: 1.0000e-04\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.9647 - accuracy: 0.6613 - val_loss: 1.2157 - val_accuracy: 0.3750 - lr: 1.0000e-04\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.8849 - accuracy: 0.7097 - val_loss: 1.2158 - val_accuracy: 0.3750 - lr: 1.0000e-04\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.8543 - accuracy: 0.7097 - val_loss: 1.2158 - val_accuracy: 0.3750 - lr: 1.0000e-04\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.8862 - accuracy: 0.7419 - val_loss: 1.2159 - val_accuracy: 0.3750 - lr: 1.0000e-04\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.8506 - accuracy: 0.7258 - val_loss: 1.2160 - val_accuracy: 0.3750 - lr: 1.0000e-04\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.9085 - accuracy: 0.7258 - val_loss: 1.2161 - val_accuracy: 0.3750 - lr: 1.0000e-04\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.8381 - accuracy: 0.8065 - val_loss: 1.2161 - val_accuracy: 0.3750 - lr: 1.0000e-04\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.8910 - accuracy: 0.7097 - val_loss: 1.2162 - val_accuracy: 0.3750 - lr: 1.0000e-04\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.8343 - accuracy: 0.7581 - val_loss: 1.2163 - val_accuracy: 0.3750 - lr: 1.0000e-04\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.7983 - accuracy: 0.8226 - val_loss: 1.2164 - val_accuracy: 0.3750 - lr: 1.0000e-04\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.8455 - accuracy: 0.7742 - val_loss: 1.2166 - val_accuracy: 0.3750 - lr: 1.0000e-04\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.7965 - accuracy: 0.8226 - val_loss: 1.2167 - val_accuracy: 0.3750 - lr: 1.0000e-04\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.8297 - accuracy: 0.7419 - val_loss: 1.2168 - val_accuracy: 0.3750 - lr: 1.0000e-04\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.8337 - accuracy: 0.7419 - val_loss: 1.2170 - val_accuracy: 0.3750 - lr: 1.0000e-04\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.8417 - accuracy: 0.7419 - val_loss: 1.2170 - val_accuracy: 0.3750 - lr: 1.0000e-04\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.8359 - accuracy: 0.7258 - val_loss: 1.2171 - val_accuracy: 0.3750 - lr: 1.0000e-04\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.7737 - accuracy: 0.8226 - val_loss: 1.2172 - val_accuracy: 0.3750 - lr: 1.0000e-04\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.7715 - accuracy: 0.8065 - val_loss: 1.2172 - val_accuracy: 0.3750 - lr: 1.0000e-04\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.7687 - accuracy: 0.8387 - val_loss: 1.2174 - val_accuracy: 0.3750 - lr: 1.0000e-04\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.8045 - accuracy: 0.7742 - val_loss: 1.2175 - val_accuracy: 0.3750 - lr: 1.0000e-04\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.7274 - accuracy: 0.9032 - val_loss: 1.2177 - val_accuracy: 0.3750 - lr: 1.0000e-04\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.7738 - accuracy: 0.8387 - val_loss: 1.2179 - val_accuracy: 0.3750 - lr: 1.0000e-04\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.7789 - accuracy: 0.8226 - val_loss: 1.2179 - val_accuracy: 0.3750 - lr: 1.0000e-04\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.7485 - accuracy: 0.8710 - val_loss: 1.2179 - val_accuracy: 0.3750 - lr: 1.0000e-04\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 172ms/step - loss: 0.7181 - accuracy: 0.8548 - val_loss: 1.2180 - val_accuracy: 0.3750 - lr: 1.0000e-04\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.7499 - accuracy: 0.8387 - val_loss: 1.2180 - val_accuracy: 0.3750 - lr: 1.0000e-04\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.7262 - accuracy: 0.8710 - val_loss: 1.2180 - val_accuracy: 0.3750 - lr: 1.0000e-04\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.6739 - accuracy: 0.9032 - val_loss: 1.2180 - val_accuracy: 0.3750 - lr: 1.0000e-04\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.7199 - accuracy: 0.8871 - val_loss: 1.2179 - val_accuracy: 0.3750 - lr: 1.0000e-04\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.7086 - accuracy: 0.8710 - val_loss: 1.2178 - val_accuracy: 0.3750 - lr: 1.0000e-04\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 145ms/step - loss: 0.7188 - accuracy: 0.8387 - val_loss: 1.2176 - val_accuracy: 0.3750 - lr: 1.0000e-04\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.7022 - accuracy: 0.8710 - val_loss: 1.2175 - val_accuracy: 0.3750 - lr: 1.0000e-04\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.6846 - accuracy: 0.8226 - val_loss: 1.2174 - val_accuracy: 0.3750 - lr: 1.0000e-04\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.7066 - accuracy: 0.8548 - val_loss: 1.2171 - val_accuracy: 0.3750 - lr: 1.0000e-04\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.6795 - accuracy: 0.8710 - val_loss: 1.2169 - val_accuracy: 0.3750 - lr: 1.0000e-04\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.6411 - accuracy: 0.8710 - val_loss: 1.2165 - val_accuracy: 0.3750 - lr: 1.0000e-04\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.6338 - accuracy: 0.9032 - val_loss: 1.2161 - val_accuracy: 0.3750 - lr: 1.0000e-04\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.6572 - accuracy: 0.9032 - val_loss: 1.2156 - val_accuracy: 0.3750 - lr: 1.0000e-04\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.6187 - accuracy: 0.9194 - val_loss: 1.2151 - val_accuracy: 0.3750 - lr: 1.0000e-04\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.6419 - accuracy: 0.8871 - val_loss: 1.2145 - val_accuracy: 0.3750 - lr: 1.0000e-04\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.5921 - accuracy: 0.9355 - val_loss: 1.2140 - val_accuracy: 0.3750 - lr: 1.0000e-04\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.6529 - accuracy: 0.8871 - val_loss: 1.2135 - val_accuracy: 0.3750 - lr: 1.0000e-04\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.6398 - accuracy: 0.9355 - val_loss: 1.2129 - val_accuracy: 0.3750 - lr: 1.0000e-04\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.6243 - accuracy: 0.9355 - val_loss: 1.2123 - val_accuracy: 0.3750 - lr: 1.0000e-04\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.6027 - accuracy: 0.9355 - val_loss: 1.2117 - val_accuracy: 0.3750 - lr: 1.0000e-04\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 0.6103 - accuracy: 0.9516 - val_loss: 1.2112 - val_accuracy: 0.3750 - lr: 1.0000e-04\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.5694 - accuracy: 0.9194 - val_loss: 1.2107 - val_accuracy: 0.3750 - lr: 1.0000e-04\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.6024 - accuracy: 0.9194 - val_loss: 1.2102 - val_accuracy: 0.3750 - lr: 1.0000e-04\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.5620 - accuracy: 1.0000 - val_loss: 1.2096 - val_accuracy: 0.3750 - lr: 1.0000e-04\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.5282 - accuracy: 0.9839 - val_loss: 1.2090 - val_accuracy: 0.3750 - lr: 1.0000e-04\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 1.1466 - accuracy: 0.4167\n",
      "Evaluation Metrics for RNN:\n",
      "loss: 1.1466299295425415\n",
      "accuracy: 0.4166666567325592\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "\n",
    "X_train_rnn, X_test_rnn, y_train_rnn, y_test_rnn = train_test_split(X, Y, test_size=0.2)\n",
    "\n",
    "def RNN_model(vocab_size, sequence_length):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=vocab_size, output_dim=128, input_length=sequence_length))\n",
    "    model.add(SimpleRNN(units=128, activation='relu', return_sequences=True))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(SimpleRNN(units=64, activation='sigmoid'))\n",
    "    model.add(Dense(256, activation='sigmoid', kernel_regularizer=l2(0.001)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "    optimizer = Adam(learning_rate=0.001)\n",
    "    \n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=0.0001, verbose=1)\n",
    "#EMBEDDING_DIM = 110\n",
    "rnn_model = RNN_model(vocab_size, max_sequence_length)\n",
    "early_stopping_rnn = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "history_updated_rnn = rnn_model.fit(X_train_rnn, y_train_rnn, validation_data=(X_test_rnn, y_test_rnn), epochs=100, batch_size=64, callbacks=[reduce_lr])\n",
    "\n",
    "# Evaluate the RNN model\n",
    "evaluation_metrics_updated_rnn = rnn_model.evaluate(X_test, Y_test)\n",
    "print(\"Evaluation Metrics for RNN:\")\n",
    "for metric_name, metric_value in zip(rnn_model.metrics_names, evaluation_metrics_updated_rnn):\n",
    "    print(f\"{metric_name}: {metric_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*<h4>LSTM Model<h4>*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1/1 [==============================] - 7s 7s/step - loss: 1.2150 - accuracy: 0.3226 - val_loss: 1.1985 - val_accuracy: 0.3750\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 227ms/step - loss: 1.1559 - accuracy: 0.6290 - val_loss: 1.1964 - val_accuracy: 0.3750\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 1.1063 - accuracy: 0.5484 - val_loss: 1.1944 - val_accuracy: 0.3750\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 1.0632 - accuracy: 0.5806 - val_loss: 1.1926 - val_accuracy: 0.3750\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 1.0078 - accuracy: 0.6129 - val_loss: 1.1907 - val_accuracy: 0.3750\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.9457 - accuracy: 0.5968 - val_loss: 1.1892 - val_accuracy: 0.3750\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.8645 - accuracy: 0.7258 - val_loss: 1.1883 - val_accuracy: 0.6250\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 354ms/step - loss: 0.7540 - accuracy: 0.7903 - val_loss: 1.1874 - val_accuracy: 0.5000\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 139ms/step - loss: 0.6369 - accuracy: 0.7903 - val_loss: 1.1858 - val_accuracy: 0.4375\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 0.5293 - accuracy: 0.7742 - val_loss: 1.1831 - val_accuracy: 0.4375\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 0.4322 - accuracy: 0.8387 - val_loss: 1.1799 - val_accuracy: 0.5000\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 0.3472 - accuracy: 0.9032 - val_loss: 1.1767 - val_accuracy: 0.5625\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.2691 - accuracy: 0.9677 - val_loss: 1.1734 - val_accuracy: 0.5625\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 0.2103 - accuracy: 1.0000 - val_loss: 1.1692 - val_accuracy: 0.4375\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.1652 - accuracy: 1.0000 - val_loss: 1.1630 - val_accuracy: 0.6875\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.2080 - accuracy: 0.9839 - val_loss: 1.1607 - val_accuracy: 0.4375\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.1147 - accuracy: 1.0000 - val_loss: 1.1582 - val_accuracy: 0.4375\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.0990 - accuracy: 1.0000 - val_loss: 1.1559 - val_accuracy: 0.4375\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.0948 - accuracy: 1.0000 - val_loss: 1.1542 - val_accuracy: 0.4375\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.0904 - accuracy: 1.0000 - val_loss: 1.1528 - val_accuracy: 0.5000\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 1.9230 - accuracy: 0.9839 - val_loss: 1.1569 - val_accuracy: 0.4375\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 2.6032 - accuracy: 0.9839 - val_loss: 1.1619 - val_accuracy: 0.3750\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.4465 - accuracy: 0.9677 - val_loss: 1.1652 - val_accuracy: 0.3750\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 4.5590 - accuracy: 0.9677 - val_loss: 1.1654 - val_accuracy: 0.4375\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.3463 - accuracy: 0.9516 - val_loss: 1.1662 - val_accuracy: 0.5000\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.1317 - accuracy: 0.9839 - val_loss: 1.1672 - val_accuracy: 0.5000\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.1025 - accuracy: 1.0000 - val_loss: 1.1683 - val_accuracy: 0.3750\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.1986 - accuracy: 0.9516 - val_loss: 1.1687 - val_accuracy: 0.3750\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.1003 - accuracy: 1.0000 - val_loss: 1.1694 - val_accuracy: 0.3125\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.1122 - accuracy: 0.9839 - val_loss: 1.1696 - val_accuracy: 0.3125\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 1.0841 - accuracy: 0.5833\n",
      "Evaluation Metrics for LSTM:\n",
      "loss: 1.0841199159622192\n",
      "accuracy: 0.5833333134651184\n"
     ]
    }
   ],
   "source": [
    "def LSTM_model(vocab_size, sequence_length):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=vocab_size, output_dim=128, input_length=sequence_length))\n",
    "    model.add(LSTM(units=128, activation='relu', return_sequences=True))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LSTM(units=64, activation='relu'))\n",
    "    model.add(Dense(256, activation='relu', kernel_regularizer=l2(0.001)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "    optimizer = Adam(learning_rate=0.001)\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "#EMBEDDING_DIM = 110\n",
    "lstm_model = LSTM_model(vocab_size, max_sequence_length)\n",
    "early_stopping_rnn = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "history_updated_rnn = lstm_model.fit(X_train_rnn, y_train_rnn, validation_data=(X_test_rnn, y_test_rnn), epochs=100, batch_size=64, callbacks=[early_stopping_rnn])\n",
    "\n",
    "\n",
    "# Evaluate the lstm model\n",
    "evaluation_metrics_updated_lstm = lstm_model.evaluate(X_test, Y_test)\n",
    "print(\"Evaluation Metrics for LSTM:\")\n",
    "for metric_name, metric_value in zip(lstm_model.metrics_names, evaluation_metrics_updated_lstm):\n",
    "    print(f\"{metric_name}: {metric_value}\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*<h4>TRANSFORMER Model<h4>*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, density, rate=0.1, l2_reg=0.01):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            layers.Dense(density, activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(l2_reg)),\n",
    "            layers.Dense(embed_dim, kernel_regularizer=tf.keras.regularizers.l2(l2_reg)),\n",
    "        ])\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "\n",
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions\n",
    "\n",
    "def build_transformer_model(maxlen, vocab_size, embed_dim, num_heads, density, transformer_units, mlp_units, dropout_rate, num_classes):\n",
    "    inputs = layers.Input(shape=(maxlen,))\n",
    "    embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
    "    x = embedding_layer(inputs)\n",
    "    for _ in range(transformer_units):\n",
    "        x = TransformerBlock(embed_dim, num_heads, density, rate=dropout_rate)(x)\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    for dim in mlp_units:\n",
    "        x = layers.Dense(dim, activation=\"relu\")(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "    outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = max_sequence_length\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "embed_dim = 120\n",
    "num_heads = 2\n",
    "density = 3\n",
    "transformer_units = 4\n",
    "mlp_units = [128]\n",
    "dropout_rate = 0.5\n",
    "num_classes = len(df['grade'].unique())\n",
    "\n",
    "transformer_model = build_transformer_model(maxlen, vocab_size, embed_dim, num_heads, density, transformer_units, mlp_units, dropout_rate, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "3/3 [==============================] - 16s 757ms/step - loss: 1.8096 - accuracy: 0.2879 - val_loss: 1.6058 - val_accuracy: 0.4167\n",
      "Epoch 2/30\n",
      "3/3 [==============================] - 0s 75ms/step - loss: 1.6686 - accuracy: 0.4242 - val_loss: 1.5668 - val_accuracy: 0.4167\n",
      "Epoch 3/30\n",
      "3/3 [==============================] - 1s 214ms/step - loss: 1.5590 - accuracy: 0.4394 - val_loss: 1.7208 - val_accuracy: 0.4167\n",
      "Epoch 4/30\n",
      "3/3 [==============================] - 0s 79ms/step - loss: 1.4725 - accuracy: 0.5000 - val_loss: 1.5734 - val_accuracy: 0.4167\n",
      "Epoch 5/30\n",
      "3/3 [==============================] - 1s 241ms/step - loss: 1.4542 - accuracy: 0.5000 - val_loss: 1.5181 - val_accuracy: 0.4167\n",
      "Epoch 6/30\n",
      "3/3 [==============================] - 0s 70ms/step - loss: 1.4267 - accuracy: 0.5758 - val_loss: 1.6197 - val_accuracy: 0.2500\n",
      "Epoch 7/30\n",
      "3/3 [==============================] - 0s 96ms/step - loss: 1.4814 - accuracy: 0.4545 - val_loss: 1.4468 - val_accuracy: 0.6667\n",
      "Epoch 8/30\n",
      "3/3 [==============================] - 0s 89ms/step - loss: 1.3848 - accuracy: 0.5455 - val_loss: 1.3650 - val_accuracy: 0.5000\n",
      "Epoch 9/30\n",
      "3/3 [==============================] - 0s 78ms/step - loss: 1.2268 - accuracy: 0.6515 - val_loss: 1.3726 - val_accuracy: 0.4167\n",
      "Epoch 10/30\n",
      "3/3 [==============================] - 0s 64ms/step - loss: 1.1761 - accuracy: 0.6667 - val_loss: 1.3234 - val_accuracy: 0.6667\n",
      "Epoch 11/30\n",
      "3/3 [==============================] - 0s 66ms/step - loss: 1.0148 - accuracy: 0.7879 - val_loss: 1.3639 - val_accuracy: 0.6667\n",
      "Epoch 12/30\n",
      "3/3 [==============================] - 0s 97ms/step - loss: 0.8744 - accuracy: 0.8182 - val_loss: 1.5726 - val_accuracy: 0.5000\n",
      "Epoch 13/30\n",
      "3/3 [==============================] - 0s 100ms/step - loss: 0.9037 - accuracy: 0.8333 - val_loss: 1.4580 - val_accuracy: 0.5000\n",
      "Epoch 14/30\n",
      "3/3 [==============================] - 0s 72ms/step - loss: 0.6762 - accuracy: 0.8788 - val_loss: 1.0788 - val_accuracy: 0.7500\n",
      "Epoch 15/30\n",
      "3/3 [==============================] - 0s 65ms/step - loss: 0.6974 - accuracy: 0.8939 - val_loss: 0.9039 - val_accuracy: 0.8333\n",
      "Epoch 16/30\n",
      "3/3 [==============================] - 0s 64ms/step - loss: 0.6588 - accuracy: 0.9091 - val_loss: 0.7299 - val_accuracy: 0.8333\n",
      "Epoch 17/30\n",
      "3/3 [==============================] - 0s 70ms/step - loss: 0.5687 - accuracy: 0.9697 - val_loss: 0.7974 - val_accuracy: 0.7500\n",
      "Epoch 18/30\n",
      "3/3 [==============================] - 0s 86ms/step - loss: 0.5011 - accuracy: 1.0000 - val_loss: 0.6188 - val_accuracy: 0.8333\n",
      "Epoch 19/30\n",
      "3/3 [==============================] - 0s 68ms/step - loss: 0.4658 - accuracy: 1.0000 - val_loss: 0.5103 - val_accuracy: 0.9167\n",
      "Epoch 20/30\n",
      "3/3 [==============================] - 0s 63ms/step - loss: 0.4260 - accuracy: 1.0000 - val_loss: 0.5760 - val_accuracy: 0.9167\n",
      "Epoch 21/30\n",
      "3/3 [==============================] - 0s 62ms/step - loss: 0.4308 - accuracy: 0.9848 - val_loss: 0.5197 - val_accuracy: 0.9167\n",
      "Epoch 22/30\n",
      "3/3 [==============================] - 0s 64ms/step - loss: 0.4057 - accuracy: 1.0000 - val_loss: 0.5465 - val_accuracy: 0.9167\n",
      "Epoch 23/30\n",
      "3/3 [==============================] - 0s 97ms/step - loss: 0.4351 - accuracy: 0.9848 - val_loss: 0.5094 - val_accuracy: 1.0000\n",
      "Epoch 24/30\n",
      "3/3 [==============================] - 0s 68ms/step - loss: 0.3828 - accuracy: 1.0000 - val_loss: 0.5157 - val_accuracy: 0.9167\n",
      "Epoch 25/30\n",
      "3/3 [==============================] - 0s 128ms/step - loss: 0.3770 - accuracy: 1.0000 - val_loss: 0.6046 - val_accuracy: 0.9167\n",
      "Epoch 26/30\n",
      "3/3 [==============================] - 0s 69ms/step - loss: 0.3744 - accuracy: 1.0000 - val_loss: 0.6749 - val_accuracy: 0.9167\n",
      "Epoch 27/30\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.3632 - accuracy: 1.0000 - val_loss: 0.7157 - val_accuracy: 0.9167\n",
      "Epoch 28/30\n",
      "3/3 [==============================] - 0s 70ms/step - loss: 0.3573 - accuracy: 1.0000 - val_loss: 0.7244 - val_accuracy: 0.9167\n",
      "Epoch 29/30\n",
      "3/3 [==============================] - 0s 65ms/step - loss: 0.3545 - accuracy: 1.0000 - val_loss: 0.7324 - val_accuracy: 0.9167\n",
      "Epoch 30/30\n",
      "3/3 [==============================] - 0s 82ms/step - loss: 0.3513 - accuracy: 1.0000 - val_loss: 0.7296 - val_accuracy: 0.9167\n",
      "Epoch 1/30\n",
      "3/3 [==============================] - 0s 95ms/step - loss: 0.3436 - accuracy: 1.0000 - val_loss: 0.7292 - val_accuracy: 0.9167\n",
      "Epoch 2/30\n",
      "3/3 [==============================] - 0s 60ms/step - loss: 0.3392 - accuracy: 1.0000 - val_loss: 0.7446 - val_accuracy: 0.9167\n",
      "Epoch 3/30\n",
      "3/3 [==============================] - 0s 77ms/step - loss: 0.3332 - accuracy: 1.0000 - val_loss: 0.7563 - val_accuracy: 0.9167\n",
      "Epoch 4/30\n",
      "3/3 [==============================] - 0s 64ms/step - loss: 0.3292 - accuracy: 1.0000 - val_loss: 0.7673 - val_accuracy: 0.9167\n",
      "Epoch 5/30\n",
      "3/3 [==============================] - 0s 66ms/step - loss: 0.3266 - accuracy: 1.0000 - val_loss: 0.7748 - val_accuracy: 0.9167\n",
      "Epoch 6/30\n",
      "3/3 [==============================] - 0s 59ms/step - loss: 0.3228 - accuracy: 1.0000 - val_loss: 0.7929 - val_accuracy: 0.9167\n",
      "Epoch 7/30\n",
      "3/3 [==============================] - 0s 64ms/step - loss: 0.3162 - accuracy: 1.0000 - val_loss: 0.8149 - val_accuracy: 0.9167\n",
      "Epoch 8/30\n",
      "3/3 [==============================] - 0s 55ms/step - loss: 0.3116 - accuracy: 1.0000 - val_loss: 0.8327 - val_accuracy: 0.9167\n",
      "Epoch 9/30\n",
      "3/3 [==============================] - 0s 65ms/step - loss: 0.3074 - accuracy: 1.0000 - val_loss: 0.8455 - val_accuracy: 0.9167\n",
      "Epoch 10/30\n",
      "3/3 [==============================] - 0s 86ms/step - loss: 0.3036 - accuracy: 1.0000 - val_loss: 0.8558 - val_accuracy: 0.9167\n",
      "Epoch 11/30\n",
      "3/3 [==============================] - 0s 84ms/step - loss: 0.3005 - accuracy: 1.0000 - val_loss: 0.8706 - val_accuracy: 0.9167\n",
      "Epoch 12/30\n",
      "3/3 [==============================] - 0s 78ms/step - loss: 0.2968 - accuracy: 1.0000 - val_loss: 0.8889 - val_accuracy: 0.9167\n",
      "Epoch 13/30\n",
      "3/3 [==============================] - 0s 75ms/step - loss: 0.2928 - accuracy: 1.0000 - val_loss: 0.9069 - val_accuracy: 0.9167\n",
      "Epoch 14/30\n",
      "3/3 [==============================] - 0s 78ms/step - loss: 0.2899 - accuracy: 1.0000 - val_loss: 0.9233 - val_accuracy: 0.9167\n",
      "Epoch 15/30\n",
      "3/3 [==============================] - 0s 70ms/step - loss: 0.2833 - accuracy: 1.0000 - val_loss: 0.9348 - val_accuracy: 0.9167\n",
      "Epoch 16/30\n",
      "3/3 [==============================] - 0s 65ms/step - loss: 0.2800 - accuracy: 1.0000 - val_loss: 0.9416 - val_accuracy: 0.9167\n",
      "Epoch 17/30\n",
      "3/3 [==============================] - 0s 73ms/step - loss: 0.2770 - accuracy: 1.0000 - val_loss: 0.9477 - val_accuracy: 0.9167\n",
      "Epoch 18/30\n",
      "3/3 [==============================] - 0s 75ms/step - loss: 0.2713 - accuracy: 1.0000 - val_loss: 0.9519 - val_accuracy: 0.9167\n",
      "Epoch 19/30\n",
      "3/3 [==============================] - 0s 79ms/step - loss: 0.2684 - accuracy: 1.0000 - val_loss: 0.9502 - val_accuracy: 0.9167\n",
      "Epoch 20/30\n",
      "3/3 [==============================] - 0s 108ms/step - loss: 0.2673 - accuracy: 1.0000 - val_loss: 0.9418 - val_accuracy: 0.9167\n",
      "Epoch 21/30\n",
      "3/3 [==============================] - 0s 86ms/step - loss: 0.2610 - accuracy: 1.0000 - val_loss: 0.9342 - val_accuracy: 0.9167\n",
      "Epoch 22/30\n",
      "3/3 [==============================] - 0s 66ms/step - loss: 0.2573 - accuracy: 1.0000 - val_loss: 0.9279 - val_accuracy: 0.9167\n",
      "Epoch 23/30\n",
      "3/3 [==============================] - 0s 66ms/step - loss: 0.2651 - accuracy: 0.9848 - val_loss: 0.9286 - val_accuracy: 0.9167\n",
      "Epoch 24/30\n",
      "3/3 [==============================] - 0s 63ms/step - loss: 0.2494 - accuracy: 1.0000 - val_loss: 0.9324 - val_accuracy: 0.9167\n",
      "Epoch 25/30\n",
      "3/3 [==============================] - 0s 68ms/step - loss: 0.2452 - accuracy: 1.0000 - val_loss: 0.9345 - val_accuracy: 0.9167\n",
      "Epoch 26/30\n",
      "3/3 [==============================] - 0s 141ms/step - loss: 0.2416 - accuracy: 1.0000 - val_loss: 0.9345 - val_accuracy: 0.9167\n",
      "Epoch 27/30\n",
      "3/3 [==============================] - 0s 94ms/step - loss: 0.2414 - accuracy: 1.0000 - val_loss: 0.9186 - val_accuracy: 0.9167\n",
      "Epoch 28/30\n",
      "3/3 [==============================] - 0s 114ms/step - loss: 0.2354 - accuracy: 1.0000 - val_loss: 0.8971 - val_accuracy: 0.9167\n",
      "Epoch 29/30\n",
      "3/3 [==============================] - 0s 91ms/step - loss: 0.2315 - accuracy: 1.0000 - val_loss: 0.8724 - val_accuracy: 0.9167\n",
      "Epoch 30/30\n",
      "3/3 [==============================] - 0s 167ms/step - loss: 0.2279 - accuracy: 1.0000 - val_loss: 0.8364 - val_accuracy: 0.9167\n"
     ]
    }
   ],
   "source": [
    "transformer_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "transformer_model.fit(X_train, Y_train, epochs=30, batch_size=32, validation_data=(X_test, Y_test))\n",
    "history = transformer_model.fit(X_train, Y_train, epochs=30, batch_size=32, validation_data=(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 304ms/step - loss: 0.8364 - accuracy: 0.9167\n",
      "Evaluation Metrics Transformer:\n",
      "loss: 0.8364205956459045\n",
      "accuracy: 0.9166666865348816\n"
     ]
    }
   ],
   "source": [
    "evaluation_metrics_transformer = transformer_model.evaluate(X_test, Y_test)\n",
    "print(\"Evaluation Metrics Transformer:\")\n",
    "for metric_name, metric_value in zip(transformer_model.metrics_names, evaluation_metrics_transformer):\n",
    "    print(f\"{metric_name}: {metric_value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 133ms/step - loss: 0.8364 - accuracy: 0.9167\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   28   1   2  47  87  17]\n",
      " [  0   0   0   0   0   0   0   0   6 103 104  60 105 106 107 108  20   1\n",
      "  109  26   3   4   1  13]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    8 202   7   9   2  21]\n",
      " [  0   0   0   0   0   0   0   0   0   0   1 143   9  20   6  77   3   4\n",
      "    1  13  18  15   2  24]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  49   1\n",
      "   12   2  18  39 140  11]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   1 311   3  11]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  25  42\n",
      "    2   8  17   7  11  14]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  21\n",
      "    2   8 116   7  11  14]\n",
      " [  0   0   0   0   0   0   0   0   0  19 240 241   2  95  24  10 242   2\n",
      "    4  86   5  46  14  32]\n",
      " [  0   0   0   0   0   0   0  10 101  55  22 281   3   4   1  13  18  15\n",
      "    2  24   1   8  19  59]\n",
      " [  0   0   0   0   0   0   0   0   0   0  43   3   4   1  12  15  79   2\n",
      "  272  25   2   5   9  17]\n",
      " [  0   0   0   0   0   0   0   0   6  55  79  80 173 174  30  15  81   1\n",
      "   13  39 175   2   9  26]]\n",
      "\n",
      "Real and Predicted Values:\n",
      "    Real  Predicted\n",
      "0      1          1\n",
      "1      2          2\n",
      "2      0          0\n",
      "3      2          2\n",
      "4      1          0\n",
      "5      2          2\n",
      "6      0          0\n",
      "7      0          0\n",
      "8      1          1\n",
      "9      2          2\n",
      "10     2          2\n",
      "11     1          1\n"
     ]
    }
   ],
   "source": [
    "evaluation_metrics_transformer = transformer_model.evaluate(X_test, Y_test)\n",
    "\n",
    "predictions = transformer_model.predict(X_test)\n",
    "print(X_test)\n",
    "\n",
    "# Convert one-hot encoded labels to class indices\n",
    "y_true = np.argmax(Y_test, axis=1)\n",
    "\n",
    "# Convert predicted probabilities to class indices\n",
    "y_pred = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Create DataFrame\n",
    "results_df = pd.DataFrame({\"Real\": y_true, \"Predicted\": y_pred})\n",
    "\n",
    "# Display DataFrame\n",
    "print(\"\\nReal and Predicted Values:\")\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy RNN: 0.4166666567325592\n",
      "Accuracy LSTM: 0.5833333134651184\n",
      "Accuracy Transformer: 0.9166666865348816\n"
     ]
    }
   ],
   "source": [
    "# Evaluate RNN model\n",
    "rnn_accuracy = rnn_model.evaluate(X_test, Y_test, verbose=0)[1]\n",
    "print(\"Accuracy RNN:\", rnn_accuracy)\n",
    "\n",
    "# Evaluate LSTM model\n",
    "lstm_accuracy = lstm_model.evaluate(X_test, Y_test, verbose=0)[1]\n",
    "print(\"Accuracy LSTM:\", lstm_accuracy)\n",
    "\n",
    "# Evaluate Transformer model\n",
    "transformer_accuracy = transformer_model.evaluate(X_test, Y_test, verbose=0)[1]\n",
    "print(\"Accuracy Transformer:\", transformer_accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model (Transformer) with accuracy 0.9166666865348816 has been saved to './savedModels/q3_model.h5'\n"
     ]
    }
   ],
   "source": [
    "# Determine the best model\n",
    "best_model_name, best_model_accuracy = max([('RNN', rnn_accuracy), ('LSTM', lstm_accuracy), ('Transformer', transformer_accuracy)], key=lambda x: x[1])\n",
    "\n",
    "save_path = './savedModels/q3_model.h5'\n",
    "# Save the best model\n",
    "if best_model_name == 'RNN':\n",
    "    rnn_model.save(save_path)\n",
    "elif best_model_name == 'LSTM':\n",
    "    lstm_model.save(save_path)\n",
    "elif best_model_name == 'Transformer':\n",
    "    transformer_model.save(save_path)\n",
    "\n",
    "print(f\"The best model ({best_model_name}) with accuracy {best_model_accuracy} has been saved to '{save_path}'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
