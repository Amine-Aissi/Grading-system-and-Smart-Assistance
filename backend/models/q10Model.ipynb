{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\amine\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow as keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.regularizers import l2\n",
    "from keras import layers\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Embedding, SimpleRNN, Dense, Dropout, LSTM, Bidirectional, Input\n",
    "from keras.layers import Attention, GlobalAveragePooling1D\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import stanza\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import BatchNormalization, Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*<h3>Load Data<h3>*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_id</th>\n",
       "      <th>answer</th>\n",
       "      <th>grade</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>سورة المدثر هي السورة التي نزلت بكاملها دفعة و...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>السورة التي نزلت كاملةً دفعةً واحدة هي سورة ال...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>سورة بها 56 آية في الجزء 29 نزلت دفعة واحدة.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>هناك سورة نزلت مرة واحدة وتعد من السور الطويلة...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>السورة الكاملة التي نزلت في وقت واحد تحتوي على...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10</td>\n",
       "      <td>السورة التي نزلت بكاملها تبدأ بأمر النبي صلى ا...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10</td>\n",
       "      <td>سورة المدثر نزلت مرة واحدة بكاملها.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10</td>\n",
       "      <td>السورة التي نزلت بأكملها في مرة واحدة هي سورة ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10</td>\n",
       "      <td>سورة النور هي السورة التي نزلت دفعة واحدة بكام...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>السورة التي نزلت كاملة هي سورة الرحمن.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   question_id                                             answer  grade\n",
       "0           10  سورة المدثر هي السورة التي نزلت بكاملها دفعة و...      2\n",
       "1           10  السورة التي نزلت كاملةً دفعةً واحدة هي سورة ال...      2\n",
       "2           10       سورة بها 56 آية في الجزء 29 نزلت دفعة واحدة.      1\n",
       "3           10  هناك سورة نزلت مرة واحدة وتعد من السور الطويلة...      1\n",
       "4           10  السورة الكاملة التي نزلت في وقت واحد تحتوي على...      1\n",
       "5           10  السورة التي نزلت بكاملها تبدأ بأمر النبي صلى ا...      1\n",
       "6           10                سورة المدثر نزلت مرة واحدة بكاملها.      2\n",
       "7           10  السورة التي نزلت بأكملها في مرة واحدة هي سورة ...      0\n",
       "8           10  سورة النور هي السورة التي نزلت دفعة واحدة بكام...      0\n",
       "9           10             السورة التي نزلت كاملة هي سورة الرحمن.      0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read csv file into pandas\n",
    "df = pd.read_csv(\"../datasets/shuffled10.csv\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*<h3>EDA<h3>*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100 entries, 0 to 99\n",
      "Data columns (total 3 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   question_id  100 non-null    int64 \n",
      " 1   answer       100 non-null    object\n",
      " 2   grade        100 non-null    int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 2.5+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "grade\n",
       "0    27\n",
       "1    38\n",
       "2    35\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('grade').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(data=df, x='grade')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*<h3>Data Cleaning<h3>*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('question_id', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     سورة المدثر هي السورة التي نزلت بكاملها دفعة و...\n",
      "1     السورة التي نزلت كاملةً دفعةً واحدة هي سورة ال...\n",
      "2          سورة بها 56 آية في الجزء 29 نزلت دفعة واحدة.\n",
      "3     هناك سورة نزلت مرة واحدة وتعد من السور الطويلة...\n",
      "4     السورة الكاملة التي نزلت في وقت واحد تحتوي على...\n",
      "                            ...                        \n",
      "95    سورة المدثر كانت السورة القرآنية التي أُنزلت د...\n",
      "96    السورة التي نزلت دفعة واحدة تشتهر بتأكيدها على...\n",
      "97                                         سورة الزلزلة\n",
      "98      السورة التي نزلت دفعة واحدة هي من السور المكية.\n",
      "99                               الجواب هو: سورة المدثر\n",
      "Name: answer, Length: 100, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df.drop_duplicates(inplace=True)\n",
    "print(df['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*<h3>Data Pre-Preocessing<h3>*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "704a103f3abd4cd0b89f60b10d604b94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.7.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-09 22:56:01 INFO: Downloading default packages for language: ar (Arabic) ...\n",
      "2024-01-09 22:56:02 INFO: File exists: C:\\Users\\amine\\stanza_resources\\ar\\default.zip\n",
      "2024-01-09 22:56:09 INFO: Finished downloading models and saved to C:\\Users\\amine\\stanza_resources.\n",
      "2024-01-09 22:56:09 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf1ddf70881a464d8b1a7849b69c4d21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.7.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-09 22:56:14 INFO: Loading these models for language: ar (Arabic):\n",
      "=============================\n",
      "| Processor | Package       |\n",
      "-----------------------------\n",
      "| tokenize  | padt          |\n",
      "| mwt       | padt          |\n",
      "| pos       | padt_charlm   |\n",
      "| lemma     | padt_nocharlm |\n",
      "| depparse  | padt_charlm   |\n",
      "| ner       | aqmar_charlm  |\n",
      "=============================\n",
      "\n",
      "2024-01-09 22:56:14 INFO: Using device: cpu\n",
      "2024-01-09 22:56:14 INFO: Loading: tokenize\n",
      "2024-01-09 22:56:17 INFO: Loading: mwt\n",
      "2024-01-09 22:56:17 INFO: Loading: pos\n",
      "2024-01-09 22:56:18 INFO: Loading: lemma\n",
      "2024-01-09 22:56:18 INFO: Loading: depparse\n",
      "2024-01-09 22:56:19 INFO: Loading: ner\n",
      "2024-01-09 22:56:20 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "df['grade'] = le.fit_transform(df['grade'])\n",
    "\n",
    "stanza.download('ar')\n",
    "nlp = stanza.Pipeline('ar')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    doc = nlp(text)\n",
    "    tokens = [word.lemma for sent in doc.sentences for word in sent.words if word.upos != 'PUNCT']\n",
    "    return tokens\n",
    "\n",
    "df['answer'] = df['answer'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0 ...   2  10   7]\n",
      " [  0   0   0 ...   2   1  12]\n",
      " [  0   0   0 ...   3  10   7]\n",
      " ...\n",
      " [  0   0   0 ...   0   1 135]\n",
      " [  0   0   0 ...  15  27  75]\n",
      " [  0   0   0 ...   2   1  12]]\n",
      "0     [سُورَة, المدثر, هُوَ, سُورَة, اَلَّذِي, نَزَل...\n",
      "1     [سُورَة, اَلَّذِي, نَزَل, كَامِل, دَفعَة, وَ, ...\n",
      "2     [سُورَة, بِ, هُوَ, 56, آية, فِي, جُزء, 29, نَز...\n",
      "3     [هُنَاكَ, سُورَة, نَزَل, مَرَّة, وَاحِد, وَ, ع...\n",
      "4     [سُورَة, كَامِل, اَلَّذِي, نَزَل, فِي, وَقت, و...\n",
      "                            ...                        \n",
      "95    [سُورَة, المدثر, كَان, سُورَة, قَرِينِيّ, اَلَ...\n",
      "96    [سُورَة, اَلَّذِي, نَزَل, دَفعَة, وَاحِد, اِشت...\n",
      "97                                    [سُورَة, الزلزلة]\n",
      "98    [سُورَة, اَلَّذِي, نَزَل, دَفعَة, وَاحِد, هُوَ...\n",
      "99                       [جَوَاب, هُوَ, سُورَة, مَدثَر]\n",
      "Name: answer, Length: 100, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Tokenization\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df['answer'])\n",
    "sequences = tokenizer.texts_to_sequences(df['answer'])\n",
    "max_sequence_length = max(len(s) for s in sequences)\n",
    "sequences = pad_sequences(sequences,max_sequence_length)\n",
    "word2idx = tokenizer.word_index\n",
    "vocab_size = len(word2idx) + 1\n",
    "\n",
    "\n",
    "X = pad_sequences(sequences, padding='post', truncating='post', maxlen=max_sequence_length)\n",
    "\n",
    "print(sequences)\n",
    "print(df['answer'])\n",
    "\n",
    "Y = to_categorical(df['grade'], num_classes=3)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.15, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*<h3>build Models<h3>*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*<h4>RNN Model<h4>*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\amine\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:From c:\\Users\\amine\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\amine\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "2/2 [==============================] - 17s 2s/step - loss: 1.2572 - accuracy: 0.4000 - val_loss: 1.3385 - val_accuracy: 0.2500 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "2/2 [==============================] - 0s 350ms/step - loss: 1.1937 - accuracy: 0.4375 - val_loss: 1.2840 - val_accuracy: 0.2500 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "2/2 [==============================] - 0s 252ms/step - loss: 1.1125 - accuracy: 0.5000 - val_loss: 1.2676 - val_accuracy: 0.2500 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "2/2 [==============================] - 0s 228ms/step - loss: 1.1428 - accuracy: 0.4125 - val_loss: 1.2569 - val_accuracy: 0.2500 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "2/2 [==============================] - 0s 114ms/step - loss: 1.0822 - accuracy: 0.5375 - val_loss: 1.2445 - val_accuracy: 0.2500 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "2/2 [==============================] - 0s 109ms/step - loss: 1.0114 - accuracy: 0.6125 - val_loss: 1.2581 - val_accuracy: 0.2500 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.9559 - accuracy: 0.7031\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "2/2 [==============================] - 0s 115ms/step - loss: 0.9690 - accuracy: 0.7000 - val_loss: 1.2806 - val_accuracy: 0.2500 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "2/2 [==============================] - 0s 335ms/step - loss: 0.8935 - accuracy: 0.7000 - val_loss: 1.2809 - val_accuracy: 0.2500 - lr: 2.0000e-04\n",
      "Epoch 9/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.8429 - accuracy: 0.7500\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.0001.\n",
      "2/2 [==============================] - 0s 165ms/step - loss: 0.8478 - accuracy: 0.7625 - val_loss: 1.2791 - val_accuracy: 0.2500 - lr: 2.0000e-04\n",
      "Epoch 10/100\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.8396 - accuracy: 0.8000 - val_loss: 1.2791 - val_accuracy: 0.2500 - lr: 1.0000e-04\n",
      "Epoch 11/100\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 0.8626 - accuracy: 0.7250 - val_loss: 1.2784 - val_accuracy: 0.2500 - lr: 1.0000e-04\n",
      "Epoch 12/100\n",
      "2/2 [==============================] - 0s 219ms/step - loss: 0.8800 - accuracy: 0.7750 - val_loss: 1.2768 - val_accuracy: 0.2500 - lr: 1.0000e-04\n",
      "Epoch 13/100\n",
      "2/2 [==============================] - 0s 183ms/step - loss: 0.8897 - accuracy: 0.7250 - val_loss: 1.2741 - val_accuracy: 0.2500 - lr: 1.0000e-04\n",
      "Epoch 14/100\n",
      "2/2 [==============================] - 0s 153ms/step - loss: 0.8512 - accuracy: 0.7750 - val_loss: 1.2712 - val_accuracy: 0.2500 - lr: 1.0000e-04\n",
      "Epoch 15/100\n",
      "2/2 [==============================] - 0s 199ms/step - loss: 0.8052 - accuracy: 0.8000 - val_loss: 1.2686 - val_accuracy: 0.2500 - lr: 1.0000e-04\n",
      "Epoch 16/100\n",
      "2/2 [==============================] - 0s 122ms/step - loss: 0.7857 - accuracy: 0.8750 - val_loss: 1.2653 - val_accuracy: 0.2500 - lr: 1.0000e-04\n",
      "Epoch 17/100\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.8045 - accuracy: 0.8250 - val_loss: 1.2627 - val_accuracy: 0.2500 - lr: 1.0000e-04\n",
      "Epoch 18/100\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.8255 - accuracy: 0.8000 - val_loss: 1.2621 - val_accuracy: 0.2500 - lr: 1.0000e-04\n",
      "Epoch 19/100\n",
      "2/2 [==============================] - 0s 315ms/step - loss: 0.7919 - accuracy: 0.7875 - val_loss: 1.2623 - val_accuracy: 0.2500 - lr: 1.0000e-04\n",
      "Epoch 20/100\n",
      "2/2 [==============================] - 0s 103ms/step - loss: 0.7558 - accuracy: 0.8625 - val_loss: 1.2624 - val_accuracy: 0.2500 - lr: 1.0000e-04\n",
      "Epoch 21/100\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 0.8182 - accuracy: 0.7500 - val_loss: 1.2626 - val_accuracy: 0.2500 - lr: 1.0000e-04\n",
      "Epoch 22/100\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 0.7595 - accuracy: 0.8125 - val_loss: 1.2625 - val_accuracy: 0.2500 - lr: 1.0000e-04\n",
      "Epoch 23/100\n",
      "2/2 [==============================] - 0s 103ms/step - loss: 0.7844 - accuracy: 0.8125 - val_loss: 1.2616 - val_accuracy: 0.2500 - lr: 1.0000e-04\n",
      "Epoch 24/100\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 0.7806 - accuracy: 0.8250 - val_loss: 1.2604 - val_accuracy: 0.2500 - lr: 1.0000e-04\n",
      "Epoch 25/100\n",
      "2/2 [==============================] - 0s 114ms/step - loss: 0.7763 - accuracy: 0.8375 - val_loss: 1.2599 - val_accuracy: 0.2500 - lr: 1.0000e-04\n",
      "Epoch 26/100\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.7653 - accuracy: 0.8125 - val_loss: 1.2593 - val_accuracy: 0.2500 - lr: 1.0000e-04\n",
      "Epoch 27/100\n",
      "2/2 [==============================] - 0s 129ms/step - loss: 0.7580 - accuracy: 0.8500 - val_loss: 1.2571 - val_accuracy: 0.2500 - lr: 1.0000e-04\n",
      "Epoch 28/100\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 0.7423 - accuracy: 0.8250 - val_loss: 1.2555 - val_accuracy: 0.2500 - lr: 1.0000e-04\n",
      "Epoch 29/100\n",
      "2/2 [==============================] - 0s 130ms/step - loss: 0.7286 - accuracy: 0.8375 - val_loss: 1.2537 - val_accuracy: 0.2500 - lr: 1.0000e-04\n",
      "Epoch 30/100\n",
      "2/2 [==============================] - 0s 292ms/step - loss: 0.7893 - accuracy: 0.8000 - val_loss: 1.2524 - val_accuracy: 0.2500 - lr: 1.0000e-04\n",
      "Epoch 31/100\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.7252 - accuracy: 0.8875 - val_loss: 1.2508 - val_accuracy: 0.2500 - lr: 1.0000e-04\n",
      "Epoch 32/100\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 0.7485 - accuracy: 0.8500 - val_loss: 1.2487 - val_accuracy: 0.2500 - lr: 1.0000e-04\n",
      "Epoch 33/100\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.6845 - accuracy: 0.8875 - val_loss: 1.2473 - val_accuracy: 0.2500 - lr: 1.0000e-04\n",
      "Epoch 34/100\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.6886 - accuracy: 0.8750 - val_loss: 1.2452 - val_accuracy: 0.2500 - lr: 1.0000e-04\n",
      "Epoch 35/100\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.7027 - accuracy: 0.8625 - val_loss: 1.2419 - val_accuracy: 0.2500 - lr: 1.0000e-04\n",
      "Epoch 36/100\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.6670 - accuracy: 0.8500 - val_loss: 1.2389 - val_accuracy: 0.2500 - lr: 1.0000e-04\n",
      "Epoch 37/100\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.6548 - accuracy: 0.9125 - val_loss: 1.2374 - val_accuracy: 0.2500 - lr: 1.0000e-04\n",
      "Epoch 38/100\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.6662 - accuracy: 0.8500 - val_loss: 1.2365 - val_accuracy: 0.2500 - lr: 1.0000e-04\n",
      "Epoch 39/100\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.6516 - accuracy: 0.9000 - val_loss: 1.2354 - val_accuracy: 0.2500 - lr: 1.0000e-04\n",
      "Epoch 40/100\n",
      "2/2 [==============================] - 0s 104ms/step - loss: 0.6678 - accuracy: 0.8750 - val_loss: 1.2329 - val_accuracy: 0.3000 - lr: 1.0000e-04\n",
      "Epoch 41/100\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 0.6423 - accuracy: 0.8750 - val_loss: 1.2306 - val_accuracy: 0.3000 - lr: 1.0000e-04\n",
      "Epoch 42/100\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 0.6136 - accuracy: 0.9000 - val_loss: 1.2290 - val_accuracy: 0.3000 - lr: 1.0000e-04\n",
      "Epoch 43/100\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.6230 - accuracy: 0.8875 - val_loss: 1.2285 - val_accuracy: 0.3000 - lr: 1.0000e-04\n",
      "Epoch 44/100\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.6266 - accuracy: 0.9000 - val_loss: 1.2276 - val_accuracy: 0.3000 - lr: 1.0000e-04\n",
      "Epoch 45/100\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.6193 - accuracy: 0.8875 - val_loss: 1.2261 - val_accuracy: 0.3000 - lr: 1.0000e-04\n",
      "Epoch 46/100\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.6145 - accuracy: 0.8500 - val_loss: 1.2231 - val_accuracy: 0.3000 - lr: 1.0000e-04\n",
      "Epoch 47/100\n",
      "2/2 [==============================] - 0s 103ms/step - loss: 0.5991 - accuracy: 0.9000 - val_loss: 1.2184 - val_accuracy: 0.3000 - lr: 1.0000e-04\n",
      "Epoch 48/100\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 0.5690 - accuracy: 0.9125 - val_loss: 1.2141 - val_accuracy: 0.3000 - lr: 1.0000e-04\n",
      "Epoch 49/100\n",
      "2/2 [==============================] - 0s 164ms/step - loss: 0.5800 - accuracy: 0.8875 - val_loss: 1.2110 - val_accuracy: 0.3000 - lr: 1.0000e-04\n",
      "Epoch 50/100\n",
      "2/2 [==============================] - 0s 98ms/step - loss: 0.5793 - accuracy: 0.8875 - val_loss: 1.2076 - val_accuracy: 0.3000 - lr: 1.0000e-04\n",
      "Epoch 51/100\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.5668 - accuracy: 0.8750 - val_loss: 1.2033 - val_accuracy: 0.3000 - lr: 1.0000e-04\n",
      "Epoch 52/100\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.5583 - accuracy: 0.9000 - val_loss: 1.2001 - val_accuracy: 0.4000 - lr: 1.0000e-04\n",
      "Epoch 53/100\n",
      "2/2 [==============================] - 0s 178ms/step - loss: 0.5435 - accuracy: 0.9250 - val_loss: 1.1978 - val_accuracy: 0.4000 - lr: 1.0000e-04\n",
      "Epoch 54/100\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.5244 - accuracy: 0.9125 - val_loss: 1.1953 - val_accuracy: 0.5000 - lr: 1.0000e-04\n",
      "Epoch 55/100\n",
      "2/2 [==============================] - 0s 105ms/step - loss: 0.5523 - accuracy: 0.9125 - val_loss: 1.1924 - val_accuracy: 0.5000 - lr: 1.0000e-04\n",
      "Epoch 56/100\n",
      "2/2 [==============================] - 0s 118ms/step - loss: 0.5204 - accuracy: 0.9250 - val_loss: 1.1907 - val_accuracy: 0.5000 - lr: 1.0000e-04\n",
      "Epoch 57/100\n",
      "2/2 [==============================] - 0s 111ms/step - loss: 0.5516 - accuracy: 0.8875 - val_loss: 1.1899 - val_accuracy: 0.5000 - lr: 1.0000e-04\n",
      "Epoch 58/100\n",
      "2/2 [==============================] - 0s 111ms/step - loss: 0.5119 - accuracy: 0.9375 - val_loss: 1.1891 - val_accuracy: 0.5000 - lr: 1.0000e-04\n",
      "Epoch 59/100\n",
      "2/2 [==============================] - 0s 192ms/step - loss: 0.5109 - accuracy: 0.9125 - val_loss: 1.1880 - val_accuracy: 0.5000 - lr: 1.0000e-04\n",
      "Epoch 60/100\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.4953 - accuracy: 0.9500 - val_loss: 1.1868 - val_accuracy: 0.5000 - lr: 1.0000e-04\n",
      "Epoch 61/100\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.4865 - accuracy: 0.9000 - val_loss: 1.1831 - val_accuracy: 0.5000 - lr: 1.0000e-04\n",
      "Epoch 62/100\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.4662 - accuracy: 0.9375 - val_loss: 1.1773 - val_accuracy: 0.5000 - lr: 1.0000e-04\n",
      "Epoch 63/100\n",
      "2/2 [==============================] - 0s 107ms/step - loss: 0.4877 - accuracy: 0.9000 - val_loss: 1.1731 - val_accuracy: 0.5000 - lr: 1.0000e-04\n",
      "Epoch 64/100\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.5022 - accuracy: 0.9125 - val_loss: 1.1726 - val_accuracy: 0.5000 - lr: 1.0000e-04\n",
      "Epoch 65/100\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.5020 - accuracy: 0.9000 - val_loss: 1.1716 - val_accuracy: 0.5000 - lr: 1.0000e-04\n",
      "Epoch 66/100\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 0.4975 - accuracy: 0.9250 - val_loss: 1.1666 - val_accuracy: 0.5000 - lr: 1.0000e-04\n",
      "Epoch 67/100\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 0.4696 - accuracy: 0.9000 - val_loss: 1.1578 - val_accuracy: 0.5000 - lr: 1.0000e-04\n",
      "Epoch 68/100\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.4900 - accuracy: 0.9250 - val_loss: 1.1482 - val_accuracy: 0.5000 - lr: 1.0000e-04\n",
      "Epoch 69/100\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 0.4421 - accuracy: 0.9250 - val_loss: 1.1407 - val_accuracy: 0.5000 - lr: 1.0000e-04\n",
      "Epoch 70/100\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.4396 - accuracy: 0.9625 - val_loss: 1.1379 - val_accuracy: 0.5000 - lr: 1.0000e-04\n",
      "Epoch 71/100\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.4687 - accuracy: 0.9250 - val_loss: 1.1335 - val_accuracy: 0.5000 - lr: 1.0000e-04\n",
      "Epoch 72/100\n",
      "2/2 [==============================] - 0s 127ms/step - loss: 0.4499 - accuracy: 0.9375 - val_loss: 1.1288 - val_accuracy: 0.4500 - lr: 1.0000e-04\n",
      "Epoch 73/100\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.4349 - accuracy: 0.9375 - val_loss: 1.1227 - val_accuracy: 0.4500 - lr: 1.0000e-04\n",
      "Epoch 74/100\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.4457 - accuracy: 0.9625 - val_loss: 1.1162 - val_accuracy: 0.4500 - lr: 1.0000e-04\n",
      "Epoch 75/100\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.4037 - accuracy: 0.9625 - val_loss: 1.1113 - val_accuracy: 0.4500 - lr: 1.0000e-04\n",
      "Epoch 76/100\n",
      "2/2 [==============================] - 0s 328ms/step - loss: 0.4220 - accuracy: 0.9375 - val_loss: 1.1074 - val_accuracy: 0.5000 - lr: 1.0000e-04\n",
      "Epoch 77/100\n",
      "2/2 [==============================] - 0s 229ms/step - loss: 0.4290 - accuracy: 0.9250 - val_loss: 1.1052 - val_accuracy: 0.5000 - lr: 1.0000e-04\n",
      "Epoch 78/100\n",
      "2/2 [==============================] - 0s 187ms/step - loss: 0.4141 - accuracy: 0.9625 - val_loss: 1.1015 - val_accuracy: 0.5000 - lr: 1.0000e-04\n",
      "Epoch 79/100\n",
      "2/2 [==============================] - 0s 117ms/step - loss: 0.4091 - accuracy: 0.9625 - val_loss: 1.0983 - val_accuracy: 0.5000 - lr: 1.0000e-04\n",
      "Epoch 80/100\n",
      "2/2 [==============================] - 0s 119ms/step - loss: 0.3946 - accuracy: 0.9625 - val_loss: 1.0931 - val_accuracy: 0.5000 - lr: 1.0000e-04\n",
      "Epoch 81/100\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 0.4080 - accuracy: 0.9625 - val_loss: 1.0855 - val_accuracy: 0.5000 - lr: 1.0000e-04\n",
      "Epoch 82/100\n",
      "2/2 [==============================] - 0s 116ms/step - loss: 0.3730 - accuracy: 0.9750 - val_loss: 1.0815 - val_accuracy: 0.5000 - lr: 1.0000e-04\n",
      "Epoch 83/100\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.3974 - accuracy: 0.9375 - val_loss: 1.0748 - val_accuracy: 0.5000 - lr: 1.0000e-04\n",
      "Epoch 84/100\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.3656 - accuracy: 0.9875 - val_loss: 1.0656 - val_accuracy: 0.5500 - lr: 1.0000e-04\n",
      "Epoch 85/100\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 0.3830 - accuracy: 0.9750 - val_loss: 1.0573 - val_accuracy: 0.6500 - lr: 1.0000e-04\n",
      "Epoch 86/100\n",
      "2/2 [==============================] - 0s 113ms/step - loss: 0.3739 - accuracy: 0.9625 - val_loss: 1.0499 - val_accuracy: 0.7000 - lr: 1.0000e-04\n",
      "Epoch 87/100\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 0.3494 - accuracy: 0.9875 - val_loss: 1.0421 - val_accuracy: 0.7500 - lr: 1.0000e-04\n",
      "Epoch 88/100\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.3602 - accuracy: 1.0000 - val_loss: 1.0381 - val_accuracy: 0.7500 - lr: 1.0000e-04\n",
      "Epoch 89/100\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.3577 - accuracy: 0.9875 - val_loss: 1.0372 - val_accuracy: 0.7500 - lr: 1.0000e-04\n",
      "Epoch 90/100\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.3413 - accuracy: 0.9875 - val_loss: 1.0399 - val_accuracy: 0.7000 - lr: 1.0000e-04\n",
      "Epoch 91/100\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.3478 - accuracy: 1.0000 - val_loss: 1.0359 - val_accuracy: 0.7000 - lr: 1.0000e-04\n",
      "Epoch 92/100\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.3413 - accuracy: 0.9750 - val_loss: 1.0188 - val_accuracy: 0.7500 - lr: 1.0000e-04\n",
      "Epoch 93/100\n",
      "2/2 [==============================] - 0s 116ms/step - loss: 0.3321 - accuracy: 1.0000 - val_loss: 0.9992 - val_accuracy: 0.7500 - lr: 1.0000e-04\n",
      "Epoch 94/100\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 0.3179 - accuracy: 1.0000 - val_loss: 0.9985 - val_accuracy: 0.7500 - lr: 1.0000e-04\n",
      "Epoch 95/100\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 0.3113 - accuracy: 0.9875 - val_loss: 1.0064 - val_accuracy: 0.7500 - lr: 1.0000e-04\n",
      "Epoch 96/100\n",
      "2/2 [==============================] - 0s 95ms/step - loss: 0.3219 - accuracy: 1.0000 - val_loss: 1.0062 - val_accuracy: 0.7500 - lr: 1.0000e-04\n",
      "Epoch 97/100\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 0.3265 - accuracy: 1.0000 - val_loss: 0.9984 - val_accuracy: 0.7000 - lr: 1.0000e-04\n",
      "Epoch 98/100\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.3218 - accuracy: 0.9875 - val_loss: 0.9842 - val_accuracy: 0.7500 - lr: 1.0000e-04\n",
      "Epoch 99/100\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.2956 - accuracy: 1.0000 - val_loss: 0.9743 - val_accuracy: 0.8000 - lr: 1.0000e-04\n",
      "Epoch 100/100\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.3172 - accuracy: 1.0000 - val_loss: 0.9696 - val_accuracy: 0.8000 - lr: 1.0000e-04\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.8348 - accuracy: 1.0000\n",
      "Evaluation Metrics for RNN:\n",
      "loss: 0.8348044753074646\n",
      "accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "\n",
    "X_train_rnn, X_test_rnn, y_train_rnn, y_test_rnn = train_test_split(X, Y, test_size=0.2)\n",
    "\n",
    "def RNN_model(vocab_size, sequence_length):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=vocab_size, output_dim=128, input_length=sequence_length))\n",
    "    model.add(SimpleRNN(units=128, activation='relu', return_sequences=True))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(SimpleRNN(units=64, activation='sigmoid'))\n",
    "    model.add(Dense(256, activation='sigmoid', kernel_regularizer=l2(0.001)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "    optimizer = Adam(learning_rate=0.001)\n",
    "    \n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=0.0001, verbose=1)\n",
    "#EMBEDDING_DIM = 110\n",
    "rnn_model = RNN_model(vocab_size, max_sequence_length)\n",
    "early_stopping_rnn = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "history_updated_rnn = rnn_model.fit(X_train_rnn, y_train_rnn, validation_data=(X_test_rnn, y_test_rnn), epochs=100, batch_size=64, callbacks=[reduce_lr])\n",
    "\n",
    "# Evaluate the RNN model\n",
    "evaluation_metrics_updated_rnn = rnn_model.evaluate(X_test, Y_test)\n",
    "print(\"Evaluation Metrics for RNN:\")\n",
    "for metric_name, metric_value in zip(rnn_model.metrics_names, evaluation_metrics_updated_rnn):\n",
    "    print(f\"{metric_name}: {metric_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*<h4>LSTM Model<h4>*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2/2 [==============================] - 16s 3s/step - loss: 1.1855 - accuracy: 0.4500 - val_loss: 1.1989 - val_accuracy: 0.4500\n",
      "Epoch 2/100\n",
      "2/2 [==============================] - 0s 147ms/step - loss: 1.0993 - accuracy: 0.8000 - val_loss: 1.1971 - val_accuracy: 0.3500\n",
      "Epoch 3/100\n",
      "2/2 [==============================] - 0s 118ms/step - loss: 0.9792 - accuracy: 0.8625 - val_loss: 1.1925 - val_accuracy: 0.6500\n",
      "Epoch 4/100\n",
      "2/2 [==============================] - 0s 283ms/step - loss: 0.8668 - accuracy: 0.8000 - val_loss: 1.1826 - val_accuracy: 0.6500\n",
      "Epoch 5/100\n",
      "2/2 [==============================] - 0s 353ms/step - loss: 0.7119 - accuracy: 0.7750 - val_loss: 1.1681 - val_accuracy: 0.5500\n",
      "Epoch 6/100\n",
      "2/2 [==============================] - 0s 293ms/step - loss: 0.5463 - accuracy: 0.8500 - val_loss: 1.1507 - val_accuracy: 0.7000\n",
      "Epoch 7/100\n",
      "2/2 [==============================] - 0s 157ms/step - loss: 0.5231 - accuracy: 0.8375 - val_loss: 1.1386 - val_accuracy: 0.7000\n",
      "Epoch 8/100\n",
      "2/2 [==============================] - 0s 355ms/step - loss: 0.4257 - accuracy: 0.8375 - val_loss: 1.1292 - val_accuracy: 0.6500\n",
      "Epoch 9/100\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.4050 - accuracy: 0.8375 - val_loss: 1.1258 - val_accuracy: 0.6500\n",
      "Epoch 10/100\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 0.3777 - accuracy: 0.8625 - val_loss: 1.1235 - val_accuracy: 0.7000\n",
      "Epoch 11/100\n",
      "2/2 [==============================] - 0s 98ms/step - loss: 0.3607 - accuracy: 0.8625 - val_loss: 1.1221 - val_accuracy: 0.6500\n",
      "Epoch 12/100\n",
      "2/2 [==============================] - 0s 113ms/step - loss: 0.3246 - accuracy: 0.8750 - val_loss: 1.1155 - val_accuracy: 0.7500\n",
      "Epoch 13/100\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.3057 - accuracy: 0.9000 - val_loss: 1.1021 - val_accuracy: 0.7000\n",
      "Epoch 14/100\n",
      "2/2 [==============================] - 0s 107ms/step - loss: 0.2625 - accuracy: 0.9625 - val_loss: 1.0854 - val_accuracy: 0.7000\n",
      "Epoch 15/100\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.2447 - accuracy: 0.9625 - val_loss: 1.0739 - val_accuracy: 0.7500\n",
      "Epoch 16/100\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.2226 - accuracy: 0.9625 - val_loss: 1.0706 - val_accuracy: 0.7500\n",
      "Epoch 17/100\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.2127 - accuracy: 0.9500 - val_loss: 1.0622 - val_accuracy: 0.8000\n",
      "Epoch 18/100\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.1512 - accuracy: 0.9875 - val_loss: 1.0498 - val_accuracy: 0.6500\n",
      "Epoch 19/100\n",
      "2/2 [==============================] - 0s 130ms/step - loss: 0.1581 - accuracy: 0.9875 - val_loss: 1.0388 - val_accuracy: 0.7500\n",
      "Epoch 20/100\n",
      "2/2 [==============================] - 0s 299ms/step - loss: 0.1411 - accuracy: 0.9875 - val_loss: 1.0270 - val_accuracy: 0.7500\n",
      "Epoch 21/100\n",
      "2/2 [==============================] - 0s 121ms/step - loss: 0.1336 - accuracy: 0.9875 - val_loss: 1.0128 - val_accuracy: 0.7500\n",
      "Epoch 22/100\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.1134 - accuracy: 0.9875 - val_loss: 1.0089 - val_accuracy: 0.7500\n",
      "Epoch 23/100\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 0.1213 - accuracy: 0.9875 - val_loss: 1.0090 - val_accuracy: 0.8000\n",
      "Epoch 24/100\n",
      "2/2 [==============================] - 0s 115ms/step - loss: 0.1026 - accuracy: 0.9875 - val_loss: 1.0151 - val_accuracy: 0.8000\n",
      "Epoch 25/100\n",
      "2/2 [==============================] - 0s 108ms/step - loss: 0.1347 - accuracy: 0.9625 - val_loss: 1.0169 - val_accuracy: 0.9000\n",
      "Epoch 26/100\n",
      "2/2 [==============================] - 0s 129ms/step - loss: 0.1306 - accuracy: 0.9875 - val_loss: 1.0187 - val_accuracy: 0.9500\n",
      "Epoch 27/100\n",
      "2/2 [==============================] - 0s 114ms/step - loss: 0.1199 - accuracy: 1.0000 - val_loss: 1.0229 - val_accuracy: 0.9500\n",
      "Epoch 28/100\n",
      "2/2 [==============================] - 0s 226ms/step - loss: 0.1291 - accuracy: 0.9875 - val_loss: 1.0305 - val_accuracy: 0.9000\n",
      "Epoch 29/100\n",
      "2/2 [==============================] - 0s 214ms/step - loss: 0.1058 - accuracy: 0.9875 - val_loss: 1.0390 - val_accuracy: 0.8000\n",
      "Epoch 30/100\n",
      "2/2 [==============================] - 0s 366ms/step - loss: 0.1004 - accuracy: 0.9875 - val_loss: 1.0445 - val_accuracy: 0.5500\n",
      "Epoch 31/100\n",
      "2/2 [==============================] - 0s 128ms/step - loss: 0.1415 - accuracy: 0.9750 - val_loss: 1.0347 - val_accuracy: 0.7500\n",
      "Epoch 32/100\n",
      "2/2 [==============================] - 0s 167ms/step - loss: 0.1196 - accuracy: 0.9875 - val_loss: 1.0252 - val_accuracy: 0.8500\n",
      "1/1 [==============================] - 0s 157ms/step - loss: 1.0071 - accuracy: 0.7333\n",
      "Evaluation Metrics for LSTM:\n",
      "loss: 1.0070637464523315\n",
      "accuracy: 0.7333333492279053\n"
     ]
    }
   ],
   "source": [
    "def LSTM_model(vocab_size, sequence_length):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=vocab_size, output_dim=128, input_length=sequence_length))\n",
    "    model.add(LSTM(units=128, activation='relu', return_sequences=True))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LSTM(units=64, activation='relu'))\n",
    "    model.add(Dense(256, activation='relu', kernel_regularizer=l2(0.001)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "    optimizer = Adam(learning_rate=0.001)\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "#EMBEDDING_DIM = 110\n",
    "lstm_model = LSTM_model(vocab_size, max_sequence_length)\n",
    "early_stopping_rnn = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "history_updated_rnn = lstm_model.fit(X_train_rnn, y_train_rnn, validation_data=(X_test_rnn, y_test_rnn), epochs=100, batch_size=64, callbacks=[early_stopping_rnn])\n",
    "\n",
    "\n",
    "# Evaluate the lstm model\n",
    "evaluation_metrics_updated_lstm = lstm_model.evaluate(X_test, Y_test)\n",
    "print(\"Evaluation Metrics for LSTM:\")\n",
    "for metric_name, metric_value in zip(lstm_model.metrics_names, evaluation_metrics_updated_lstm):\n",
    "    print(f\"{metric_name}: {metric_value}\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*<h4>TRANSFORMER Model<h4>*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, density, rate=0.1, l2_reg=0.01):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            layers.Dense(density, activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(l2_reg)),\n",
    "            layers.Dense(embed_dim, kernel_regularizer=tf.keras.regularizers.l2(l2_reg)),\n",
    "        ])\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "\n",
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions\n",
    "\n",
    "def build_transformer_model(maxlen, vocab_size, embed_dim, num_heads, density, transformer_units, mlp_units, dropout_rate, num_classes):\n",
    "    inputs = layers.Input(shape=(maxlen,))\n",
    "    embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
    "    x = embedding_layer(inputs)\n",
    "    for _ in range(transformer_units):\n",
    "        x = TransformerBlock(embed_dim, num_heads, density, rate=dropout_rate)(x)\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    for dim in mlp_units:\n",
    "        x = layers.Dense(dim, activation=\"relu\")(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "    outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = max_sequence_length\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "embed_dim = 120\n",
    "num_heads = 2\n",
    "density = 3\n",
    "transformer_units = 4\n",
    "mlp_units = [128]\n",
    "dropout_rate = 0.5\n",
    "num_classes = len(df['grade'].unique())\n",
    "\n",
    "transformer_model = build_transformer_model(maxlen, vocab_size, embed_dim, num_heads, density, transformer_units, mlp_units, dropout_rate, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\amine\\anaconda3\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Epoch 1/30\n",
      "3/3 [==============================] - 19s 859ms/step - loss: 1.6170 - accuracy: 0.4471 - val_loss: 1.4325 - val_accuracy: 0.6000\n",
      "Epoch 2/30\n",
      "3/3 [==============================] - 0s 105ms/step - loss: 1.5027 - accuracy: 0.4824 - val_loss: 1.3982 - val_accuracy: 0.5333\n",
      "Epoch 3/30\n",
      "3/3 [==============================] - 0s 172ms/step - loss: 1.4301 - accuracy: 0.5176 - val_loss: 1.1546 - val_accuracy: 0.8667\n",
      "Epoch 4/30\n",
      "3/3 [==============================] - 0s 117ms/step - loss: 1.2126 - accuracy: 0.6824 - val_loss: 0.9450 - val_accuracy: 0.8667\n",
      "Epoch 5/30\n",
      "3/3 [==============================] - 0s 112ms/step - loss: 1.0389 - accuracy: 0.7647 - val_loss: 0.9095 - val_accuracy: 0.8667\n",
      "Epoch 6/30\n",
      "3/3 [==============================] - 0s 116ms/step - loss: 0.9321 - accuracy: 0.8235 - val_loss: 0.7584 - val_accuracy: 0.8667\n",
      "Epoch 7/30\n",
      "3/3 [==============================] - 0s 109ms/step - loss: 0.8585 - accuracy: 0.8118 - val_loss: 0.5907 - val_accuracy: 0.9333\n",
      "Epoch 8/30\n",
      "3/3 [==============================] - 0s 88ms/step - loss: 0.7430 - accuracy: 0.8706 - val_loss: 0.7278 - val_accuracy: 0.8667\n",
      "Epoch 9/30\n",
      "3/3 [==============================] - 0s 106ms/step - loss: 0.7463 - accuracy: 0.8941 - val_loss: 0.6499 - val_accuracy: 0.8667\n",
      "Epoch 10/30\n",
      "3/3 [==============================] - 0s 96ms/step - loss: 0.6556 - accuracy: 0.9176 - val_loss: 0.4955 - val_accuracy: 0.9333\n",
      "Epoch 11/30\n",
      "3/3 [==============================] - 0s 113ms/step - loss: 0.5355 - accuracy: 0.9647 - val_loss: 0.4952 - val_accuracy: 0.9333\n",
      "Epoch 12/30\n",
      "3/3 [==============================] - 0s 99ms/step - loss: 0.4910 - accuracy: 0.9765 - val_loss: 0.5888 - val_accuracy: 0.8667\n",
      "Epoch 13/30\n",
      "3/3 [==============================] - 0s 102ms/step - loss: 0.4313 - accuracy: 0.9882 - val_loss: 0.4483 - val_accuracy: 0.9333\n",
      "Epoch 14/30\n",
      "3/3 [==============================] - 0s 85ms/step - loss: 0.4565 - accuracy: 0.9765 - val_loss: 0.4130 - val_accuracy: 1.0000\n",
      "Epoch 15/30\n",
      "3/3 [==============================] - 0s 109ms/step - loss: 0.4609 - accuracy: 0.9882 - val_loss: 0.4296 - val_accuracy: 1.0000\n",
      "Epoch 16/30\n",
      "3/3 [==============================] - 0s 129ms/step - loss: 0.4181 - accuracy: 0.9882 - val_loss: 0.4644 - val_accuracy: 0.9333\n",
      "Epoch 17/30\n",
      "3/3 [==============================] - 0s 92ms/step - loss: 0.3877 - accuracy: 0.9882 - val_loss: 0.4888 - val_accuracy: 0.9333\n",
      "Epoch 18/30\n",
      "3/3 [==============================] - 0s 122ms/step - loss: 0.3784 - accuracy: 0.9882 - val_loss: 0.5655 - val_accuracy: 0.8667\n",
      "Epoch 19/30\n",
      "3/3 [==============================] - 0s 174ms/step - loss: 0.3696 - accuracy: 0.9882 - val_loss: 0.6604 - val_accuracy: 0.9333\n",
      "Epoch 20/30\n",
      "3/3 [==============================] - 0s 140ms/step - loss: 0.3625 - accuracy: 0.9882 - val_loss: 0.6684 - val_accuracy: 0.9333\n",
      "Epoch 21/30\n",
      "3/3 [==============================] - 0s 114ms/step - loss: 0.3570 - accuracy: 0.9882 - val_loss: 0.7249 - val_accuracy: 0.9333\n",
      "Epoch 22/30\n",
      "3/3 [==============================] - 0s 126ms/step - loss: 0.3193 - accuracy: 1.0000 - val_loss: 0.8271 - val_accuracy: 0.9333\n",
      "Epoch 23/30\n",
      "3/3 [==============================] - 0s 112ms/step - loss: 0.3376 - accuracy: 0.9882 - val_loss: 0.8581 - val_accuracy: 0.8000\n",
      "Epoch 24/30\n",
      "3/3 [==============================] - 0s 106ms/step - loss: 0.3227 - accuracy: 0.9882 - val_loss: 0.7822 - val_accuracy: 0.9333\n",
      "Epoch 25/30\n",
      "3/3 [==============================] - 0s 121ms/step - loss: 0.2973 - accuracy: 1.0000 - val_loss: 0.7454 - val_accuracy: 0.9333\n",
      "Epoch 26/30\n",
      "3/3 [==============================] - 0s 117ms/step - loss: 0.2979 - accuracy: 1.0000 - val_loss: 0.7617 - val_accuracy: 0.9333\n",
      "Epoch 27/30\n",
      "3/3 [==============================] - 0s 148ms/step - loss: 0.2835 - accuracy: 1.0000 - val_loss: 0.8334 - val_accuracy: 0.8667\n",
      "Epoch 28/30\n",
      "3/3 [==============================] - 0s 150ms/step - loss: 0.2790 - accuracy: 1.0000 - val_loss: 0.7045 - val_accuracy: 0.9333\n",
      "Epoch 29/30\n",
      "3/3 [==============================] - 0s 122ms/step - loss: 0.2711 - accuracy: 1.0000 - val_loss: 0.6011 - val_accuracy: 0.9333\n",
      "Epoch 30/30\n",
      "3/3 [==============================] - 0s 130ms/step - loss: 0.2638 - accuracy: 1.0000 - val_loss: 0.5415 - val_accuracy: 0.9333\n",
      "Epoch 1/30\n",
      "3/3 [==============================] - 0s 187ms/step - loss: 0.2588 - accuracy: 1.0000 - val_loss: 0.4928 - val_accuracy: 0.9333\n",
      "Epoch 2/30\n",
      "3/3 [==============================] - 0s 88ms/step - loss: 0.2510 - accuracy: 1.0000 - val_loss: 0.4592 - val_accuracy: 0.9333\n",
      "Epoch 3/30\n",
      "3/3 [==============================] - 0s 104ms/step - loss: 0.2451 - accuracy: 1.0000 - val_loss: 0.4208 - val_accuracy: 0.9333\n",
      "Epoch 4/30\n",
      "3/3 [==============================] - 0s 121ms/step - loss: 0.2388 - accuracy: 1.0000 - val_loss: 0.3970 - val_accuracy: 0.9333\n",
      "Epoch 5/30\n",
      "3/3 [==============================] - 0s 148ms/step - loss: 0.2354 - accuracy: 1.0000 - val_loss: 0.3805 - val_accuracy: 0.9333\n",
      "Epoch 6/30\n",
      "3/3 [==============================] - 0s 122ms/step - loss: 0.2281 - accuracy: 1.0000 - val_loss: 0.3661 - val_accuracy: 0.9333\n",
      "Epoch 7/30\n",
      "3/3 [==============================] - 0s 160ms/step - loss: 0.2209 - accuracy: 1.0000 - val_loss: 0.3721 - val_accuracy: 0.9333\n",
      "Epoch 8/30\n",
      "3/3 [==============================] - 0s 129ms/step - loss: 0.2155 - accuracy: 1.0000 - val_loss: 0.3764 - val_accuracy: 0.9333\n",
      "Epoch 9/30\n",
      "3/3 [==============================] - 0s 133ms/step - loss: 0.2115 - accuracy: 1.0000 - val_loss: 0.3888 - val_accuracy: 0.9333\n",
      "Epoch 10/30\n",
      "3/3 [==============================] - 0s 123ms/step - loss: 0.2046 - accuracy: 1.0000 - val_loss: 0.4036 - val_accuracy: 0.9333\n",
      "Epoch 11/30\n",
      "3/3 [==============================] - 0s 105ms/step - loss: 0.1996 - accuracy: 1.0000 - val_loss: 0.4109 - val_accuracy: 0.9333\n",
      "Epoch 12/30\n",
      "3/3 [==============================] - 0s 142ms/step - loss: 0.1947 - accuracy: 1.0000 - val_loss: 0.4082 - val_accuracy: 0.9333\n",
      "Epoch 13/30\n",
      "3/3 [==============================] - 0s 137ms/step - loss: 0.1894 - accuracy: 1.0000 - val_loss: 0.3952 - val_accuracy: 0.9333\n",
      "Epoch 14/30\n",
      "3/3 [==============================] - 0s 130ms/step - loss: 0.1842 - accuracy: 1.0000 - val_loss: 0.3691 - val_accuracy: 0.9333\n",
      "Epoch 15/30\n",
      "3/3 [==============================] - 0s 109ms/step - loss: 0.1794 - accuracy: 1.0000 - val_loss: 0.3413 - val_accuracy: 0.9333\n",
      "Epoch 16/30\n",
      "3/3 [==============================] - 0s 115ms/step - loss: 0.1760 - accuracy: 1.0000 - val_loss: 0.3433 - val_accuracy: 0.9333\n",
      "Epoch 17/30\n",
      "3/3 [==============================] - 0s 119ms/step - loss: 0.1701 - accuracy: 1.0000 - val_loss: 0.3420 - val_accuracy: 0.9333\n",
      "Epoch 18/30\n",
      "3/3 [==============================] - 0s 102ms/step - loss: 0.1667 - accuracy: 1.0000 - val_loss: 0.3373 - val_accuracy: 0.9333\n",
      "Epoch 19/30\n",
      "3/3 [==============================] - 0s 102ms/step - loss: 0.1620 - accuracy: 1.0000 - val_loss: 0.3336 - val_accuracy: 0.9333\n",
      "Epoch 20/30\n",
      "3/3 [==============================] - 0s 105ms/step - loss: 0.1572 - accuracy: 1.0000 - val_loss: 0.3215 - val_accuracy: 0.9333\n",
      "Epoch 21/30\n",
      "3/3 [==============================] - 1s 236ms/step - loss: 0.1527 - accuracy: 1.0000 - val_loss: 0.3107 - val_accuracy: 0.9333\n",
      "Epoch 22/30\n",
      "3/3 [==============================] - 0s 122ms/step - loss: 0.1489 - accuracy: 1.0000 - val_loss: 0.2922 - val_accuracy: 0.9333\n",
      "Epoch 23/30\n",
      "3/3 [==============================] - 0s 119ms/step - loss: 0.1443 - accuracy: 1.0000 - val_loss: 0.2739 - val_accuracy: 0.9333\n",
      "Epoch 24/30\n",
      "3/3 [==============================] - 0s 124ms/step - loss: 0.1406 - accuracy: 1.0000 - val_loss: 0.2601 - val_accuracy: 0.9333\n",
      "Epoch 25/30\n",
      "3/3 [==============================] - 0s 112ms/step - loss: 0.1367 - accuracy: 1.0000 - val_loss: 0.2504 - val_accuracy: 0.9333\n",
      "Epoch 26/30\n",
      "3/3 [==============================] - 0s 100ms/step - loss: 0.1331 - accuracy: 1.0000 - val_loss: 0.2433 - val_accuracy: 0.9333\n",
      "Epoch 27/30\n",
      "3/3 [==============================] - 0s 102ms/step - loss: 0.1291 - accuracy: 1.0000 - val_loss: 0.2361 - val_accuracy: 0.9333\n",
      "Epoch 28/30\n",
      "3/3 [==============================] - 0s 123ms/step - loss: 0.1261 - accuracy: 1.0000 - val_loss: 0.2307 - val_accuracy: 0.9333\n",
      "Epoch 29/30\n",
      "3/3 [==============================] - 0s 105ms/step - loss: 0.1222 - accuracy: 1.0000 - val_loss: 0.2234 - val_accuracy: 0.9333\n",
      "Epoch 30/30\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 0.1188 - accuracy: 1.0000 - val_loss: 0.2155 - val_accuracy: 0.9333\n"
     ]
    }
   ],
   "source": [
    "transformer_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "transformer_model.fit(X_train, Y_train, epochs=30, batch_size=32, validation_data=(X_test, Y_test))\n",
    "history = transformer_model.fit(X_train, Y_train, epochs=30, batch_size=32, validation_data=(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 88ms/step - loss: 0.2155 - accuracy: 0.9333\n",
      "Evaluation Metrics Transformer:\n",
      "loss: 0.21553809940814972\n",
      "accuracy: 0.9333333373069763\n"
     ]
    }
   ],
   "source": [
    "evaluation_metrics_transformer = transformer_model.evaluate(X_test, Y_test)\n",
    "print(\"Evaluation Metrics Transformer:\")\n",
    "for metric_name, metric_value in zip(transformer_model.metrics_names, evaluation_metrics_transformer):\n",
    "    print(f\"{metric_name}: {metric_value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 73ms/step - loss: 0.2155 - accuracy: 0.9333\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "[[  0   0   0   0   0   0   0   0  15  27   5   3   6   4   2   8  13   7\n",
      "    2  12]\n",
      " [  0   0   0   0   0   0   0   0   1   8  14   6   2  16  17   3  10   7\n",
      "   43 109]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    1 119]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   3   1  11   4   2   8\n",
      "   13   7]\n",
      " [  0   0   0   0   0   0   0   0   0   0   1  11  61  15  16  17 101   4\n",
      "   10   7]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   1   5   3   4   2\n",
      "    1   2]\n",
      " [  0   0   0   0   0   1  11   2   1  23   5  25  21   2   6  20   2   8\n",
      "   13   7]\n",
      " [  0   0   0   0   0   0   0   0   1   5   3   6   4   2  22   8 124 125\n",
      "    9 126]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   1  50   2   1   5   3   6\n",
      "    4   2]\n",
      " [  0   0   0   0   0   0   0   0   0   1  11   2   1   5   3   6   4   2\n",
      "   10   7]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    1  93]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    1  49]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    1 121]\n",
      " [  0   0   0   0   0   0   0   0  26   1   8  14  24   2  16  17   9   3\n",
      "   10   7]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   1   8  18  32  15  14\n",
      "    3   4]]\n",
      "\n",
      "Real and Predicted Values:\n",
      "    Real  Predicted\n",
      "0      2          2\n",
      "1      1          1\n",
      "2      0          2\n",
      "3      2          2\n",
      "4      1          1\n",
      "5      0          0\n",
      "6      2          2\n",
      "7      1          1\n",
      "8      0          0\n",
      "9      2          2\n",
      "10     0          0\n",
      "11     0          0\n",
      "12     0          0\n",
      "13     1          1\n",
      "14     1          1\n"
     ]
    }
   ],
   "source": [
    "evaluation_metrics_transformer = transformer_model.evaluate(X_test, Y_test)\n",
    "\n",
    "predictions = transformer_model.predict(X_test)\n",
    "print(X_test)\n",
    "\n",
    "# Convert one-hot encoded labels to class indices\n",
    "y_true = np.argmax(Y_test, axis=1)\n",
    "\n",
    "# Convert predicted probabilities to class indices\n",
    "y_pred = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Create DataFrame\n",
    "results_df = pd.DataFrame({\"Real\": y_true, \"Predicted\": y_pred})\n",
    "\n",
    "# Display DataFrame\n",
    "print(\"\\nReal and Predicted Values:\")\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy RNN: 1.0\n",
      "Accuracy LSTM: 0.7333333492279053\n",
      "Accuracy Transformer: 0.9333333373069763\n"
     ]
    }
   ],
   "source": [
    "# Evaluate RNN model\n",
    "rnn_accuracy = rnn_model.evaluate(X_test, Y_test, verbose=0)[1]\n",
    "print(\"Accuracy RNN:\", rnn_accuracy)\n",
    "\n",
    "# Evaluate LSTM model\n",
    "lstm_accuracy = lstm_model.evaluate(X_test, Y_test, verbose=0)[1]\n",
    "print(\"Accuracy LSTM:\", lstm_accuracy)\n",
    "\n",
    "# Evaluate Transformer model\n",
    "transformer_accuracy = transformer_model.evaluate(X_test, Y_test, verbose=0)[1]\n",
    "print(\"Accuracy Transformer:\", transformer_accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model (RNN) with accuracy 1.0 has been saved to './savedModels/q10_model.h5'\n"
     ]
    }
   ],
   "source": [
    "# Determine the best model\n",
    "best_model_name, best_model_accuracy = max([('RNN', rnn_accuracy), ('LSTM', lstm_accuracy), ('Transformer', transformer_accuracy)], key=lambda x: x[1])\n",
    "\n",
    "save_path = './savedModels/q10_model.h5'\n",
    "# Save the best model\n",
    "if best_model_name == 'RNN':\n",
    "    rnn_model.save(save_path)\n",
    "elif best_model_name == 'LSTM':\n",
    "    lstm_model.save(save_path)\n",
    "elif best_model_name == 'Transformer':\n",
    "    transformer_model.save(save_path)\n",
    "\n",
    "print(f\"The best model ({best_model_name}) with accuracy {best_model_accuracy} has been saved to '{save_path}'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
