{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\amine\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow as keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.regularizers import l2\n",
    "from keras import layers\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Embedding, SimpleRNN, Dense, Dropout, LSTM, Bidirectional, Input\n",
    "from keras.layers import Attention, GlobalAveragePooling1D\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import stanza\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import BatchNormalization, Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*<h3>Load Data<h3>*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_id</th>\n",
       "      <th>answer</th>\n",
       "      <th>grade</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>سورة النبأ هي السورة الأولى في الجزء الثلاثين.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>سورة تتألف من 40 آية</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9</td>\n",
       "      <td>أول سورة في الجزء الثلاثين من القرآن هي سورة ا...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>سورة فيها 40 آيات</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>في الجزء الثلاثين، السورة الأولى هي النبأ</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>9</td>\n",
       "      <td>السورة 78 في القرآن</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>9</td>\n",
       "      <td>السورة الأولى في الجزء 30 هي النبأ</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>9</td>\n",
       "      <td>السورة الثامنة والسبعون</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>سورة بعدد آياتها 40</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>الجزء الثلاثين ينطلق بسورة النبأ</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   question_id                                             answer  grade\n",
       "0            9     سورة النبأ هي السورة الأولى في الجزء الثلاثين.      2\n",
       "1            9                               سورة تتألف من 40 آية      1\n",
       "2            9  أول سورة في الجزء الثلاثين من القرآن هي سورة ا...      0\n",
       "3            9                                  سورة فيها 40 آيات      1\n",
       "4            9          في الجزء الثلاثين، السورة الأولى هي النبأ      2\n",
       "5            9                                السورة 78 في القرآن      1\n",
       "6            9                 السورة الأولى في الجزء 30 هي النبأ      2\n",
       "7            9                            السورة الثامنة والسبعون      1\n",
       "8            9                                سورة بعدد آياتها 40      1\n",
       "9            9                   الجزء الثلاثين ينطلق بسورة النبأ      2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read csv file into pandas\n",
    "df = pd.read_csv(\"../datasets/shuffled9.csv\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*<h3>EDA<h3>*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100 entries, 0 to 99\n",
      "Data columns (total 3 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   question_id  100 non-null    int64 \n",
      " 1   answer       100 non-null    object\n",
      " 2   grade        100 non-null    int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 2.5+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "grade\n",
       "0    29\n",
       "1    30\n",
       "2    41\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('grade').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(data=df, x='grade')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*<h3>Data Cleaning<h3>*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('question_id', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        سورة النبأ هي السورة الأولى في الجزء الثلاثين.\n",
      "1                                  سورة تتألف من 40 آية\n",
      "2     أول سورة في الجزء الثلاثين من القرآن هي سورة ا...\n",
      "3                                     سورة فيها 40 آيات\n",
      "4             في الجزء الثلاثين، السورة الأولى هي النبأ\n",
      "                            ...                        \n",
      "95    أول سورة في الجزء الثلاثين من القرآن هي السورة...\n",
      "96                                      سورة بها 40 آية\n",
      "97    أول سورة في الجزء الثلاثين من القرآن هي سورة ا...\n",
      "98            السورة التي تفتتح الجزء الثلاثين هي النبأ\n",
      "99    تلك السورة القرآنية الواقعة في الجزء الثلاثين،...\n",
      "Name: answer, Length: 100, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df.drop_duplicates(inplace=True)\n",
    "print(df['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*<h3>Data Pre-Preocessing<h3>*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bf33f15986243d1a5c5b8d4ef83bed3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.7.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-09 22:55:20 INFO: Downloading default packages for language: ar (Arabic) ...\n",
      "2024-01-09 22:55:21 INFO: File exists: C:\\Users\\amine\\stanza_resources\\ar\\default.zip\n",
      "2024-01-09 22:55:26 INFO: Finished downloading models and saved to C:\\Users\\amine\\stanza_resources.\n",
      "2024-01-09 22:55:26 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "669d65ad08984d7ea5c8623042d3a514",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.7.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-09 22:55:30 INFO: Loading these models for language: ar (Arabic):\n",
      "=============================\n",
      "| Processor | Package       |\n",
      "-----------------------------\n",
      "| tokenize  | padt          |\n",
      "| mwt       | padt          |\n",
      "| pos       | padt_charlm   |\n",
      "| lemma     | padt_nocharlm |\n",
      "| depparse  | padt_charlm   |\n",
      "| ner       | aqmar_charlm  |\n",
      "=============================\n",
      "\n",
      "2024-01-09 22:55:30 INFO: Using device: cpu\n",
      "2024-01-09 22:55:30 INFO: Loading: tokenize\n",
      "2024-01-09 22:55:31 INFO: Loading: mwt\n",
      "2024-01-09 22:55:31 INFO: Loading: pos\n",
      "2024-01-09 22:55:32 INFO: Loading: lemma\n",
      "2024-01-09 22:55:32 INFO: Loading: depparse\n",
      "2024-01-09 22:55:32 INFO: Loading: ner\n",
      "2024-01-09 22:55:34 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "df['grade'] = le.fit_transform(df['grade'])\n",
    "\n",
    "stanza.download('ar')\n",
    "nlp = stanza.Pipeline('ar')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    doc = nlp(text)\n",
    "    tokens = [word.lemma for sent in doc.sentences for word in sent.words if word.upos != 'PUNCT']\n",
    "    return tokens\n",
    "\n",
    "df['answer'] = df['answer'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0 ...   5   2   3]\n",
      " [  0   0   0 ...   7  11  15]\n",
      " [  0   0   0 ...   4   1  48]\n",
      " ...\n",
      " [  0   0   0 ...   4   1 100]\n",
      " [  0   0   0 ...   3   4   9]\n",
      " [ 45   1  25 ...  46  16  12]]\n",
      "0     [سُورَة, النبأ, هُوَ, سُورَة, أَوَّل, فِي, جُز...\n",
      "1                      [سُورَة, تَأَلَّف, مِن, 40, آية]\n",
      "2     [أَوَّل, سُورَة, فِي, جُزء, ثَلَاثُون, مِن, قُ...\n",
      "3                         [سُورَة, فِي, هُوَ, 40, آيَة]\n",
      "4     [فِي, جُزء, ثَلَاثُون, سُورَة, أَوَّل, هُوَ, ا...\n",
      "                            ...                        \n",
      "95    [أَوَّل, سُورَة, فِي, جُزء, ثَلَاثُون, مِن, قُ...\n",
      "96                          [سُورَة, بِ, هُوَ, 40, آية]\n",
      "97    [أَوَّل, سُورَة, فِي, جُزء, ثَلَاثُون, مِن, قُ...\n",
      "98    [سُورَة, اَلَّذِي, اِفتَتَح, جُزء, ثَلَاثُون, ...\n",
      "99    [ذٰلِكَ, سُورَة, قَرِينِيّ, وَاقِع, فِي, جُزء,...\n",
      "Name: answer, Length: 100, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Tokenization\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df['answer'])\n",
    "sequences = tokenizer.texts_to_sequences(df['answer'])\n",
    "max_sequence_length = max(len(s) for s in sequences)\n",
    "sequences = pad_sequences(sequences,max_sequence_length)\n",
    "word2idx = tokenizer.word_index\n",
    "vocab_size = len(word2idx) + 1\n",
    "\n",
    "\n",
    "X = pad_sequences(sequences, padding='post', truncating='post', maxlen=max_sequence_length)\n",
    "\n",
    "print(sequences)\n",
    "print(df['answer'])\n",
    "\n",
    "Y = to_categorical(df['grade'], num_classes=3)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.15, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*<h3>build Models<h3>*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*<h4>RNN Model<h4>*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\amine\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:From c:\\Users\\amine\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\amine\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "2/2 [==============================] - 7s 878ms/step - loss: 1.3353 - accuracy: 0.2750 - val_loss: 1.2046 - val_accuracy: 0.4000 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 1.1496 - accuracy: 0.4750 - val_loss: 1.2031 - val_accuracy: 0.4000 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 1.1627 - accuracy: 0.4750 - val_loss: 1.2186 - val_accuracy: 0.4000 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 1.1141 - accuracy: 0.5156\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "2/2 [==============================] - 0s 129ms/step - loss: 1.1584 - accuracy: 0.4625 - val_loss: 1.2078 - val_accuracy: 0.4000 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "2/2 [==============================] - 0s 112ms/step - loss: 1.0813 - accuracy: 0.5625 - val_loss: 1.2024 - val_accuracy: 0.4000 - lr: 2.0000e-04\n",
      "Epoch 6/100\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 1.0343 - accuracy: 0.5000 - val_loss: 1.1968 - val_accuracy: 0.4000 - lr: 2.0000e-04\n",
      "Epoch 7/100\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 1.0734 - accuracy: 0.5000 - val_loss: 1.1921 - val_accuracy: 0.4000 - lr: 2.0000e-04\n",
      "Epoch 8/100\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 1.0403 - accuracy: 0.6000 - val_loss: 1.1877 - val_accuracy: 0.4000 - lr: 2.0000e-04\n",
      "Epoch 9/100\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.9411 - accuracy: 0.7000 - val_loss: 1.1840 - val_accuracy: 0.4000 - lr: 2.0000e-04\n",
      "Epoch 10/100\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.9964 - accuracy: 0.5875 - val_loss: 1.1809 - val_accuracy: 0.4000 - lr: 2.0000e-04\n",
      "Epoch 11/100\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.9635 - accuracy: 0.7000 - val_loss: 1.1788 - val_accuracy: 0.4000 - lr: 2.0000e-04\n",
      "Epoch 12/100\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 0.9616 - accuracy: 0.6375 - val_loss: 1.1774 - val_accuracy: 0.4000 - lr: 2.0000e-04\n",
      "Epoch 13/100\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.9056 - accuracy: 0.6875 - val_loss: 1.1762 - val_accuracy: 0.4000 - lr: 2.0000e-04\n",
      "Epoch 14/100\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.9106 - accuracy: 0.7250 - val_loss: 1.1748 - val_accuracy: 0.4000 - lr: 2.0000e-04\n",
      "Epoch 15/100\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.9345 - accuracy: 0.6875 - val_loss: 1.1738 - val_accuracy: 0.4000 - lr: 2.0000e-04\n",
      "Epoch 16/100\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 0.8331 - accuracy: 0.8375 - val_loss: 1.1728 - val_accuracy: 0.4000 - lr: 2.0000e-04\n",
      "Epoch 17/100\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 0.8785 - accuracy: 0.7125 - val_loss: 1.1713 - val_accuracy: 0.4000 - lr: 2.0000e-04\n",
      "Epoch 18/100\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.8352 - accuracy: 0.8125 - val_loss: 1.1693 - val_accuracy: 0.4000 - lr: 2.0000e-04\n",
      "Epoch 19/100\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 0.8278 - accuracy: 0.7750 - val_loss: 1.1678 - val_accuracy: 0.4000 - lr: 2.0000e-04\n",
      "Epoch 20/100\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.7960 - accuracy: 0.8250 - val_loss: 1.1664 - val_accuracy: 0.4000 - lr: 2.0000e-04\n",
      "Epoch 21/100\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.7705 - accuracy: 0.8000 - val_loss: 1.1641 - val_accuracy: 0.4000 - lr: 2.0000e-04\n",
      "Epoch 22/100\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.8125 - accuracy: 0.7625 - val_loss: 1.1612 - val_accuracy: 0.4000 - lr: 2.0000e-04\n",
      "Epoch 23/100\n",
      "2/2 [==============================] - 0s 261ms/step - loss: 0.7532 - accuracy: 0.7375 - val_loss: 1.1582 - val_accuracy: 0.4000 - lr: 2.0000e-04\n",
      "Epoch 24/100\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.7602 - accuracy: 0.8375 - val_loss: 1.1558 - val_accuracy: 0.4000 - lr: 2.0000e-04\n",
      "Epoch 25/100\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 0.6906 - accuracy: 0.9000 - val_loss: 1.1533 - val_accuracy: 0.4000 - lr: 2.0000e-04\n",
      "Epoch 26/100\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 0.7559 - accuracy: 0.8625 - val_loss: 1.1502 - val_accuracy: 0.4000 - lr: 2.0000e-04\n",
      "Epoch 27/100\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.7130 - accuracy: 0.8625 - val_loss: 1.1467 - val_accuracy: 0.4000 - lr: 2.0000e-04\n",
      "Epoch 28/100\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.6870 - accuracy: 0.9000 - val_loss: 1.1437 - val_accuracy: 0.4000 - lr: 2.0000e-04\n",
      "Epoch 29/100\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.6543 - accuracy: 0.9125 - val_loss: 1.1412 - val_accuracy: 0.4000 - lr: 2.0000e-04\n",
      "Epoch 30/100\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.6703 - accuracy: 0.8750 - val_loss: 1.1383 - val_accuracy: 0.4000 - lr: 2.0000e-04\n",
      "Epoch 31/100\n",
      "2/2 [==============================] - 0s 179ms/step - loss: 0.6221 - accuracy: 0.9375 - val_loss: 1.1353 - val_accuracy: 0.4000 - lr: 2.0000e-04\n",
      "Epoch 32/100\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 0.6217 - accuracy: 0.9250 - val_loss: 1.1325 - val_accuracy: 0.4000 - lr: 2.0000e-04\n",
      "Epoch 33/100\n",
      "2/2 [==============================] - 0s 171ms/step - loss: 0.5905 - accuracy: 0.9375 - val_loss: 1.1291 - val_accuracy: 0.4000 - lr: 2.0000e-04\n",
      "Epoch 34/100\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.5599 - accuracy: 0.9375 - val_loss: 1.1255 - val_accuracy: 0.4000 - lr: 2.0000e-04\n",
      "Epoch 35/100\n",
      "2/2 [==============================] - 0s 109ms/step - loss: 0.5820 - accuracy: 0.9625 - val_loss: 1.1219 - val_accuracy: 0.4000 - lr: 2.0000e-04\n",
      "Epoch 36/100\n",
      "2/2 [==============================] - 0s 117ms/step - loss: 0.5742 - accuracy: 0.9375 - val_loss: 1.1178 - val_accuracy: 0.4000 - lr: 2.0000e-04\n",
      "Epoch 37/100\n",
      "2/2 [==============================] - 0s 113ms/step - loss: 0.5483 - accuracy: 0.9750 - val_loss: 1.1130 - val_accuracy: 0.4000 - lr: 2.0000e-04\n",
      "Epoch 38/100\n",
      "2/2 [==============================] - 0s 180ms/step - loss: 0.5297 - accuracy: 0.9500 - val_loss: 1.1078 - val_accuracy: 0.4000 - lr: 2.0000e-04\n",
      "Epoch 39/100\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.5220 - accuracy: 0.9750 - val_loss: 1.1024 - val_accuracy: 0.4000 - lr: 2.0000e-04\n",
      "Epoch 40/100\n",
      "2/2 [==============================] - 0s 219ms/step - loss: 0.5133 - accuracy: 0.9500 - val_loss: 1.0976 - val_accuracy: 0.4000 - lr: 2.0000e-04\n",
      "Epoch 41/100\n",
      "2/2 [==============================] - 0s 111ms/step - loss: 0.5072 - accuracy: 0.9625 - val_loss: 1.0927 - val_accuracy: 0.4000 - lr: 2.0000e-04\n",
      "Epoch 42/100\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.4984 - accuracy: 0.9625 - val_loss: 1.0875 - val_accuracy: 0.4000 - lr: 2.0000e-04\n",
      "Epoch 43/100\n",
      "2/2 [==============================] - 0s 126ms/step - loss: 0.4939 - accuracy: 0.9500 - val_loss: 1.0825 - val_accuracy: 0.4000 - lr: 2.0000e-04\n",
      "Epoch 44/100\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 0.4728 - accuracy: 0.9875 - val_loss: 1.0775 - val_accuracy: 0.4000 - lr: 2.0000e-04\n",
      "Epoch 45/100\n",
      "2/2 [==============================] - 0s 117ms/step - loss: 0.4240 - accuracy: 0.9875 - val_loss: 1.0729 - val_accuracy: 0.4500 - lr: 2.0000e-04\n",
      "Epoch 46/100\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.4240 - accuracy: 0.9500 - val_loss: 1.0687 - val_accuracy: 0.5500 - lr: 2.0000e-04\n",
      "Epoch 47/100\n",
      "2/2 [==============================] - 0s 192ms/step - loss: 0.4230 - accuracy: 1.0000 - val_loss: 1.0644 - val_accuracy: 0.6000 - lr: 2.0000e-04\n",
      "Epoch 48/100\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.4078 - accuracy: 1.0000 - val_loss: 1.0596 - val_accuracy: 0.6000 - lr: 2.0000e-04\n",
      "Epoch 49/100\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 0.3929 - accuracy: 0.9750 - val_loss: 1.0545 - val_accuracy: 0.6000 - lr: 2.0000e-04\n",
      "Epoch 50/100\n",
      "2/2 [==============================] - 0s 186ms/step - loss: 0.3992 - accuracy: 0.9875 - val_loss: 1.0500 - val_accuracy: 0.6000 - lr: 2.0000e-04\n",
      "Epoch 51/100\n",
      "2/2 [==============================] - 0s 155ms/step - loss: 0.3649 - accuracy: 0.9750 - val_loss: 1.0458 - val_accuracy: 0.6000 - lr: 2.0000e-04\n",
      "Epoch 52/100\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.3586 - accuracy: 0.9875 - val_loss: 1.0416 - val_accuracy: 0.5500 - lr: 2.0000e-04\n",
      "Epoch 53/100\n",
      "2/2 [==============================] - 0s 165ms/step - loss: 0.3671 - accuracy: 0.9750 - val_loss: 1.0365 - val_accuracy: 0.5500 - lr: 2.0000e-04\n",
      "Epoch 54/100\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.3399 - accuracy: 1.0000 - val_loss: 1.0306 - val_accuracy: 0.5500 - lr: 2.0000e-04\n",
      "Epoch 55/100\n",
      "2/2 [==============================] - 0s 228ms/step - loss: 0.3388 - accuracy: 1.0000 - val_loss: 1.0249 - val_accuracy: 0.5500 - lr: 2.0000e-04\n",
      "Epoch 56/100\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.3428 - accuracy: 1.0000 - val_loss: 1.0184 - val_accuracy: 0.5500 - lr: 2.0000e-04\n",
      "Epoch 57/100\n",
      "2/2 [==============================] - 0s 172ms/step - loss: 0.3110 - accuracy: 1.0000 - val_loss: 1.0118 - val_accuracy: 0.5500 - lr: 2.0000e-04\n",
      "Epoch 58/100\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.3018 - accuracy: 1.0000 - val_loss: 1.0045 - val_accuracy: 0.5500 - lr: 2.0000e-04\n",
      "Epoch 59/100\n",
      "2/2 [==============================] - 0s 161ms/step - loss: 0.3090 - accuracy: 1.0000 - val_loss: 0.9974 - val_accuracy: 0.6000 - lr: 2.0000e-04\n",
      "Epoch 60/100\n",
      "2/2 [==============================] - 0s 115ms/step - loss: 0.2972 - accuracy: 1.0000 - val_loss: 0.9901 - val_accuracy: 0.6000 - lr: 2.0000e-04\n",
      "Epoch 61/100\n",
      "2/2 [==============================] - 0s 105ms/step - loss: 0.2926 - accuracy: 1.0000 - val_loss: 0.9822 - val_accuracy: 0.6000 - lr: 2.0000e-04\n",
      "Epoch 62/100\n",
      "2/2 [==============================] - 0s 149ms/step - loss: 0.2781 - accuracy: 1.0000 - val_loss: 0.9740 - val_accuracy: 0.6000 - lr: 2.0000e-04\n",
      "Epoch 63/100\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.2758 - accuracy: 1.0000 - val_loss: 0.9671 - val_accuracy: 0.6500 - lr: 2.0000e-04\n",
      "Epoch 64/100\n",
      "2/2 [==============================] - 0s 162ms/step - loss: 0.2772 - accuracy: 1.0000 - val_loss: 0.9617 - val_accuracy: 0.6500 - lr: 2.0000e-04\n",
      "Epoch 65/100\n",
      "2/2 [==============================] - 0s 112ms/step - loss: 0.2631 - accuracy: 1.0000 - val_loss: 0.9556 - val_accuracy: 0.6500 - lr: 2.0000e-04\n",
      "Epoch 66/100\n",
      "2/2 [==============================] - 0s 124ms/step - loss: 0.2576 - accuracy: 1.0000 - val_loss: 0.9492 - val_accuracy: 0.6500 - lr: 2.0000e-04\n",
      "Epoch 67/100\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.2573 - accuracy: 1.0000 - val_loss: 0.9422 - val_accuracy: 0.6500 - lr: 2.0000e-04\n",
      "Epoch 68/100\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.2485 - accuracy: 1.0000 - val_loss: 0.9346 - val_accuracy: 0.6500 - lr: 2.0000e-04\n",
      "Epoch 69/100\n",
      "2/2 [==============================] - 0s 265ms/step - loss: 0.2479 - accuracy: 1.0000 - val_loss: 0.9273 - val_accuracy: 0.6500 - lr: 2.0000e-04\n",
      "Epoch 70/100\n",
      "2/2 [==============================] - 0s 150ms/step - loss: 0.2555 - accuracy: 1.0000 - val_loss: 0.9202 - val_accuracy: 0.6500 - lr: 2.0000e-04\n",
      "Epoch 71/100\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 0.2308 - accuracy: 1.0000 - val_loss: 0.9129 - val_accuracy: 0.7000 - lr: 2.0000e-04\n",
      "Epoch 72/100\n",
      "2/2 [==============================] - 0s 151ms/step - loss: 0.2427 - accuracy: 1.0000 - val_loss: 0.9053 - val_accuracy: 0.7000 - lr: 2.0000e-04\n",
      "Epoch 73/100\n",
      "2/2 [==============================] - 0s 129ms/step - loss: 0.2358 - accuracy: 1.0000 - val_loss: 0.8981 - val_accuracy: 0.7500 - lr: 2.0000e-04\n",
      "Epoch 74/100\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.2247 - accuracy: 1.0000 - val_loss: 0.8916 - val_accuracy: 0.7500 - lr: 2.0000e-04\n",
      "Epoch 75/100\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 0.2244 - accuracy: 1.0000 - val_loss: 0.8853 - val_accuracy: 0.7500 - lr: 2.0000e-04\n",
      "Epoch 76/100\n",
      "2/2 [==============================] - 0s 112ms/step - loss: 0.2268 - accuracy: 1.0000 - val_loss: 0.8784 - val_accuracy: 0.7500 - lr: 2.0000e-04\n",
      "Epoch 77/100\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.2284 - accuracy: 1.0000 - val_loss: 0.8701 - val_accuracy: 0.8000 - lr: 2.0000e-04\n",
      "Epoch 78/100\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 0.2177 - accuracy: 1.0000 - val_loss: 0.8621 - val_accuracy: 0.8000 - lr: 2.0000e-04\n",
      "Epoch 79/100\n",
      "2/2 [==============================] - 0s 200ms/step - loss: 0.2150 - accuracy: 1.0000 - val_loss: 0.8560 - val_accuracy: 0.8000 - lr: 2.0000e-04\n",
      "Epoch 80/100\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.2095 - accuracy: 1.0000 - val_loss: 0.8516 - val_accuracy: 0.8000 - lr: 2.0000e-04\n",
      "Epoch 81/100\n",
      "2/2 [==============================] - 0s 154ms/step - loss: 0.2136 - accuracy: 1.0000 - val_loss: 0.8492 - val_accuracy: 0.8000 - lr: 2.0000e-04\n",
      "Epoch 82/100\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.2138 - accuracy: 1.0000 - val_loss: 0.8450 - val_accuracy: 0.8500 - lr: 2.0000e-04\n",
      "Epoch 83/100\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 0.1954 - accuracy: 1.0000 - val_loss: 0.8387 - val_accuracy: 0.8500 - lr: 2.0000e-04\n",
      "Epoch 84/100\n",
      "2/2 [==============================] - 0s 122ms/step - loss: 0.1911 - accuracy: 1.0000 - val_loss: 0.8335 - val_accuracy: 0.8500 - lr: 2.0000e-04\n",
      "Epoch 85/100\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.1939 - accuracy: 1.0000 - val_loss: 0.8269 - val_accuracy: 0.8500 - lr: 2.0000e-04\n",
      "Epoch 86/100\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.1998 - accuracy: 1.0000 - val_loss: 0.8212 - val_accuracy: 0.8500 - lr: 2.0000e-04\n",
      "Epoch 87/100\n",
      "2/2 [==============================] - 0s 122ms/step - loss: 0.1843 - accuracy: 1.0000 - val_loss: 0.8179 - val_accuracy: 0.8500 - lr: 2.0000e-04\n",
      "Epoch 88/100\n",
      "2/2 [==============================] - 0s 104ms/step - loss: 0.1962 - accuracy: 1.0000 - val_loss: 0.8139 - val_accuracy: 0.8500 - lr: 2.0000e-04\n",
      "Epoch 89/100\n",
      "2/2 [==============================] - 0s 159ms/step - loss: 0.1908 - accuracy: 1.0000 - val_loss: 0.8080 - val_accuracy: 0.8500 - lr: 2.0000e-04\n",
      "Epoch 90/100\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 0.2013 - accuracy: 1.0000 - val_loss: 0.8006 - val_accuracy: 0.8500 - lr: 2.0000e-04\n",
      "Epoch 91/100\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.1909 - accuracy: 1.0000 - val_loss: 0.7924 - val_accuracy: 0.9000 - lr: 2.0000e-04\n",
      "Epoch 92/100\n",
      "2/2 [==============================] - 0s 98ms/step - loss: 0.1937 - accuracy: 1.0000 - val_loss: 0.7837 - val_accuracy: 0.9000 - lr: 2.0000e-04\n",
      "Epoch 93/100\n",
      "2/2 [==============================] - 0s 130ms/step - loss: 0.1910 - accuracy: 1.0000 - val_loss: 0.7754 - val_accuracy: 0.9000 - lr: 2.0000e-04\n",
      "Epoch 94/100\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.1823 - accuracy: 1.0000 - val_loss: 0.7684 - val_accuracy: 0.9000 - lr: 2.0000e-04\n",
      "Epoch 95/100\n",
      "2/2 [==============================] - 0s 265ms/step - loss: 0.1801 - accuracy: 1.0000 - val_loss: 0.7623 - val_accuracy: 0.9000 - lr: 2.0000e-04\n",
      "Epoch 96/100\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.1812 - accuracy: 1.0000 - val_loss: 0.7559 - val_accuracy: 0.9000 - lr: 2.0000e-04\n",
      "Epoch 97/100\n",
      "2/2 [==============================] - 0s 152ms/step - loss: 0.1802 - accuracy: 1.0000 - val_loss: 0.7466 - val_accuracy: 0.9000 - lr: 2.0000e-04\n",
      "Epoch 98/100\n",
      "2/2 [==============================] - 0s 131ms/step - loss: 0.1787 - accuracy: 1.0000 - val_loss: 0.7369 - val_accuracy: 0.9000 - lr: 2.0000e-04\n",
      "Epoch 99/100\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.1717 - accuracy: 1.0000 - val_loss: 0.7278 - val_accuracy: 0.9000 - lr: 2.0000e-04\n",
      "Epoch 100/100\n",
      "2/2 [==============================] - 0s 176ms/step - loss: 0.1773 - accuracy: 1.0000 - val_loss: 0.7195 - val_accuracy: 0.9000 - lr: 2.0000e-04\n",
      "1/1 [==============================] - 0s 139ms/step - loss: 0.6161 - accuracy: 0.9333\n",
      "Evaluation Metrics for RNN:\n",
      "loss: 0.6161287426948547\n",
      "accuracy: 0.9333333373069763\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "\n",
    "X_train_rnn, X_test_rnn, y_train_rnn, y_test_rnn = train_test_split(X, Y, test_size=0.2)\n",
    "\n",
    "def RNN_model(vocab_size, sequence_length):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=vocab_size, output_dim=128, input_length=sequence_length))\n",
    "    model.add(SimpleRNN(units=128, activation='relu', return_sequences=True))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(SimpleRNN(units=64, activation='sigmoid'))\n",
    "    model.add(Dense(256, activation='sigmoid', kernel_regularizer=l2(0.001)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "    optimizer = Adam(learning_rate=0.001)\n",
    "    \n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=0.0001, verbose=1)\n",
    "#EMBEDDING_DIM = 110\n",
    "rnn_model = RNN_model(vocab_size, max_sequence_length)\n",
    "early_stopping_rnn = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "history_updated_rnn = rnn_model.fit(X_train_rnn, y_train_rnn, validation_data=(X_test_rnn, y_test_rnn), epochs=100, batch_size=64, callbacks=[reduce_lr])\n",
    "\n",
    "# Evaluate the RNN model\n",
    "evaluation_metrics_updated_rnn = rnn_model.evaluate(X_test, Y_test)\n",
    "print(\"Evaluation Metrics for RNN:\")\n",
    "for metric_name, metric_value in zip(rnn_model.metrics_names, evaluation_metrics_updated_rnn):\n",
    "    print(f\"{metric_name}: {metric_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*<h4>LSTM Model<h4>*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2/2 [==============================] - 10s 1s/step - loss: 1.1890 - accuracy: 0.4875 - val_loss: 1.1949 - val_accuracy: 0.4000\n",
      "Epoch 2/100\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 1.1171 - accuracy: 0.8875 - val_loss: 1.1886 - val_accuracy: 0.4000\n",
      "Epoch 3/100\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 1.0250 - accuracy: 0.8000 - val_loss: 1.1807 - val_accuracy: 0.4000\n",
      "Epoch 4/100\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 0.8716 - accuracy: 0.8500 - val_loss: 1.1709 - val_accuracy: 0.4000\n",
      "Epoch 5/100\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 0.6768 - accuracy: 0.8500 - val_loss: 1.1575 - val_accuracy: 0.4000\n",
      "Epoch 6/100\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 0.4641 - accuracy: 0.8375 - val_loss: 1.1385 - val_accuracy: 0.4000\n",
      "Epoch 7/100\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.3230 - accuracy: 0.9000 - val_loss: 1.1210 - val_accuracy: 0.4500\n",
      "Epoch 8/100\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.3273 - accuracy: 0.9250 - val_loss: 1.1163 - val_accuracy: 0.4500\n",
      "Epoch 9/100\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 0.2590 - accuracy: 0.9250 - val_loss: 1.1039 - val_accuracy: 0.4500\n",
      "Epoch 10/100\n",
      "2/2 [==============================] - 0s 106ms/step - loss: 0.2129 - accuracy: 0.9625 - val_loss: 1.1010 - val_accuracy: 0.4500\n",
      "Epoch 11/100\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 0.1647 - accuracy: 0.9750 - val_loss: 1.1053 - val_accuracy: 0.4000\n",
      "Epoch 12/100\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 0.2963 - accuracy: 0.8875 - val_loss: 1.0987 - val_accuracy: 0.4000\n",
      "Epoch 13/100\n",
      "2/2 [==============================] - 0s 112ms/step - loss: 0.1623 - accuracy: 0.9750 - val_loss: 1.0953 - val_accuracy: 0.4000\n",
      "Epoch 14/100\n",
      "2/2 [==============================] - 0s 268ms/step - loss: 0.2333 - accuracy: 0.9500 - val_loss: 1.0919 - val_accuracy: 0.4000\n",
      "Epoch 15/100\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.2146 - accuracy: 0.9625 - val_loss: 1.0880 - val_accuracy: 0.4000\n",
      "Epoch 16/100\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 0.1684 - accuracy: 0.9625 - val_loss: 1.0830 - val_accuracy: 0.4000\n",
      "Epoch 17/100\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 0.1298 - accuracy: 1.0000 - val_loss: 1.0761 - val_accuracy: 0.4000\n",
      "Epoch 18/100\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 0.1119 - accuracy: 1.0000 - val_loss: 1.0672 - val_accuracy: 0.4500\n",
      "Epoch 19/100\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.1047 - accuracy: 1.0000 - val_loss: 1.0579 - val_accuracy: 0.4500\n",
      "Epoch 20/100\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 0.0969 - accuracy: 1.0000 - val_loss: 1.0486 - val_accuracy: 0.4500\n",
      "Epoch 21/100\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 0.0936 - accuracy: 1.0000 - val_loss: 1.0393 - val_accuracy: 0.4500\n",
      "Epoch 22/100\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.0874 - accuracy: 1.0000 - val_loss: 1.0300 - val_accuracy: 0.4500\n",
      "Epoch 23/100\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 0.0889 - accuracy: 1.0000 - val_loss: 1.0215 - val_accuracy: 0.4500\n",
      "Epoch 24/100\n",
      "2/2 [==============================] - 0s 454ms/step - loss: 0.0838 - accuracy: 1.0000 - val_loss: 1.0142 - val_accuracy: 0.4500\n",
      "Epoch 25/100\n",
      "2/2 [==============================] - 0s 282ms/step - loss: 0.0781 - accuracy: 1.0000 - val_loss: 1.0077 - val_accuracy: 0.4500\n",
      "Epoch 26/100\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 0.0769 - accuracy: 1.0000 - val_loss: 1.0017 - val_accuracy: 0.4500\n",
      "Epoch 27/100\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 0.0763 - accuracy: 1.0000 - val_loss: 0.9962 - val_accuracy: 0.4500\n",
      "Epoch 28/100\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 0.0765 - accuracy: 1.0000 - val_loss: 0.9912 - val_accuracy: 0.4500\n",
      "Epoch 29/100\n",
      "2/2 [==============================] - 0s 156ms/step - loss: 0.0748 - accuracy: 1.0000 - val_loss: 0.9858 - val_accuracy: 0.4500\n",
      "Epoch 30/100\n",
      "2/2 [==============================] - 0s 261ms/step - loss: 0.0745 - accuracy: 1.0000 - val_loss: 0.9802 - val_accuracy: 0.4500\n",
      "Epoch 31/100\n",
      "2/2 [==============================] - 0s 208ms/step - loss: 0.0728 - accuracy: 1.0000 - val_loss: 0.9749 - val_accuracy: 0.4500\n",
      "Epoch 32/100\n",
      "2/2 [==============================] - 0s 196ms/step - loss: 0.0743 - accuracy: 1.0000 - val_loss: 0.9741 - val_accuracy: 0.4500\n",
      "Epoch 33/100\n",
      "2/2 [==============================] - 1s 379ms/step - loss: 0.0711 - accuracy: 1.0000 - val_loss: 0.9775 - val_accuracy: 0.4500\n",
      "Epoch 34/100\n",
      "2/2 [==============================] - 0s 234ms/step - loss: 0.0703 - accuracy: 1.0000 - val_loss: 0.9803 - val_accuracy: 0.4500\n",
      "Epoch 35/100\n",
      "2/2 [==============================] - 1s 757ms/step - loss: 0.0725 - accuracy: 1.0000 - val_loss: 0.9777 - val_accuracy: 0.4500\n",
      "Epoch 36/100\n",
      "2/2 [==============================] - 0s 123ms/step - loss: 0.0690 - accuracy: 1.0000 - val_loss: 0.9715 - val_accuracy: 0.4500\n",
      "Epoch 37/100\n",
      "2/2 [==============================] - 0s 218ms/step - loss: 0.0686 - accuracy: 1.0000 - val_loss: 0.9661 - val_accuracy: 0.4500\n",
      "Epoch 38/100\n",
      "2/2 [==============================] - 1s 683ms/step - loss: 0.0676 - accuracy: 1.0000 - val_loss: 0.9614 - val_accuracy: 0.4500\n",
      "Epoch 39/100\n",
      "2/2 [==============================] - 0s 123ms/step - loss: 0.0669 - accuracy: 1.0000 - val_loss: 0.9572 - val_accuracy: 0.4500\n",
      "Epoch 40/100\n",
      "2/2 [==============================] - 0s 344ms/step - loss: 0.0663 - accuracy: 1.0000 - val_loss: 0.9533 - val_accuracy: 0.4500\n",
      "Epoch 41/100\n",
      "2/2 [==============================] - 0s 331ms/step - loss: 0.0657 - accuracy: 1.0000 - val_loss: 0.9496 - val_accuracy: 0.4500\n",
      "Epoch 42/100\n",
      "2/2 [==============================] - 0s 253ms/step - loss: 0.0652 - accuracy: 1.0000 - val_loss: 0.9462 - val_accuracy: 0.4500\n",
      "Epoch 43/100\n",
      "2/2 [==============================] - 0s 240ms/step - loss: 0.0646 - accuracy: 1.0000 - val_loss: 0.9429 - val_accuracy: 0.4500\n",
      "Epoch 44/100\n",
      "2/2 [==============================] - 0s 109ms/step - loss: 0.0644 - accuracy: 1.0000 - val_loss: 0.9396 - val_accuracy: 0.5000\n",
      "Epoch 45/100\n",
      "2/2 [==============================] - 0s 410ms/step - loss: 0.0636 - accuracy: 1.0000 - val_loss: 0.9364 - val_accuracy: 0.5000\n",
      "Epoch 46/100\n",
      "2/2 [==============================] - 0s 314ms/step - loss: 0.0629 - accuracy: 1.0000 - val_loss: 0.9333 - val_accuracy: 0.5000\n",
      "Epoch 47/100\n",
      "2/2 [==============================] - 0s 262ms/step - loss: 0.0623 - accuracy: 1.0000 - val_loss: 0.9303 - val_accuracy: 0.5000\n",
      "Epoch 48/100\n",
      "2/2 [==============================] - 0s 149ms/step - loss: 0.0618 - accuracy: 1.0000 - val_loss: 0.9274 - val_accuracy: 0.5000\n",
      "Epoch 49/100\n",
      "2/2 [==============================] - 0s 153ms/step - loss: 0.0613 - accuracy: 1.0000 - val_loss: 0.9245 - val_accuracy: 0.5000\n",
      "Epoch 50/100\n",
      "2/2 [==============================] - 0s 150ms/step - loss: 0.0611 - accuracy: 1.0000 - val_loss: 0.9217 - val_accuracy: 0.5000\n",
      "Epoch 51/100\n",
      "2/2 [==============================] - 0s 266ms/step - loss: 0.0603 - accuracy: 1.0000 - val_loss: 0.9188 - val_accuracy: 0.5000\n",
      "Epoch 52/100\n",
      "2/2 [==============================] - 0s 381ms/step - loss: 0.0599 - accuracy: 1.0000 - val_loss: 0.9159 - val_accuracy: 0.5000\n",
      "Epoch 53/100\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 0.0594 - accuracy: 1.0000 - val_loss: 0.9132 - val_accuracy: 0.5000\n",
      "Epoch 54/100\n",
      "2/2 [==============================] - 0s 167ms/step - loss: 0.0589 - accuracy: 1.0000 - val_loss: 0.9104 - val_accuracy: 0.5500\n",
      "Epoch 55/100\n",
      "2/2 [==============================] - 0s 115ms/step - loss: 0.0584 - accuracy: 1.0000 - val_loss: 0.9076 - val_accuracy: 0.5500\n",
      "Epoch 56/100\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 0.0579 - accuracy: 1.0000 - val_loss: 0.9048 - val_accuracy: 0.5500\n",
      "Epoch 57/100\n",
      "2/2 [==============================] - 0s 106ms/step - loss: 0.0575 - accuracy: 1.0000 - val_loss: 0.9019 - val_accuracy: 0.5500\n",
      "Epoch 58/100\n",
      "2/2 [==============================] - 0s 200ms/step - loss: 0.0570 - accuracy: 1.0000 - val_loss: 0.8991 - val_accuracy: 0.5500\n",
      "Epoch 59/100\n",
      "2/2 [==============================] - 0s 408ms/step - loss: 0.0566 - accuracy: 1.0000 - val_loss: 0.8964 - val_accuracy: 0.5500\n",
      "Epoch 60/100\n",
      "2/2 [==============================] - 1s 558ms/step - loss: 0.0562 - accuracy: 1.0000 - val_loss: 0.8936 - val_accuracy: 0.5500\n",
      "Epoch 61/100\n",
      "2/2 [==============================] - 0s 234ms/step - loss: 0.0558 - accuracy: 1.0000 - val_loss: 0.8908 - val_accuracy: 0.5500\n",
      "Epoch 62/100\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 0.0553 - accuracy: 1.0000 - val_loss: 0.8881 - val_accuracy: 0.5500\n",
      "Epoch 63/100\n",
      "2/2 [==============================] - 0s 159ms/step - loss: 0.0550 - accuracy: 1.0000 - val_loss: 0.8853 - val_accuracy: 0.5500\n",
      "Epoch 64/100\n",
      "2/2 [==============================] - 0s 158ms/step - loss: 0.0546 - accuracy: 1.0000 - val_loss: 0.8823 - val_accuracy: 0.5500\n",
      "Epoch 65/100\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.0541 - accuracy: 1.0000 - val_loss: 0.8793 - val_accuracy: 0.5500\n",
      "Epoch 66/100\n",
      "2/2 [==============================] - 0s 114ms/step - loss: 0.0537 - accuracy: 1.0000 - val_loss: 0.8764 - val_accuracy: 0.5500\n",
      "Epoch 67/100\n",
      "2/2 [==============================] - 0s 161ms/step - loss: 0.0534 - accuracy: 1.0000 - val_loss: 0.8733 - val_accuracy: 0.5500\n",
      "Epoch 68/100\n",
      "2/2 [==============================] - 0s 184ms/step - loss: 0.0530 - accuracy: 1.0000 - val_loss: 0.8704 - val_accuracy: 0.5500\n",
      "Epoch 69/100\n",
      "2/2 [==============================] - 0s 151ms/step - loss: 0.0526 - accuracy: 1.0000 - val_loss: 0.8675 - val_accuracy: 0.5500\n",
      "Epoch 70/100\n",
      "2/2 [==============================] - 0s 162ms/step - loss: 0.0523 - accuracy: 1.0000 - val_loss: 0.8644 - val_accuracy: 0.5500\n",
      "Epoch 71/100\n",
      "2/2 [==============================] - 1s 431ms/step - loss: 0.0519 - accuracy: 1.0000 - val_loss: 0.8614 - val_accuracy: 0.5500\n",
      "Epoch 72/100\n",
      "2/2 [==============================] - 0s 243ms/step - loss: 0.0515 - accuracy: 1.0000 - val_loss: 0.8584 - val_accuracy: 0.5500\n",
      "Epoch 73/100\n",
      "2/2 [==============================] - 0s 173ms/step - loss: 0.0512 - accuracy: 1.0000 - val_loss: 0.8554 - val_accuracy: 0.5500\n",
      "Epoch 74/100\n",
      "2/2 [==============================] - 0s 213ms/step - loss: 0.0508 - accuracy: 1.0000 - val_loss: 0.8524 - val_accuracy: 0.5500\n",
      "Epoch 75/100\n",
      "2/2 [==============================] - 0s 347ms/step - loss: 0.0505 - accuracy: 1.0000 - val_loss: 0.8492 - val_accuracy: 0.5500\n",
      "Epoch 76/100\n",
      "2/2 [==============================] - 0s 264ms/step - loss: 0.0502 - accuracy: 1.0000 - val_loss: 0.8462 - val_accuracy: 0.6000\n",
      "Epoch 77/100\n",
      "2/2 [==============================] - 0s 252ms/step - loss: 0.0499 - accuracy: 1.0000 - val_loss: 0.8429 - val_accuracy: 0.6000\n",
      "Epoch 78/100\n",
      "2/2 [==============================] - 0s 352ms/step - loss: 0.0495 - accuracy: 1.0000 - val_loss: 0.8396 - val_accuracy: 0.6000\n",
      "Epoch 79/100\n",
      "2/2 [==============================] - 1s 474ms/step - loss: 0.0492 - accuracy: 1.0000 - val_loss: 0.8364 - val_accuracy: 0.6000\n",
      "Epoch 80/100\n",
      "2/2 [==============================] - 1s 622ms/step - loss: 0.0489 - accuracy: 1.0000 - val_loss: 0.8334 - val_accuracy: 0.6000\n",
      "Epoch 81/100\n",
      "2/2 [==============================] - 0s 225ms/step - loss: 0.0485 - accuracy: 1.0000 - val_loss: 0.8303 - val_accuracy: 0.6000\n",
      "Epoch 82/100\n",
      "2/2 [==============================] - 0s 114ms/step - loss: 0.0482 - accuracy: 1.0000 - val_loss: 0.8273 - val_accuracy: 0.6000\n",
      "Epoch 83/100\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 0.0479 - accuracy: 1.0000 - val_loss: 0.8243 - val_accuracy: 0.6000\n",
      "Epoch 84/100\n",
      "2/2 [==============================] - 0s 388ms/step - loss: 0.0477 - accuracy: 1.0000 - val_loss: 0.8216 - val_accuracy: 0.6000\n",
      "Epoch 85/100\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 0.0474 - accuracy: 1.0000 - val_loss: 0.8190 - val_accuracy: 0.6000\n",
      "Epoch 86/100\n",
      "2/2 [==============================] - 0s 115ms/step - loss: 0.0470 - accuracy: 1.0000 - val_loss: 0.8164 - val_accuracy: 0.6000\n",
      "Epoch 87/100\n",
      "2/2 [==============================] - 0s 161ms/step - loss: 0.0467 - accuracy: 1.0000 - val_loss: 0.8137 - val_accuracy: 0.6000\n",
      "Epoch 88/100\n",
      "2/2 [==============================] - 0s 220ms/step - loss: 0.0465 - accuracy: 1.0000 - val_loss: 0.8106 - val_accuracy: 0.6000\n",
      "Epoch 89/100\n",
      "2/2 [==============================] - 0s 290ms/step - loss: 0.0461 - accuracy: 1.0000 - val_loss: 0.8077 - val_accuracy: 0.6000\n",
      "Epoch 90/100\n",
      "2/2 [==============================] - 0s 177ms/step - loss: 0.0459 - accuracy: 1.0000 - val_loss: 0.8046 - val_accuracy: 0.6000\n",
      "Epoch 91/100\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.0456 - accuracy: 1.0000 - val_loss: 0.8015 - val_accuracy: 0.6000\n",
      "Epoch 92/100\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 0.0453 - accuracy: 1.0000 - val_loss: 0.7982 - val_accuracy: 0.6500\n",
      "Epoch 93/100\n",
      "2/2 [==============================] - 0s 287ms/step - loss: 0.0450 - accuracy: 1.0000 - val_loss: 0.7949 - val_accuracy: 0.6500\n",
      "Epoch 94/100\n",
      "2/2 [==============================] - 0s 110ms/step - loss: 0.0447 - accuracy: 1.0000 - val_loss: 0.7918 - val_accuracy: 0.6500\n",
      "Epoch 95/100\n",
      "2/2 [==============================] - 0s 108ms/step - loss: 0.0445 - accuracy: 1.0000 - val_loss: 0.7884 - val_accuracy: 0.6500\n",
      "Epoch 96/100\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 0.0442 - accuracy: 1.0000 - val_loss: 0.7850 - val_accuracy: 0.7000\n",
      "Epoch 97/100\n",
      "2/2 [==============================] - 0s 181ms/step - loss: 0.0440 - accuracy: 1.0000 - val_loss: 0.7816 - val_accuracy: 0.7000\n",
      "Epoch 98/100\n",
      "2/2 [==============================] - 0s 238ms/step - loss: 0.0437 - accuracy: 1.0000 - val_loss: 0.7783 - val_accuracy: 0.7000\n",
      "Epoch 99/100\n",
      "2/2 [==============================] - 0s 168ms/step - loss: 0.0434 - accuracy: 1.0000 - val_loss: 0.7749 - val_accuracy: 0.7000\n",
      "Epoch 100/100\n",
      "2/2 [==============================] - 0s 164ms/step - loss: 0.0432 - accuracy: 1.0000 - val_loss: 0.7714 - val_accuracy: 0.7500\n",
      "1/1 [==============================] - 0s 253ms/step - loss: 0.7858 - accuracy: 0.8000\n",
      "Evaluation Metrics for LSTM:\n",
      "loss: 0.7858165502548218\n",
      "accuracy: 0.800000011920929\n"
     ]
    }
   ],
   "source": [
    "def LSTM_model(vocab_size, sequence_length):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=vocab_size, output_dim=128, input_length=sequence_length))\n",
    "    model.add(LSTM(units=128, activation='relu', return_sequences=True))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LSTM(units=64, activation='relu'))\n",
    "    model.add(Dense(256, activation='relu', kernel_regularizer=l2(0.001)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "    optimizer = Adam(learning_rate=0.001)\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "#EMBEDDING_DIM = 110\n",
    "lstm_model = LSTM_model(vocab_size, max_sequence_length)\n",
    "early_stopping_rnn = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "history_updated_rnn = lstm_model.fit(X_train_rnn, y_train_rnn, validation_data=(X_test_rnn, y_test_rnn), epochs=100, batch_size=64, callbacks=[early_stopping_rnn])\n",
    "\n",
    "\n",
    "# Evaluate the lstm model\n",
    "evaluation_metrics_updated_lstm = lstm_model.evaluate(X_test, Y_test)\n",
    "print(\"Evaluation Metrics for LSTM:\")\n",
    "for metric_name, metric_value in zip(lstm_model.metrics_names, evaluation_metrics_updated_lstm):\n",
    "    print(f\"{metric_name}: {metric_value}\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*<h4>TRANSFORMER Model<h4>*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, density, rate=0.1, l2_reg=0.01):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            layers.Dense(density, activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(l2_reg)),\n",
    "            layers.Dense(embed_dim, kernel_regularizer=tf.keras.regularizers.l2(l2_reg)),\n",
    "        ])\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "\n",
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions\n",
    "\n",
    "def build_transformer_model(maxlen, vocab_size, embed_dim, num_heads, density, transformer_units, mlp_units, dropout_rate, num_classes):\n",
    "    inputs = layers.Input(shape=(maxlen,))\n",
    "    embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
    "    x = embedding_layer(inputs)\n",
    "    for _ in range(transformer_units):\n",
    "        x = TransformerBlock(embed_dim, num_heads, density, rate=dropout_rate)(x)\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    for dim in mlp_units:\n",
    "        x = layers.Dense(dim, activation=\"relu\")(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "    outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = max_sequence_length\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "embed_dim = 120\n",
    "num_heads = 2\n",
    "density = 3\n",
    "transformer_units = 4\n",
    "mlp_units = [128]\n",
    "dropout_rate = 0.5\n",
    "num_classes = len(df['grade'].unique())\n",
    "\n",
    "transformer_model = build_transformer_model(maxlen, vocab_size, embed_dim, num_heads, density, transformer_units, mlp_units, dropout_rate, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\amine\\anaconda3\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Epoch 1/30\n",
      "3/3 [==============================] - 39s 3s/step - loss: 1.6266 - accuracy: 0.3294 - val_loss: 1.4322 - val_accuracy: 0.7333\n",
      "Epoch 2/30\n",
      "3/3 [==============================] - 0s 97ms/step - loss: 1.4532 - accuracy: 0.4941 - val_loss: 1.2497 - val_accuracy: 0.7333\n",
      "Epoch 3/30\n",
      "3/3 [==============================] - 0s 117ms/step - loss: 1.4311 - accuracy: 0.5294 - val_loss: 1.2159 - val_accuracy: 0.5333\n",
      "Epoch 4/30\n",
      "3/3 [==============================] - 0s 154ms/step - loss: 1.2749 - accuracy: 0.6588 - val_loss: 0.9405 - val_accuracy: 0.9333\n",
      "Epoch 5/30\n",
      "3/3 [==============================] - 0s 92ms/step - loss: 1.1422 - accuracy: 0.7294 - val_loss: 0.7597 - val_accuracy: 0.9333\n",
      "Epoch 6/30\n",
      "3/3 [==============================] - 0s 82ms/step - loss: 0.9655 - accuracy: 0.8471 - val_loss: 0.6425 - val_accuracy: 1.0000\n",
      "Epoch 7/30\n",
      "3/3 [==============================] - 0s 90ms/step - loss: 0.7279 - accuracy: 0.9294 - val_loss: 0.5244 - val_accuracy: 1.0000\n",
      "Epoch 8/30\n",
      "3/3 [==============================] - 0s 92ms/step - loss: 0.5761 - accuracy: 0.9529 - val_loss: 0.4619 - val_accuracy: 1.0000\n",
      "Epoch 9/30\n",
      "3/3 [==============================] - 0s 76ms/step - loss: 0.4838 - accuracy: 0.9882 - val_loss: 0.4802 - val_accuracy: 0.9333\n",
      "Epoch 10/30\n",
      "3/3 [==============================] - 0s 145ms/step - loss: 0.4325 - accuracy: 1.0000 - val_loss: 0.4051 - val_accuracy: 1.0000\n",
      "Epoch 11/30\n",
      "3/3 [==============================] - 0s 72ms/step - loss: 0.4088 - accuracy: 1.0000 - val_loss: 0.3961 - val_accuracy: 1.0000\n",
      "Epoch 12/30\n",
      "3/3 [==============================] - 0s 95ms/step - loss: 0.4003 - accuracy: 1.0000 - val_loss: 0.3890 - val_accuracy: 1.0000\n",
      "Epoch 13/30\n",
      "3/3 [==============================] - 0s 69ms/step - loss: 0.3925 - accuracy: 1.0000 - val_loss: 0.3820 - val_accuracy: 1.0000\n",
      "Epoch 14/30\n",
      "3/3 [==============================] - 0s 84ms/step - loss: 0.3888 - accuracy: 1.0000 - val_loss: 0.3751 - val_accuracy: 1.0000\n",
      "Epoch 15/30\n",
      "3/3 [==============================] - 0s 85ms/step - loss: 0.3757 - accuracy: 1.0000 - val_loss: 0.3682 - val_accuracy: 1.0000\n",
      "Epoch 16/30\n",
      "3/3 [==============================] - 0s 71ms/step - loss: 0.3679 - accuracy: 1.0000 - val_loss: 0.3615 - val_accuracy: 1.0000\n",
      "Epoch 17/30\n",
      "3/3 [==============================] - 0s 73ms/step - loss: 0.3609 - accuracy: 1.0000 - val_loss: 0.3554 - val_accuracy: 1.0000\n",
      "Epoch 18/30\n",
      "3/3 [==============================] - 0s 79ms/step - loss: 0.3546 - accuracy: 1.0000 - val_loss: 0.3521 - val_accuracy: 1.0000\n",
      "Epoch 19/30\n",
      "3/3 [==============================] - 0s 72ms/step - loss: 0.3461 - accuracy: 1.0000 - val_loss: 0.3564 - val_accuracy: 1.0000\n",
      "Epoch 20/30\n",
      "3/3 [==============================] - 0s 70ms/step - loss: 0.3408 - accuracy: 1.0000 - val_loss: 0.3747 - val_accuracy: 1.0000\n",
      "Epoch 21/30\n",
      "3/3 [==============================] - 0s 116ms/step - loss: 0.3320 - accuracy: 1.0000 - val_loss: 0.4017 - val_accuracy: 0.9333\n",
      "Epoch 22/30\n",
      "3/3 [==============================] - 0s 89ms/step - loss: 0.3257 - accuracy: 1.0000 - val_loss: 0.4240 - val_accuracy: 0.9333\n",
      "Epoch 23/30\n",
      "3/3 [==============================] - 0s 121ms/step - loss: 0.3187 - accuracy: 1.0000 - val_loss: 0.4359 - val_accuracy: 0.9333\n",
      "Epoch 24/30\n",
      "3/3 [==============================] - 0s 83ms/step - loss: 0.3116 - accuracy: 1.0000 - val_loss: 0.4376 - val_accuracy: 0.9333\n",
      "Epoch 25/30\n",
      "3/3 [==============================] - 0s 70ms/step - loss: 0.3055 - accuracy: 1.0000 - val_loss: 0.4292 - val_accuracy: 0.9333\n",
      "Epoch 26/30\n",
      "3/3 [==============================] - 0s 72ms/step - loss: 0.2981 - accuracy: 1.0000 - val_loss: 0.4176 - val_accuracy: 0.9333\n",
      "Epoch 27/30\n",
      "3/3 [==============================] - 0s 70ms/step - loss: 0.2921 - accuracy: 1.0000 - val_loss: 0.4083 - val_accuracy: 0.9333\n",
      "Epoch 28/30\n",
      "3/3 [==============================] - 0s 76ms/step - loss: 0.2859 - accuracy: 1.0000 - val_loss: 0.3988 - val_accuracy: 0.9333\n",
      "Epoch 29/30\n",
      "3/3 [==============================] - 0s 75ms/step - loss: 0.2794 - accuracy: 1.0000 - val_loss: 0.3839 - val_accuracy: 0.9333\n",
      "Epoch 30/30\n",
      "3/3 [==============================] - 0s 80ms/step - loss: 0.2724 - accuracy: 1.0000 - val_loss: 0.3707 - val_accuracy: 0.9333\n",
      "Epoch 1/30\n",
      "3/3 [==============================] - 0s 112ms/step - loss: 0.2672 - accuracy: 1.0000 - val_loss: 0.3591 - val_accuracy: 0.9333\n",
      "Epoch 2/30\n",
      "3/3 [==============================] - 0s 70ms/step - loss: 0.2604 - accuracy: 1.0000 - val_loss: 0.3491 - val_accuracy: 0.9333\n",
      "Epoch 3/30\n",
      "3/3 [==============================] - 0s 69ms/step - loss: 0.2553 - accuracy: 1.0000 - val_loss: 0.3431 - val_accuracy: 0.9333\n",
      "Epoch 4/30\n",
      "3/3 [==============================] - 0s 80ms/step - loss: 0.2480 - accuracy: 1.0000 - val_loss: 0.3375 - val_accuracy: 0.9333\n",
      "Epoch 5/30\n",
      "3/3 [==============================] - 0s 71ms/step - loss: 0.2423 - accuracy: 1.0000 - val_loss: 0.3305 - val_accuracy: 0.9333\n",
      "Epoch 6/30\n",
      "3/3 [==============================] - 0s 111ms/step - loss: 0.2368 - accuracy: 1.0000 - val_loss: 0.3234 - val_accuracy: 0.9333\n",
      "Epoch 7/30\n",
      "3/3 [==============================] - 0s 71ms/step - loss: 0.2313 - accuracy: 1.0000 - val_loss: 0.3197 - val_accuracy: 0.9333\n",
      "Epoch 8/30\n",
      "3/3 [==============================] - 0s 75ms/step - loss: 0.2252 - accuracy: 1.0000 - val_loss: 0.3198 - val_accuracy: 0.9333\n",
      "Epoch 9/30\n",
      "3/3 [==============================] - 0s 69ms/step - loss: 0.2198 - accuracy: 1.0000 - val_loss: 0.3182 - val_accuracy: 0.9333\n",
      "Epoch 10/30\n",
      "3/3 [==============================] - 0s 71ms/step - loss: 0.2144 - accuracy: 1.0000 - val_loss: 0.3152 - val_accuracy: 0.9333\n",
      "Epoch 11/30\n",
      "3/3 [==============================] - 0s 72ms/step - loss: 0.2091 - accuracy: 1.0000 - val_loss: 0.3119 - val_accuracy: 0.9333\n",
      "Epoch 12/30\n",
      "3/3 [==============================] - 0s 91ms/step - loss: 0.2041 - accuracy: 1.0000 - val_loss: 0.3049 - val_accuracy: 0.9333\n",
      "Epoch 13/30\n",
      "3/3 [==============================] - 0s 69ms/step - loss: 0.1995 - accuracy: 1.0000 - val_loss: 0.2963 - val_accuracy: 0.9333\n",
      "Epoch 14/30\n",
      "3/3 [==============================] - 0s 64ms/step - loss: 0.1938 - accuracy: 1.0000 - val_loss: 0.2877 - val_accuracy: 0.9333\n",
      "Epoch 15/30\n",
      "3/3 [==============================] - 0s 70ms/step - loss: 0.1891 - accuracy: 1.0000 - val_loss: 0.2795 - val_accuracy: 0.9333\n",
      "Epoch 16/30\n",
      "3/3 [==============================] - 0s 72ms/step - loss: 0.1841 - accuracy: 1.0000 - val_loss: 0.2714 - val_accuracy: 0.9333\n",
      "Epoch 17/30\n",
      "3/3 [==============================] - 0s 72ms/step - loss: 0.1798 - accuracy: 1.0000 - val_loss: 0.2622 - val_accuracy: 0.9333\n",
      "Epoch 18/30\n",
      "3/3 [==============================] - 0s 68ms/step - loss: 0.1755 - accuracy: 1.0000 - val_loss: 0.2526 - val_accuracy: 0.9333\n",
      "Epoch 19/30\n",
      "3/3 [==============================] - 0s 69ms/step - loss: 0.1704 - accuracy: 1.0000 - val_loss: 0.2434 - val_accuracy: 0.9333\n",
      "Epoch 20/30\n",
      "3/3 [==============================] - 0s 78ms/step - loss: 0.1661 - accuracy: 1.0000 - val_loss: 0.2361 - val_accuracy: 0.9333\n",
      "Epoch 21/30\n",
      "3/3 [==============================] - 0s 69ms/step - loss: 0.1617 - accuracy: 1.0000 - val_loss: 0.2296 - val_accuracy: 0.9333\n",
      "Epoch 22/30\n",
      "3/3 [==============================] - 0s 76ms/step - loss: 0.1576 - accuracy: 1.0000 - val_loss: 0.2226 - val_accuracy: 0.9333\n",
      "Epoch 23/30\n",
      "3/3 [==============================] - 0s 73ms/step - loss: 0.1536 - accuracy: 1.0000 - val_loss: 0.2156 - val_accuracy: 0.9333\n",
      "Epoch 24/30\n",
      "3/3 [==============================] - 0s 150ms/step - loss: 0.1495 - accuracy: 1.0000 - val_loss: 0.2101 - val_accuracy: 0.9333\n",
      "Epoch 25/30\n",
      "3/3 [==============================] - 0s 71ms/step - loss: 0.1454 - accuracy: 1.0000 - val_loss: 0.2049 - val_accuracy: 0.9333\n",
      "Epoch 26/30\n",
      "3/3 [==============================] - 0s 75ms/step - loss: 0.1416 - accuracy: 1.0000 - val_loss: 0.1993 - val_accuracy: 0.9333\n",
      "Epoch 27/30\n",
      "3/3 [==============================] - 0s 79ms/step - loss: 0.1379 - accuracy: 1.0000 - val_loss: 0.1951 - val_accuracy: 0.9333\n",
      "Epoch 28/30\n",
      "3/3 [==============================] - 0s 70ms/step - loss: 0.1347 - accuracy: 1.0000 - val_loss: 0.1910 - val_accuracy: 0.9333\n",
      "Epoch 29/30\n",
      "3/3 [==============================] - 0s 70ms/step - loss: 0.1308 - accuracy: 1.0000 - val_loss: 0.1875 - val_accuracy: 0.9333\n",
      "Epoch 30/30\n",
      "3/3 [==============================] - 0s 70ms/step - loss: 0.1271 - accuracy: 1.0000 - val_loss: 0.1830 - val_accuracy: 0.9333\n"
     ]
    }
   ],
   "source": [
    "transformer_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "transformer_model.fit(X_train, Y_train, epochs=30, batch_size=32, validation_data=(X_test, Y_test))\n",
    "history = transformer_model.fit(X_train, Y_train, epochs=30, batch_size=32, validation_data=(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 62ms/step - loss: 0.1830 - accuracy: 0.9333\n",
      "Evaluation Metrics Transformer:\n",
      "loss: 0.18296551704406738\n",
      "accuracy: 0.9333333373069763\n"
     ]
    }
   ],
   "source": [
    "evaluation_metrics_transformer = transformer_model.evaluate(X_test, Y_test)\n",
    "print(\"Evaluation Metrics Transformer:\")\n",
    "for metric_name, metric_value in zip(transformer_model.metrics_names, evaluation_metrics_transformer):\n",
    "    print(f\"{metric_name}: {metric_value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 54ms/step - loss: 0.1830 - accuracy: 0.9333\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "[[ 0  0  0  1 16 12  5  8 14 90 10 24 27  4 91 11 15]\n",
      " [ 0  0  0  0  6  1  5  2  3  7  8  4  1 33 34 11 15]\n",
      " [ 0  0  0  0  0  0  0  6  1  5  2  3  7  8  4  1 43]\n",
      " [ 0  0  1  5  8 28 33 34 67 15 10 68 13 31  5  2  3]\n",
      " [ 0  0  0  0  0  0  0  0  0  6  1  5  2  3 66  1  9]\n",
      " [ 0  0  0  0  0  0  0  0  0  0 17  1  9  2  3  7  8]\n",
      " [ 0  0  0  0  0  0  0  0  0  5  2  3  1  9  4  1 39]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  5 41  2  3 18  1  9]\n",
      " [ 0  0  0  0  0  0  0  0  0  1  9 17  2  3  7  8 28]\n",
      " [ 0  0  0  0  0  0  0  0  0  1  9  4  1  6  5  2  3]\n",
      " [ 0  0  0  0  6  1  5  2  3  7  8  4  1 29  4 11 30]\n",
      " [ 0  0  0  6  1  5  2  3  7  8  4  1 10 24 27  4 11]\n",
      " [ 0  0  0  0  0  0  0  6  1  5  2  3  7  8  4  1 83]\n",
      " [ 0  0  0  0  0  0  0  0  6  1  5  2  3  7  8  4 60]\n",
      " [ 0  0  0  0  0  0  0  6  1  5  2  3  7  8  4  1 95]]\n",
      "\n",
      "Real and Predicted Values:\n",
      "    Real  Predicted\n",
      "0      1          1\n",
      "1      1          1\n",
      "2      0          0\n",
      "3      1          1\n",
      "4      2          2\n",
      "5      2          2\n",
      "6      2          2\n",
      "7      2          2\n",
      "8      2          2\n",
      "9      2          2\n",
      "10     1          1\n",
      "11     1          1\n",
      "12     0          1\n",
      "13     0          0\n",
      "14     0          0\n"
     ]
    }
   ],
   "source": [
    "evaluation_metrics_transformer = transformer_model.evaluate(X_test, Y_test)\n",
    "\n",
    "predictions = transformer_model.predict(X_test)\n",
    "print(X_test)\n",
    "\n",
    "# Convert one-hot encoded labels to class indices\n",
    "y_true = np.argmax(Y_test, axis=1)\n",
    "\n",
    "# Convert predicted probabilities to class indices\n",
    "y_pred = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Create DataFrame\n",
    "results_df = pd.DataFrame({\"Real\": y_true, \"Predicted\": y_pred})\n",
    "\n",
    "# Display DataFrame\n",
    "print(\"\\nReal and Predicted Values:\")\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy RNN: 0.9333333373069763\n",
      "Accuracy LSTM: 0.800000011920929\n",
      "Accuracy Transformer: 0.9333333373069763\n"
     ]
    }
   ],
   "source": [
    "# Evaluate RNN model\n",
    "rnn_accuracy = rnn_model.evaluate(X_test, Y_test, verbose=0)[1]\n",
    "print(\"Accuracy RNN:\", rnn_accuracy)\n",
    "\n",
    "# Evaluate LSTM model\n",
    "lstm_accuracy = lstm_model.evaluate(X_test, Y_test, verbose=0)[1]\n",
    "print(\"Accuracy LSTM:\", lstm_accuracy)\n",
    "\n",
    "# Evaluate Transformer model\n",
    "transformer_accuracy = transformer_model.evaluate(X_test, Y_test, verbose=0)[1]\n",
    "print(\"Accuracy Transformer:\", transformer_accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model (RNN) with accuracy 0.9333333373069763 has been saved to './savedModels/q9_model.h5'\n"
     ]
    }
   ],
   "source": [
    "# Determine the best model\n",
    "best_model_name, best_model_accuracy = max([('RNN', rnn_accuracy), ('LSTM', lstm_accuracy), ('Transformer', transformer_accuracy)], key=lambda x: x[1])\n",
    "\n",
    "save_path = './savedModels/q9_model.h5'\n",
    "# Save the best model\n",
    "if best_model_name == 'RNN':\n",
    "    rnn_model.save(save_path)\n",
    "elif best_model_name == 'LSTM':\n",
    "    lstm_model.save(save_path)\n",
    "elif best_model_name == 'Transformer':\n",
    "    transformer_model.save(save_path)\n",
    "\n",
    "print(f\"The best model ({best_model_name}) with accuracy {best_model_accuracy} has been saved to '{save_path}'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
