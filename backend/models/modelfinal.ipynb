{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow as keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.regularizers import l2\n",
    "from keras import layers\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Embedding, SimpleRNN, Dense, Dropout, LSTM, Bidirectional, Input\n",
    "from keras.layers import Attention, GlobalAveragePooling1D\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import stanza\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import BatchNormalization, Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*<h3>Load Data<h3>*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_id</th>\n",
       "      <th>answer</th>\n",
       "      <th>grade</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>النبي -صلى الله عليه وسلم- وُلِدَ في سنة الفيل...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>وُلِدَ النبي محمد في العام 572 هجريًا</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>في عام 571، وُلِدَ الرسول -صلى الله عليه وسلم-</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>رؤية النور كانت في عام 572 ميلادي</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>ولد رسول الله صلى الله عليه وسلم في العام العا...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>ولد رسول الله صلى الله عليه وسلم في العام الخا...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>وُلِدَ رسول الله في سنة الفيل المباركة</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>ولد النبي محمد صلى الله عليه وسلم في مكة المكر...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>السنة الفيلية شهدت وُلادة النبي -صلى الله عليه...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>رؤية النور في عام 570 هجريًا</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   question_id                                             answer  grade\n",
       "0            1  النبي -صلى الله عليه وسلم- وُلِدَ في سنة الفيل...      2\n",
       "1            1              وُلِدَ النبي محمد في العام 572 هجريًا      1\n",
       "2            1     في عام 571، وُلِدَ الرسول -صلى الله عليه وسلم-      2\n",
       "3            1                  رؤية النور كانت في عام 572 ميلادي      1\n",
       "4            1  ولد رسول الله صلى الله عليه وسلم في العام العا...      0\n",
       "5            1  ولد رسول الله صلى الله عليه وسلم في العام الخا...      0\n",
       "6            1             وُلِدَ رسول الله في سنة الفيل المباركة      2\n",
       "7            1  ولد النبي محمد صلى الله عليه وسلم في مكة المكر...      1\n",
       "8            1  السنة الفيلية شهدت وُلادة النبي -صلى الله عليه...      2\n",
       "9            1                       رؤية النور في عام 570 هجريًا      1"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read csv file into pandas\n",
    "df = pd.read_csv(\"../datasets/shuffled1.csv\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*<h3>EDA<h3>*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 110 entries, 0 to 109\n",
      "Data columns (total 3 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   question_id  110 non-null    int64 \n",
      " 1   answer       110 non-null    object\n",
      " 2   grade        110 non-null    int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 2.7+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "grade\n",
       "0    27\n",
       "1    41\n",
       "2    42\n",
       "dtype: int64"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('grade').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(data=df, x='grade')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*<h3>Data Cleaning<h3>*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('question_id', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      النبي -صلى الله عليه وسلم- وُلِدَ في سنة الفيل...\n",
      "1                  وُلِدَ النبي محمد في العام 572 هجريًا\n",
      "2         في عام 571، وُلِدَ الرسول -صلى الله عليه وسلم-\n",
      "3                      رؤية النور كانت في عام 572 ميلادي\n",
      "4      ولد رسول الله صلى الله عليه وسلم في العام العا...\n",
      "                             ...                        \n",
      "105     رسول الله صلى الله عليه وسلم ولد في مدينة الطائف\n",
      "106     النبي -صلى الله عليه وسلم- وُلِدَ في مكة المكرمة\n",
      "107    رسول الله صلى الله عليه وسلم ولد في مكة المكرم...\n",
      "108    في سنة الفيل وُلِدَ النبي الكريم -صلى الله علي...\n",
      "109    ولد رسول الله صلى الله عليه وسلم في السابع عشر...\n",
      "Name: answer, Length: 110, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df.drop_duplicates(inplace=True)\n",
    "print(df['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*<h3>Data Pre-Preocessing<h3>*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eda0e0a73349443db15537aa728ee613",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.7.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-09 21:24:38 INFO: Downloading default packages for language: ar (Arabic) ...\n",
      "2024-01-09 21:24:39 INFO: File exists: C:\\Users\\amine\\stanza_resources\\ar\\default.zip\n",
      "2024-01-09 21:24:43 INFO: Finished downloading models and saved to C:\\Users\\amine\\stanza_resources.\n",
      "2024-01-09 21:24:43 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "774a9cdf29d44093b83ae32604efc69e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.7.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-09 21:24:46 INFO: Loading these models for language: ar (Arabic):\n",
      "=============================\n",
      "| Processor | Package       |\n",
      "-----------------------------\n",
      "| tokenize  | padt          |\n",
      "| mwt       | padt          |\n",
      "| pos       | padt_charlm   |\n",
      "| lemma     | padt_nocharlm |\n",
      "| depparse  | padt_charlm   |\n",
      "| ner       | aqmar_charlm  |\n",
      "=============================\n",
      "\n",
      "2024-01-09 21:24:46 INFO: Using device: cpu\n",
      "2024-01-09 21:24:46 INFO: Loading: tokenize\n",
      "2024-01-09 21:24:46 INFO: Loading: mwt\n",
      "2024-01-09 21:24:46 INFO: Loading: pos\n",
      "2024-01-09 21:24:46 INFO: Loading: lemma\n",
      "2024-01-09 21:24:46 INFO: Loading: depparse\n",
      "2024-01-09 21:24:47 INFO: Loading: ner\n",
      "2024-01-09 21:24:47 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "df['grade'] = le.fit_transform(df['grade'])\n",
    "\n",
    "stanza.download('ar')\n",
    "nlp = stanza.Pipeline('ar')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    doc = nlp(text)\n",
    "    tokens = [word.lemma for sent in doc.sentences for word in sent.words if word.upos != 'PUNCT']\n",
    "    return tokens\n",
    "\n",
    "df['answer'] = df['answer'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0 ... 14 17 50]\n",
      " [ 0  0  0 ...  8 31 34]\n",
      " [ 0  0  0 ...  4  1  7]\n",
      " ...\n",
      " [ 0  0  0 ... 12  8 16]\n",
      " [ 0  0  0 ...  4  1  7]\n",
      " [ 0  0  0 ... 28 35 36]]\n",
      "0      [النبي, صَلَّى, الله, عَلَى, هُوَ, وَ, سَلَّم,...\n",
      "1      [وَ, أَل<َاد, النبي, محمد, فِي, عَام, 572, هِج...\n",
      "2      [فِي, عَام, 571, وَ, أَل<َاد, الرسول, صَلَّى, ...\n",
      "3       [رُؤيَة, النور, كَان, فِي, عَام, 572, مِيلَادِي]\n",
      "4      [وَ, لَدّ, رَسُول, الله, صَلَّى, الله, عَلَى, ...\n",
      "                             ...                        \n",
      "105    [رَسُول, الله, صَلَّى, الله, عَلَى, هُوَ, وَ, ...\n",
      "106    [النبي, صَلَّى, الله, عَلَى, هُوَ, وَ, سَلَّم,...\n",
      "107    [رَسُول, الله, صَلَّى, الله, عَلَى, هُوَ, وَ, ...\n",
      "108    [فِي, سَنَة, فِيل, وَ, أَل<َاد, النبي, كَرِيم,...\n",
      "109    [وَ, لَدّ, رَسُول, الله, صَلَّى, الله, عَلَى, ...\n",
      "Name: answer, Length: 110, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Tokenization\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df['answer'])\n",
    "sequences = tokenizer.texts_to_sequences(df['answer'])\n",
    "max_sequence_length = max(len(s) for s in sequences)\n",
    "sequences = pad_sequences(sequences,max_sequence_length)\n",
    "word2idx = tokenizer.word_index\n",
    "vocab_size = len(word2idx) + 1\n",
    "\n",
    "\n",
    "X = pad_sequences(sequences, padding='post', truncating='post', maxlen=max_sequence_length)\n",
    "\n",
    "print(sequences)\n",
    "print(df['answer'])\n",
    "\n",
    "Y = to_categorical(df['grade'], num_classes=3)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.15, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*<h3>build Models<h3>*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*<h4>RNN Model<h4>*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2/2 [==============================] - 4s 634ms/step - loss: 1.4511 - accuracy: 0.2955 - val_loss: 1.1860 - val_accuracy: 0.4091 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 1.2105 - accuracy: 0.3295 - val_loss: 1.2321 - val_accuracy: 0.2727 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 1.2153 - accuracy: 0.4219\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 1.1307 - accuracy: 0.4886 - val_loss: 1.3166 - val_accuracy: 0.2727 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 1.1910 - accuracy: 0.4432 - val_loss: 1.3234 - val_accuracy: 0.2727 - lr: 2.0000e-04\n",
      "Epoch 5/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 1.3151 - accuracy: 0.3438\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.0001.\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 1.2587 - accuracy: 0.3750 - val_loss: 1.3203 - val_accuracy: 0.2727 - lr: 2.0000e-04\n",
      "Epoch 6/100\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 1.1530 - accuracy: 0.4432 - val_loss: 1.3164 - val_accuracy: 0.2727 - lr: 1.0000e-04\n",
      "Epoch 7/100\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 1.1339 - accuracy: 0.4773 - val_loss: 1.3104 - val_accuracy: 0.2727 - lr: 1.0000e-04\n",
      "Epoch 8/100\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 1.2139 - accuracy: 0.4205 - val_loss: 1.3027 - val_accuracy: 0.2727 - lr: 1.0000e-04\n",
      "Epoch 9/100\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 1.2142 - accuracy: 0.4205 - val_loss: 1.2942 - val_accuracy: 0.2727 - lr: 1.0000e-04\n",
      "Epoch 10/100\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 1.2391 - accuracy: 0.4091 - val_loss: 1.2851 - val_accuracy: 0.2727 - lr: 1.0000e-04\n",
      "Epoch 11/100\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 1.0990 - accuracy: 0.4659 - val_loss: 1.2759 - val_accuracy: 0.2727 - lr: 1.0000e-04\n",
      "Epoch 12/100\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 1.1690 - accuracy: 0.4205 - val_loss: 1.2667 - val_accuracy: 0.2727 - lr: 1.0000e-04\n",
      "Epoch 13/100\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 1.0740 - accuracy: 0.5000 - val_loss: 1.2579 - val_accuracy: 0.2727 - lr: 1.0000e-04\n",
      "Epoch 14/100\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 1.0573 - accuracy: 0.4886 - val_loss: 1.2502 - val_accuracy: 0.2727 - lr: 1.0000e-04\n",
      "Epoch 15/100\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 1.1047 - accuracy: 0.5455 - val_loss: 1.2432 - val_accuracy: 0.2727 - lr: 1.0000e-04\n",
      "Epoch 16/100\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 1.0997 - accuracy: 0.4773 - val_loss: 1.2366 - val_accuracy: 0.2727 - lr: 1.0000e-04\n",
      "Epoch 17/100\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 1.0971 - accuracy: 0.5000 - val_loss: 1.2310 - val_accuracy: 0.4545 - lr: 1.0000e-04\n",
      "Epoch 18/100\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 1.1030 - accuracy: 0.5227 - val_loss: 1.2261 - val_accuracy: 0.5455 - lr: 1.0000e-04\n",
      "Epoch 19/100\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 1.1284 - accuracy: 0.4432 - val_loss: 1.2218 - val_accuracy: 0.4091 - lr: 1.0000e-04\n",
      "Epoch 20/100\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 1.0842 - accuracy: 0.5341 - val_loss: 1.2182 - val_accuracy: 0.4091 - lr: 1.0000e-04\n",
      "Epoch 21/100\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 1.0984 - accuracy: 0.4205 - val_loss: 1.2153 - val_accuracy: 0.4091 - lr: 1.0000e-04\n",
      "Epoch 22/100\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 1.1083 - accuracy: 0.4886 - val_loss: 1.2126 - val_accuracy: 0.4091 - lr: 1.0000e-04\n",
      "Epoch 23/100\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 1.0599 - accuracy: 0.5795 - val_loss: 1.2104 - val_accuracy: 0.4091 - lr: 1.0000e-04\n",
      "Epoch 24/100\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 1.0806 - accuracy: 0.5455 - val_loss: 1.2087 - val_accuracy: 0.4091 - lr: 1.0000e-04\n",
      "Epoch 25/100\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 1.0388 - accuracy: 0.6250 - val_loss: 1.2077 - val_accuracy: 0.4091 - lr: 1.0000e-04\n",
      "Epoch 26/100\n",
      "2/2 [==============================] - 0s 155ms/step - loss: 1.0208 - accuracy: 0.5341 - val_loss: 1.2074 - val_accuracy: 0.4545 - lr: 1.0000e-04\n",
      "Epoch 27/100\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.9855 - accuracy: 0.6136 - val_loss: 1.2072 - val_accuracy: 0.5000 - lr: 1.0000e-04\n",
      "Epoch 28/100\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 1.0253 - accuracy: 0.6364 - val_loss: 1.2069 - val_accuracy: 0.5000 - lr: 1.0000e-04\n",
      "Epoch 29/100\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 1.0361 - accuracy: 0.6364 - val_loss: 1.2066 - val_accuracy: 0.6364 - lr: 1.0000e-04\n",
      "Epoch 30/100\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 1.0216 - accuracy: 0.6023 - val_loss: 1.2065 - val_accuracy: 0.5909 - lr: 1.0000e-04\n",
      "Epoch 31/100\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 1.0258 - accuracy: 0.6250 - val_loss: 1.2066 - val_accuracy: 0.5909 - lr: 1.0000e-04\n",
      "Epoch 32/100\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 1.0008 - accuracy: 0.6477 - val_loss: 1.2067 - val_accuracy: 0.5909 - lr: 1.0000e-04\n",
      "Epoch 33/100\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.9866 - accuracy: 0.6477 - val_loss: 1.2071 - val_accuracy: 0.5909 - lr: 1.0000e-04\n",
      "Epoch 34/100\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.9940 - accuracy: 0.6364 - val_loss: 1.2077 - val_accuracy: 0.5455 - lr: 1.0000e-04\n",
      "Epoch 35/100\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.9354 - accuracy: 0.6818 - val_loss: 1.2080 - val_accuracy: 0.5455 - lr: 1.0000e-04\n",
      "Epoch 36/100\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.9774 - accuracy: 0.6477 - val_loss: 1.2081 - val_accuracy: 0.5909 - lr: 1.0000e-04\n",
      "Epoch 37/100\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.9972 - accuracy: 0.6591 - val_loss: 1.2080 - val_accuracy: 0.5909 - lr: 1.0000e-04\n",
      "Epoch 38/100\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.9381 - accuracy: 0.6818 - val_loss: 1.2072 - val_accuracy: 0.5909 - lr: 1.0000e-04\n",
      "Epoch 39/100\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.9002 - accuracy: 0.6932 - val_loss: 1.2063 - val_accuracy: 0.5909 - lr: 1.0000e-04\n",
      "Epoch 40/100\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.9882 - accuracy: 0.6818 - val_loss: 1.2056 - val_accuracy: 0.5000 - lr: 1.0000e-04\n",
      "Epoch 41/100\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.9019 - accuracy: 0.6932 - val_loss: 1.2047 - val_accuracy: 0.5000 - lr: 1.0000e-04\n",
      "Epoch 42/100\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.9089 - accuracy: 0.7159 - val_loss: 1.2037 - val_accuracy: 0.5000 - lr: 1.0000e-04\n",
      "Epoch 43/100\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.9186 - accuracy: 0.6477 - val_loss: 1.2018 - val_accuracy: 0.5000 - lr: 1.0000e-04\n",
      "Epoch 44/100\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.9676 - accuracy: 0.6591 - val_loss: 1.1997 - val_accuracy: 0.5000 - lr: 1.0000e-04\n",
      "Epoch 45/100\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.9235 - accuracy: 0.7273 - val_loss: 1.1976 - val_accuracy: 0.5000 - lr: 1.0000e-04\n",
      "Epoch 46/100\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.8762 - accuracy: 0.7500 - val_loss: 1.1954 - val_accuracy: 0.5000 - lr: 1.0000e-04\n",
      "Epoch 47/100\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.8461 - accuracy: 0.7159 - val_loss: 1.1937 - val_accuracy: 0.5000 - lr: 1.0000e-04\n",
      "Epoch 48/100\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.8684 - accuracy: 0.7386 - val_loss: 1.1930 - val_accuracy: 0.5909 - lr: 1.0000e-04\n",
      "Epoch 49/100\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.9034 - accuracy: 0.6591 - val_loss: 1.1925 - val_accuracy: 0.6364 - lr: 1.0000e-04\n",
      "Epoch 50/100\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.8422 - accuracy: 0.7386 - val_loss: 1.1916 - val_accuracy: 0.5909 - lr: 1.0000e-04\n",
      "Epoch 51/100\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.8740 - accuracy: 0.7159 - val_loss: 1.1901 - val_accuracy: 0.5909 - lr: 1.0000e-04\n",
      "Epoch 52/100\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.8725 - accuracy: 0.6932 - val_loss: 1.1892 - val_accuracy: 0.5909 - lr: 1.0000e-04\n",
      "Epoch 53/100\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.8071 - accuracy: 0.7955 - val_loss: 1.1885 - val_accuracy: 0.5909 - lr: 1.0000e-04\n",
      "Epoch 54/100\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.7983 - accuracy: 0.8182 - val_loss: 1.1887 - val_accuracy: 0.6364 - lr: 1.0000e-04\n",
      "Epoch 55/100\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.7991 - accuracy: 0.7841 - val_loss: 1.1873 - val_accuracy: 0.5909 - lr: 1.0000e-04\n",
      "Epoch 56/100\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.8138 - accuracy: 0.7273 - val_loss: 1.1847 - val_accuracy: 0.6364 - lr: 1.0000e-04\n",
      "Epoch 57/100\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.7492 - accuracy: 0.8068 - val_loss: 1.1820 - val_accuracy: 0.6364 - lr: 1.0000e-04\n",
      "Epoch 58/100\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.8028 - accuracy: 0.7614 - val_loss: 1.1807 - val_accuracy: 0.6364 - lr: 1.0000e-04\n",
      "Epoch 59/100\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.7548 - accuracy: 0.7727 - val_loss: 1.1792 - val_accuracy: 0.5909 - lr: 1.0000e-04\n",
      "Epoch 60/100\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.7639 - accuracy: 0.7727 - val_loss: 1.1763 - val_accuracy: 0.6364 - lr: 1.0000e-04\n",
      "Epoch 61/100\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.7579 - accuracy: 0.7955 - val_loss: 1.1731 - val_accuracy: 0.6364 - lr: 1.0000e-04\n",
      "Epoch 62/100\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.7604 - accuracy: 0.7841 - val_loss: 1.1694 - val_accuracy: 0.6364 - lr: 1.0000e-04\n",
      "Epoch 63/100\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.7078 - accuracy: 0.7727 - val_loss: 1.1674 - val_accuracy: 0.6364 - lr: 1.0000e-04\n",
      "Epoch 64/100\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.7004 - accuracy: 0.8068 - val_loss: 1.1647 - val_accuracy: 0.6364 - lr: 1.0000e-04\n",
      "Epoch 65/100\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.7008 - accuracy: 0.8295 - val_loss: 1.1620 - val_accuracy: 0.6364 - lr: 1.0000e-04\n",
      "Epoch 66/100\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.6859 - accuracy: 0.8523 - val_loss: 1.1625 - val_accuracy: 0.6364 - lr: 1.0000e-04\n",
      "Epoch 67/100\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.6974 - accuracy: 0.8182 - val_loss: 1.1589 - val_accuracy: 0.6364 - lr: 1.0000e-04\n",
      "Epoch 68/100\n",
      "2/2 [==============================] - 0s 107ms/step - loss: 0.7086 - accuracy: 0.8295 - val_loss: 1.1542 - val_accuracy: 0.6364 - lr: 1.0000e-04\n",
      "Epoch 69/100\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.6917 - accuracy: 0.8068 - val_loss: 1.1584 - val_accuracy: 0.6364 - lr: 1.0000e-04\n",
      "Epoch 70/100\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.6916 - accuracy: 0.7955 - val_loss: 1.1563 - val_accuracy: 0.6364 - lr: 1.0000e-04\n",
      "Epoch 71/100\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.6729 - accuracy: 0.8636 - val_loss: 1.1469 - val_accuracy: 0.6364 - lr: 1.0000e-04\n",
      "Epoch 72/100\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.6585 - accuracy: 0.8295 - val_loss: 1.1446 - val_accuracy: 0.6364 - lr: 1.0000e-04\n",
      "Epoch 73/100\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.6338 - accuracy: 0.8295 - val_loss: 1.1459 - val_accuracy: 0.6364 - lr: 1.0000e-04\n",
      "Epoch 74/100\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.6592 - accuracy: 0.8295 - val_loss: 1.1464 - val_accuracy: 0.6364 - lr: 1.0000e-04\n",
      "Epoch 75/100\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.6317 - accuracy: 0.8182 - val_loss: 1.1409 - val_accuracy: 0.6364 - lr: 1.0000e-04\n",
      "Epoch 76/100\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.6163 - accuracy: 0.8636 - val_loss: 1.1325 - val_accuracy: 0.6364 - lr: 1.0000e-04\n",
      "Epoch 77/100\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.6398 - accuracy: 0.8295 - val_loss: 1.1281 - val_accuracy: 0.6364 - lr: 1.0000e-04\n",
      "Epoch 78/100\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.6488 - accuracy: 0.8523 - val_loss: 1.1296 - val_accuracy: 0.6364 - lr: 1.0000e-04\n",
      "Epoch 79/100\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.6191 - accuracy: 0.8409 - val_loss: 1.1255 - val_accuracy: 0.6364 - lr: 1.0000e-04\n",
      "Epoch 80/100\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.5845 - accuracy: 0.8750 - val_loss: 1.1206 - val_accuracy: 0.6364 - lr: 1.0000e-04\n",
      "Epoch 81/100\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.6106 - accuracy: 0.8409 - val_loss: 1.1207 - val_accuracy: 0.6364 - lr: 1.0000e-04\n",
      "Epoch 82/100\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.5771 - accuracy: 0.8750 - val_loss: 1.1203 - val_accuracy: 0.6364 - lr: 1.0000e-04\n",
      "Epoch 83/100\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.5871 - accuracy: 0.8523 - val_loss: 1.1167 - val_accuracy: 0.6364 - lr: 1.0000e-04\n",
      "Epoch 84/100\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.5683 - accuracy: 0.8523 - val_loss: 1.1133 - val_accuracy: 0.6364 - lr: 1.0000e-04\n",
      "Epoch 85/100\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.5801 - accuracy: 0.8523 - val_loss: 1.1104 - val_accuracy: 0.6364 - lr: 1.0000e-04\n",
      "Epoch 86/100\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.5805 - accuracy: 0.8750 - val_loss: 1.1039 - val_accuracy: 0.6364 - lr: 1.0000e-04\n",
      "Epoch 87/100\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.5579 - accuracy: 0.8523 - val_loss: 1.1006 - val_accuracy: 0.6364 - lr: 1.0000e-04\n",
      "Epoch 88/100\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.5660 - accuracy: 0.9091 - val_loss: 1.1039 - val_accuracy: 0.6364 - lr: 1.0000e-04\n",
      "Epoch 89/100\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.5216 - accuracy: 0.9205 - val_loss: 1.1039 - val_accuracy: 0.6364 - lr: 1.0000e-04\n",
      "Epoch 90/100\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.5399 - accuracy: 0.8750 - val_loss: 1.0986 - val_accuracy: 0.6364 - lr: 1.0000e-04\n",
      "Epoch 91/100\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.5181 - accuracy: 0.8750 - val_loss: 1.0859 - val_accuracy: 0.6364 - lr: 1.0000e-04\n",
      "Epoch 92/100\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.5516 - accuracy: 0.8864 - val_loss: 1.0852 - val_accuracy: 0.6364 - lr: 1.0000e-04\n",
      "Epoch 93/100\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.5514 - accuracy: 0.8409 - val_loss: 1.0910 - val_accuracy: 0.6364 - lr: 1.0000e-04\n",
      "Epoch 94/100\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.5336 - accuracy: 0.8864 - val_loss: 1.0814 - val_accuracy: 0.6364 - lr: 1.0000e-04\n",
      "Epoch 95/100\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.5298 - accuracy: 0.9091 - val_loss: 1.0756 - val_accuracy: 0.6364 - lr: 1.0000e-04\n",
      "Epoch 96/100\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.5294 - accuracy: 0.8977 - val_loss: 1.0784 - val_accuracy: 0.6364 - lr: 1.0000e-04\n",
      "Epoch 97/100\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.5230 - accuracy: 0.8750 - val_loss: 1.0774 - val_accuracy: 0.6364 - lr: 1.0000e-04\n",
      "Epoch 98/100\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.5039 - accuracy: 0.9091 - val_loss: 1.0704 - val_accuracy: 0.6364 - lr: 1.0000e-04\n",
      "Epoch 99/100\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.5489 - accuracy: 0.8409 - val_loss: 1.0622 - val_accuracy: 0.6364 - lr: 1.0000e-04\n",
      "Epoch 100/100\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.4905 - accuracy: 0.8864 - val_loss: 1.0610 - val_accuracy: 0.6364 - lr: 1.0000e-04\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.8900 - accuracy: 0.7647\n",
      "Evaluation Metrics for RNN:\n",
      "loss: 0.8899629712104797\n",
      "accuracy: 0.7647058963775635\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "\n",
    "X_train_rnn, X_test_rnn, y_train_rnn, y_test_rnn = train_test_split(X, Y, test_size=0.2)\n",
    "\n",
    "def RNN_model(vocab_size, sequence_length):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=vocab_size, output_dim=128, input_length=sequence_length))\n",
    "    model.add(SimpleRNN(units=128, activation='relu', return_sequences=True))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(SimpleRNN(units=64, activation='sigmoid'))\n",
    "    model.add(Dense(256, activation='sigmoid', kernel_regularizer=l2(0.001)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "    optimizer = Adam(learning_rate=0.001)\n",
    "    \n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=0.0001, verbose=1)\n",
    "#EMBEDDING_DIM = 110\n",
    "rnn_model = RNN_model(vocab_size, max_sequence_length)\n",
    "early_stopping_rnn = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "history_updated_rnn = rnn_model.fit(X_train_rnn, y_train_rnn, validation_data=(X_test_rnn, y_test_rnn), epochs=100, batch_size=64, callbacks=[reduce_lr])\n",
    "\n",
    "# Evaluate the RNN model\n",
    "evaluation_metrics_updated_rnn = rnn_model.evaluate(X_test, Y_test)\n",
    "print(\"Evaluation Metrics for RNN:\")\n",
    "for metric_name, metric_value in zip(rnn_model.metrics_names, evaluation_metrics_updated_rnn):\n",
    "    print(f\"{metric_name}: {metric_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*<h4>LSTM Model<h4>*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2/2 [==============================] - 5s 672ms/step - loss: 1.1992 - accuracy: 0.3068 - val_loss: 1.1971 - val_accuracy: 0.5455\n",
      "Epoch 2/100\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 1.1527 - accuracy: 0.4886 - val_loss: 1.1948 - val_accuracy: 0.3182\n",
      "Epoch 3/100\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 1.1213 - accuracy: 0.5114 - val_loss: 1.1911 - val_accuracy: 0.4091\n",
      "Epoch 4/100\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 1.0572 - accuracy: 0.5227 - val_loss: 1.1866 - val_accuracy: 0.4091\n",
      "Epoch 5/100\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 1.0020 - accuracy: 0.5909 - val_loss: 1.1826 - val_accuracy: 0.4091\n",
      "Epoch 6/100\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 0.9383 - accuracy: 0.6591 - val_loss: 1.1795 - val_accuracy: 0.4091\n",
      "Epoch 7/100\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 0.8107 - accuracy: 0.7159 - val_loss: 1.1755 - val_accuracy: 0.4091\n",
      "Epoch 8/100\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 0.7410 - accuracy: 0.7386 - val_loss: 1.1698 - val_accuracy: 0.4545\n",
      "Epoch 9/100\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 0.6161 - accuracy: 0.7841 - val_loss: 1.1632 - val_accuracy: 0.4545\n",
      "Epoch 10/100\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.5141 - accuracy: 0.8295 - val_loss: 1.1558 - val_accuracy: 0.4091\n",
      "Epoch 11/100\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.5014 - accuracy: 0.8182 - val_loss: 1.1511 - val_accuracy: 0.5909\n",
      "Epoch 12/100\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.5428 - accuracy: 0.8636 - val_loss: 1.1485 - val_accuracy: 0.5909\n",
      "Epoch 13/100\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 0.4188 - accuracy: 0.8864 - val_loss: 1.1445 - val_accuracy: 0.4091\n",
      "Epoch 14/100\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 0.3817 - accuracy: 0.8864 - val_loss: 1.1439 - val_accuracy: 0.4091\n",
      "Epoch 15/100\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.2978 - accuracy: 0.9318 - val_loss: 1.1437 - val_accuracy: 0.4091\n",
      "Epoch 16/100\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.2798 - accuracy: 0.9545 - val_loss: 1.1420 - val_accuracy: 0.5000\n",
      "Epoch 17/100\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 0.2629 - accuracy: 0.9659 - val_loss: 1.1386 - val_accuracy: 0.5000\n",
      "Epoch 18/100\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 0.2571 - accuracy: 0.9432 - val_loss: 1.1342 - val_accuracy: 0.4545\n",
      "Epoch 19/100\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 0.2110 - accuracy: 0.9659 - val_loss: 1.1291 - val_accuracy: 0.4545\n",
      "Epoch 20/100\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 0.2170 - accuracy: 0.9545 - val_loss: 1.1224 - val_accuracy: 0.4091\n",
      "Epoch 21/100\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 0.1873 - accuracy: 0.9773 - val_loss: 1.1168 - val_accuracy: 0.4091\n",
      "Epoch 22/100\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.1842 - accuracy: 0.9659 - val_loss: 1.1150 - val_accuracy: 0.4545\n",
      "Epoch 23/100\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 0.1499 - accuracy: 0.9773 - val_loss: 1.1152 - val_accuracy: 0.6818\n",
      "Epoch 24/100\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.1535 - accuracy: 0.9659 - val_loss: 1.1141 - val_accuracy: 0.6364\n",
      "Epoch 25/100\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.1443 - accuracy: 0.9773 - val_loss: 1.1094 - val_accuracy: 0.6364\n",
      "Epoch 26/100\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.1434 - accuracy: 0.9773 - val_loss: 1.1056 - val_accuracy: 0.6364\n",
      "Epoch 27/100\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.1152 - accuracy: 0.9886 - val_loss: 1.1029 - val_accuracy: 0.6364\n",
      "Epoch 28/100\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 0.1215 - accuracy: 0.9886 - val_loss: 1.0999 - val_accuracy: 0.6364\n",
      "Epoch 29/100\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 0.1451 - accuracy: 0.9886 - val_loss: 1.1006 - val_accuracy: 0.6818\n",
      "Epoch 30/100\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 0.1580 - accuracy: 0.9886 - val_loss: 1.1017 - val_accuracy: 0.7727\n",
      "Epoch 31/100\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.3871 - accuracy: 0.9432 - val_loss: 1.0921 - val_accuracy: 0.6818\n",
      "Epoch 32/100\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 0.3190 - accuracy: 0.9545 - val_loss: 1.0961 - val_accuracy: 0.6818\n",
      "Epoch 33/100\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 0.1914 - accuracy: 0.9545 - val_loss: 1.1089 - val_accuracy: 0.6818\n",
      "Epoch 34/100\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 0.1235 - accuracy: 0.9886 - val_loss: 1.1169 - val_accuracy: 0.5000\n",
      "Epoch 35/100\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 0.1671 - accuracy: 0.9773 - val_loss: 1.1196 - val_accuracy: 0.5455\n",
      "Epoch 36/100\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 0.1786 - accuracy: 0.9659 - val_loss: 1.1190 - val_accuracy: 0.5000\n",
      "Epoch 37/100\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.1864 - accuracy: 0.9545 - val_loss: 1.1164 - val_accuracy: 0.5455\n",
      "Epoch 38/100\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.1624 - accuracy: 0.9773 - val_loss: 1.1129 - val_accuracy: 0.5909\n",
      "Epoch 39/100\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.1518 - accuracy: 0.9773 - val_loss: 1.1094 - val_accuracy: 0.7273\n",
      "Epoch 40/100\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 0.1579 - accuracy: 0.9659 - val_loss: 1.1057 - val_accuracy: 0.7727\n",
      "Epoch 41/100\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.1515 - accuracy: 0.9659 - val_loss: 1.1023 - val_accuracy: 0.6818\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 1.0393 - accuracy: 0.8824\n",
      "Evaluation Metrics for LSTM:\n",
      "loss: 1.0393486022949219\n",
      "accuracy: 0.8823529481887817\n"
     ]
    }
   ],
   "source": [
    "def LSTM_model(vocab_size, sequence_length):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=vocab_size, output_dim=128, input_length=sequence_length))\n",
    "    model.add(LSTM(units=128, activation='relu', return_sequences=True))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LSTM(units=64, activation='relu'))\n",
    "    model.add(Dense(256, activation='relu', kernel_regularizer=l2(0.001)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "    optimizer = Adam(learning_rate=0.001)\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "#EMBEDDING_DIM = 110\n",
    "lstm_model = LSTM_model(vocab_size, max_sequence_length)\n",
    "early_stopping_rnn = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "history_updated_rnn = lstm_model.fit(X_train_rnn, y_train_rnn, validation_data=(X_test_rnn, y_test_rnn), epochs=100, batch_size=64, callbacks=[early_stopping_rnn])\n",
    "\n",
    "\n",
    "# Evaluate the lstm model\n",
    "evaluation_metrics_updated_lstm = lstm_model.evaluate(X_test, Y_test)\n",
    "print(\"Evaluation Metrics for LSTM:\")\n",
    "for metric_name, metric_value in zip(lstm_model.metrics_names, evaluation_metrics_updated_lstm):\n",
    "    print(f\"{metric_name}: {metric_value}\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*<h4>TRANSFORMER Model<h4>*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, density, rate=0.1, l2_reg=0.01):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            layers.Dense(density, activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(l2_reg)),\n",
    "            layers.Dense(embed_dim, kernel_regularizer=tf.keras.regularizers.l2(l2_reg)),\n",
    "        ])\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "\n",
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions\n",
    "\n",
    "def build_transformer_model(maxlen, vocab_size, embed_dim, num_heads, density, transformer_units, mlp_units, dropout_rate, num_classes):\n",
    "    inputs = layers.Input(shape=(maxlen,))\n",
    "    embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
    "    x = embedding_layer(inputs)\n",
    "    for _ in range(transformer_units):\n",
    "        x = TransformerBlock(embed_dim, num_heads, density, rate=dropout_rate)(x)\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    for dim in mlp_units:\n",
    "        x = layers.Dense(dim, activation=\"relu\")(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "    outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = max_sequence_length\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "embed_dim = 120\n",
    "num_heads = 2\n",
    "density = 3\n",
    "transformer_units = 4\n",
    "mlp_units = [128]\n",
    "dropout_rate = 0.5\n",
    "num_classes = len(df['grade'].unique())\n",
    "\n",
    "transformer_model = build_transformer_model(maxlen, vocab_size, embed_dim, num_heads, density, transformer_units, mlp_units, dropout_rate, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "3/3 [==============================] - 14s 709ms/step - loss: 1.6633 - accuracy: 0.3656 - val_loss: 1.5926 - val_accuracy: 0.2941\n",
      "Epoch 2/30\n",
      "3/3 [==============================] - 0s 102ms/step - loss: 1.6263 - accuracy: 0.3656 - val_loss: 1.5484 - val_accuracy: 0.4118\n",
      "Epoch 3/30\n",
      "3/3 [==============================] - 0s 81ms/step - loss: 1.6045 - accuracy: 0.3871 - val_loss: 1.4988 - val_accuracy: 0.4118\n",
      "Epoch 4/30\n",
      "3/3 [==============================] - 0s 101ms/step - loss: 1.6122 - accuracy: 0.3441 - val_loss: 1.6155 - val_accuracy: 0.2941\n",
      "Epoch 5/30\n",
      "3/3 [==============================] - 0s 83ms/step - loss: 1.5242 - accuracy: 0.4086 - val_loss: 1.3933 - val_accuracy: 0.4706\n",
      "Epoch 6/30\n",
      "3/3 [==============================] - 0s 143ms/step - loss: 1.3555 - accuracy: 0.5806 - val_loss: 1.4090 - val_accuracy: 0.5294\n",
      "Epoch 7/30\n",
      "3/3 [==============================] - 0s 105ms/step - loss: 1.2691 - accuracy: 0.5699 - val_loss: 1.4024 - val_accuracy: 0.4706\n",
      "Epoch 8/30\n",
      "3/3 [==============================] - 0s 79ms/step - loss: 1.2298 - accuracy: 0.6452 - val_loss: 1.2529 - val_accuracy: 0.6471\n",
      "Epoch 9/30\n",
      "3/3 [==============================] - 0s 81ms/step - loss: 1.0744 - accuracy: 0.7419 - val_loss: 1.2669 - val_accuracy: 0.6471\n",
      "Epoch 10/30\n",
      "3/3 [==============================] - 0s 113ms/step - loss: 0.9962 - accuracy: 0.7097 - val_loss: 1.2758 - val_accuracy: 0.7059\n",
      "Epoch 11/30\n",
      "3/3 [==============================] - 0s 88ms/step - loss: 0.9704 - accuracy: 0.7312 - val_loss: 1.3633 - val_accuracy: 0.6471\n",
      "Epoch 12/30\n",
      "3/3 [==============================] - 0s 84ms/step - loss: 0.9343 - accuracy: 0.7634 - val_loss: 1.2390 - val_accuracy: 0.7647\n",
      "Epoch 13/30\n",
      "3/3 [==============================] - 0s 85ms/step - loss: 0.8236 - accuracy: 0.8172 - val_loss: 1.3113 - val_accuracy: 0.7059\n",
      "Epoch 14/30\n",
      "3/3 [==============================] - 0s 90ms/step - loss: 0.8228 - accuracy: 0.7742 - val_loss: 1.2297 - val_accuracy: 0.8235\n",
      "Epoch 15/30\n",
      "3/3 [==============================] - 0s 92ms/step - loss: 0.7785 - accuracy: 0.8280 - val_loss: 1.2267 - val_accuracy: 0.7059\n",
      "Epoch 16/30\n",
      "3/3 [==============================] - 0s 87ms/step - loss: 0.6792 - accuracy: 0.8710 - val_loss: 1.1789 - val_accuracy: 0.7647\n",
      "Epoch 17/30\n",
      "3/3 [==============================] - 0s 118ms/step - loss: 0.6297 - accuracy: 0.9355 - val_loss: 1.1892 - val_accuracy: 0.7647\n",
      "Epoch 18/30\n",
      "3/3 [==============================] - 0s 87ms/step - loss: 0.5758 - accuracy: 0.9355 - val_loss: 1.2045 - val_accuracy: 0.7647\n",
      "Epoch 19/30\n",
      "3/3 [==============================] - 0s 87ms/step - loss: 0.5094 - accuracy: 0.9355 - val_loss: 1.3226 - val_accuracy: 0.7647\n",
      "Epoch 20/30\n",
      "3/3 [==============================] - 0s 87ms/step - loss: 0.4840 - accuracy: 0.9462 - val_loss: 1.2193 - val_accuracy: 0.8235\n",
      "Epoch 21/30\n",
      "3/3 [==============================] - 0s 91ms/step - loss: 0.4585 - accuracy: 0.9462 - val_loss: 1.3416 - val_accuracy: 0.7647\n",
      "Epoch 22/30\n",
      "3/3 [==============================] - 0s 90ms/step - loss: 0.4120 - accuracy: 0.9677 - val_loss: 1.4051 - val_accuracy: 0.8235\n",
      "Epoch 23/30\n",
      "3/3 [==============================] - 0s 91ms/step - loss: 0.4033 - accuracy: 0.9785 - val_loss: 1.5068 - val_accuracy: 0.8235\n",
      "Epoch 24/30\n",
      "3/3 [==============================] - 0s 95ms/step - loss: 0.3662 - accuracy: 0.9785 - val_loss: 1.4861 - val_accuracy: 0.8235\n",
      "Epoch 25/30\n",
      "3/3 [==============================] - 0s 93ms/step - loss: 0.3531 - accuracy: 0.9677 - val_loss: 1.4930 - val_accuracy: 0.7647\n",
      "Epoch 26/30\n",
      "3/3 [==============================] - 0s 89ms/step - loss: 0.3431 - accuracy: 0.9892 - val_loss: 1.5359 - val_accuracy: 0.8235\n",
      "Epoch 27/30\n",
      "3/3 [==============================] - 0s 91ms/step - loss: 0.3346 - accuracy: 0.9785 - val_loss: 1.5960 - val_accuracy: 0.8235\n",
      "Epoch 28/30\n",
      "3/3 [==============================] - 0s 147ms/step - loss: 0.3850 - accuracy: 0.9570 - val_loss: 1.7441 - val_accuracy: 0.7647\n",
      "Epoch 29/30\n",
      "3/3 [==============================] - 0s 88ms/step - loss: 0.3108 - accuracy: 0.9892 - val_loss: 1.7217 - val_accuracy: 0.8235\n",
      "Epoch 30/30\n",
      "3/3 [==============================] - 0s 93ms/step - loss: 0.3721 - accuracy: 0.9570 - val_loss: 1.5702 - val_accuracy: 0.7647\n",
      "Epoch 1/30\n",
      "3/3 [==============================] - 0s 115ms/step - loss: 0.3901 - accuracy: 0.9677 - val_loss: 1.7307 - val_accuracy: 0.7647\n",
      "Epoch 2/30\n",
      "3/3 [==============================] - 0s 88ms/step - loss: 0.3638 - accuracy: 0.9462 - val_loss: 1.2601 - val_accuracy: 0.8235\n",
      "Epoch 3/30\n",
      "3/3 [==============================] - 0s 88ms/step - loss: 0.3393 - accuracy: 0.9677 - val_loss: 1.6910 - val_accuracy: 0.7647\n",
      "Epoch 4/30\n",
      "3/3 [==============================] - 0s 87ms/step - loss: 0.3579 - accuracy: 0.9355 - val_loss: 1.6386 - val_accuracy: 0.8235\n",
      "Epoch 5/30\n",
      "3/3 [==============================] - 0s 87ms/step - loss: 0.3305 - accuracy: 0.9892 - val_loss: 1.2484 - val_accuracy: 0.8235\n",
      "Epoch 6/30\n",
      "3/3 [==============================] - 0s 100ms/step - loss: 0.2674 - accuracy: 0.9892 - val_loss: 1.0814 - val_accuracy: 0.8824\n",
      "Epoch 7/30\n",
      "3/3 [==============================] - 0s 92ms/step - loss: 0.3002 - accuracy: 0.9677 - val_loss: 0.9455 - val_accuracy: 0.8824\n",
      "Epoch 8/30\n",
      "3/3 [==============================] - 0s 87ms/step - loss: 0.3184 - accuracy: 0.9785 - val_loss: 1.0389 - val_accuracy: 0.8235\n",
      "Epoch 9/30\n",
      "3/3 [==============================] - 0s 115ms/step - loss: 0.2888 - accuracy: 0.9677 - val_loss: 1.1821 - val_accuracy: 0.8235\n",
      "Epoch 10/30\n",
      "3/3 [==============================] - 0s 87ms/step - loss: 0.3200 - accuracy: 0.9677 - val_loss: 1.2414 - val_accuracy: 0.8235\n",
      "Epoch 11/30\n",
      "3/3 [==============================] - 0s 89ms/step - loss: 0.3272 - accuracy: 0.9677 - val_loss: 1.1790 - val_accuracy: 0.8235\n",
      "Epoch 12/30\n",
      "3/3 [==============================] - 0s 88ms/step - loss: 0.2655 - accuracy: 0.9785 - val_loss: 1.2578 - val_accuracy: 0.8235\n",
      "Epoch 13/30\n",
      "3/3 [==============================] - 0s 88ms/step - loss: 0.2609 - accuracy: 0.9892 - val_loss: 1.4738 - val_accuracy: 0.8235\n",
      "Epoch 14/30\n",
      "3/3 [==============================] - 0s 88ms/step - loss: 0.2996 - accuracy: 0.9677 - val_loss: 1.6103 - val_accuracy: 0.7647\n",
      "Epoch 15/30\n",
      "3/3 [==============================] - 0s 92ms/step - loss: 0.3259 - accuracy: 0.9570 - val_loss: 1.4625 - val_accuracy: 0.8235\n",
      "Epoch 16/30\n",
      "3/3 [==============================] - 0s 86ms/step - loss: 0.3043 - accuracy: 0.9355 - val_loss: 1.4556 - val_accuracy: 0.7647\n",
      "Epoch 17/30\n",
      "3/3 [==============================] - 0s 86ms/step - loss: 0.2764 - accuracy: 0.9677 - val_loss: 1.7098 - val_accuracy: 0.7647\n",
      "Epoch 18/30\n",
      "3/3 [==============================] - 0s 91ms/step - loss: 0.2237 - accuracy: 0.9892 - val_loss: 2.1829 - val_accuracy: 0.7059\n",
      "Epoch 19/30\n",
      "3/3 [==============================] - 0s 86ms/step - loss: 0.2783 - accuracy: 0.9677 - val_loss: 2.3196 - val_accuracy: 0.7059\n",
      "Epoch 20/30\n",
      "3/3 [==============================] - 0s 88ms/step - loss: 0.2104 - accuracy: 0.9785 - val_loss: 2.3594 - val_accuracy: 0.7059\n",
      "Epoch 21/30\n",
      "3/3 [==============================] - 0s 120ms/step - loss: 0.2258 - accuracy: 0.9892 - val_loss: 2.4116 - val_accuracy: 0.7059\n",
      "Epoch 22/30\n",
      "3/3 [==============================] - 0s 91ms/step - loss: 0.2850 - accuracy: 0.9677 - val_loss: 2.1315 - val_accuracy: 0.7059\n",
      "Epoch 23/30\n",
      "3/3 [==============================] - 0s 91ms/step - loss: 0.1843 - accuracy: 0.9892 - val_loss: 2.1438 - val_accuracy: 0.7647\n",
      "Epoch 24/30\n",
      "3/3 [==============================] - 0s 88ms/step - loss: 0.1789 - accuracy: 1.0000 - val_loss: 2.2226 - val_accuracy: 0.7059\n",
      "Epoch 25/30\n",
      "3/3 [==============================] - 0s 87ms/step - loss: 0.1953 - accuracy: 0.9892 - val_loss: 2.1909 - val_accuracy: 0.7647\n",
      "Epoch 26/30\n",
      "3/3 [==============================] - 0s 86ms/step - loss: 0.1852 - accuracy: 1.0000 - val_loss: 2.1436 - val_accuracy: 0.7647\n",
      "Epoch 27/30\n",
      "3/3 [==============================] - 0s 87ms/step - loss: 0.1955 - accuracy: 0.9892 - val_loss: 2.0269 - val_accuracy: 0.7647\n",
      "Epoch 28/30\n",
      "3/3 [==============================] - 0s 87ms/step - loss: 0.1768 - accuracy: 0.9892 - val_loss: 1.8833 - val_accuracy: 0.7647\n",
      "Epoch 29/30\n",
      "3/3 [==============================] - 0s 89ms/step - loss: 0.1628 - accuracy: 0.9892 - val_loss: 1.7481 - val_accuracy: 0.8235\n",
      "Epoch 30/30\n",
      "3/3 [==============================] - 0s 86ms/step - loss: 0.1571 - accuracy: 0.9892 - val_loss: 1.6596 - val_accuracy: 0.8235\n"
     ]
    }
   ],
   "source": [
    "transformer_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "transformer_model.fit(X_train, Y_train, epochs=30, batch_size=32, validation_data=(X_test, Y_test))\n",
    "history = transformer_model.fit(X_train, Y_train, epochs=30, batch_size=32, validation_data=(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 80ms/step - loss: 1.6596 - accuracy: 0.8235\n",
      "Evaluation Metrics Transformer:\n",
      "loss: 1.659584403038025\n",
      "accuracy: 0.8235294222831726\n"
     ]
    }
   ],
   "source": [
    "evaluation_metrics_transformer = transformer_model.evaluate(X_test, Y_test)\n",
    "print(\"Evaluation Metrics Transformer:\")\n",
    "for metric_name, metric_value in zip(transformer_model.metrics_names, evaluation_metrics_transformer):\n",
    "    print(f\"{metric_name}: {metric_value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 46ms/step - loss: 1.6596 - accuracy: 0.8235\n",
      "1/1 [==============================] - 1s 794ms/step\n",
      "[[ 0  0  0  0  0  0  0  0  0  1  9 19  5  2  6  4  1  7  3  8 21 49]\n",
      " [ 0  0  0  0  0  0  0  0  0 29 19  5  2  6  4  1  7 37  3 14 15 22]\n",
      " [ 0  0  0  0  0  1  9 10  2  5  2  6  4  1  7  3  8 41 28 42  4 43]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  1 18 40  5  2  6  4  1  7  3  8 16]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  1 18 13  3  8 55 74 14 15 75]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 32 30  3  8 16 27]\n",
      " [ 1 47 13  5  2  6  4  1  7  3 53 25 28 54 35 36  3 63 64 28  8 17]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  3 22  1 47 13]\n",
      " [ 0  0  0  0  0  0 10  2  5  2  6  4  1  7  1  9  3 59 25 28 35 36]\n",
      " [ 0  0  0  0  0  1  9 13 23  5  2  6  4  1  7  3 11 12  3  8 21 27]\n",
      " [ 0  0  0  0  0  0  1  9 13 23  5  2  6  4  1  7  3 11 12  3  8 17]\n",
      " [ 0  0  0  0  0  0  1  9 10  2  5  2  6  4  1  7  3 11 12  3  8 16]\n",
      " [ 0  0  0  0  0  0  0  0  0 13  5  2  6  4  1  7  1 26  3 14 17 50]\n",
      " [ 0  0  0  0  0  0  0 10  2  5  2  6  4  1  7  1  9  3  8 16 20 29]\n",
      " [ 0  0  0  0  0  0  0  0  0  0 13  5  2  6  4  1  7  1 26  3 46 27]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  1  9 23  5  2  6  4  1 38  3  8 22]\n",
      " [ 0  0  0  0  0  0  0  0 10  2  5  2  6  4  1  7  1  9  3  8 21 49]]\n",
      "\n",
      "Real and Predicted Values:\n",
      "    Real  Predicted\n",
      "0      1          1\n",
      "1      2          2\n",
      "2      0          0\n",
      "3      1          1\n",
      "4      1          2\n",
      "5      1          1\n",
      "6      2          2\n",
      "7      2          2\n",
      "8      0          2\n",
      "9      1          1\n",
      "10     2          1\n",
      "11     1          1\n",
      "12     2          2\n",
      "13     1          1\n",
      "14     0          0\n",
      "15     2          2\n",
      "16     1          1\n"
     ]
    }
   ],
   "source": [
    "evaluation_metrics_transformer = transformer_model.evaluate(X_test, Y_test)\n",
    "\n",
    "predictions = transformer_model.predict(X_test)\n",
    "print(X_test)\n",
    "\n",
    "# Convert one-hot encoded labels to class indices\n",
    "y_true = np.argmax(Y_test, axis=1)\n",
    "\n",
    "# Convert predicted probabilities to class indices\n",
    "y_pred = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Create DataFrame\n",
    "results_df = pd.DataFrame({\"Real\": y_true, \"Predicted\": y_pred})\n",
    "\n",
    "# Display DataFrame\n",
    "print(\"\\nReal and Predicted Values:\")\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy RNN: 0.7647058963775635\n",
      "Accuracy LSTM: 0.8823529481887817\n",
      "Accuracy Transformer: 0.8235294222831726\n"
     ]
    }
   ],
   "source": [
    "# Evaluate RNN model\n",
    "rnn_accuracy = rnn_model.evaluate(X_test, Y_test, verbose=0)[1]\n",
    "print(\"Accuracy RNN:\", rnn_accuracy)\n",
    "\n",
    "# Evaluate LSTM model\n",
    "lstm_accuracy = lstm_model.evaluate(X_test, Y_test, verbose=0)[1]\n",
    "print(\"Accuracy LSTM:\", lstm_accuracy)\n",
    "\n",
    "# Evaluate Transformer model\n",
    "transformer_accuracy = transformer_model.evaluate(X_test, Y_test, verbose=0)[1]\n",
    "print(\"Accuracy Transformer:\", transformer_accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model (LSTM) with accuracy 0.8823529481887817 has been saved to './savedModels/q1_model.h5'\n"
     ]
    }
   ],
   "source": [
    "# Determine the best model\n",
    "best_model_name, best_model_accuracy = max([('RNN', rnn_accuracy), ('LSTM', lstm_accuracy), ('Transformer', transformer_accuracy)], key=lambda x: x[1])\n",
    "\n",
    "save_path = './savedModels/q1_model.h5'\n",
    "# Save the best model\n",
    "if best_model_name == 'RNN':\n",
    "    rnn_model.save(save_path)\n",
    "elif best_model_name == 'LSTM':\n",
    "    lstm_model.save(save_path)\n",
    "elif best_model_name == 'Transformer':\n",
    "    transformer_model.save(save_path)\n",
    "\n",
    "print(f\"The best model ({best_model_name}) with accuracy {best_model_accuracy} has been saved to '{save_path}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['عَام', 'الفيل', '571', 'مِيلَادِي']]\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3 4]]\n",
      "[[0.34592092 0.27280077 0.3812784 ]]\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "ls = 'عام الفيل، 571 ميلادي '\n",
    "ls = preprocess_text(ls)\n",
    "tn = []\n",
    "tn.append(ls)\n",
    "print(tn)\n",
    "# Preprocess the Arabic sentence\n",
    "tokenizert = Tokenizer()\n",
    "tokenizert.fit_on_texts(tn)\n",
    "sequencest = tokenizert.texts_to_sequences(tn)\n",
    "sequencest = pad_sequences(sequencest,max_sequence_length)\n",
    "word2idx = tokenizert.word_index\n",
    "vocab_size = len(word2idx) + 1\n",
    "\n",
    "\n",
    "XX = pad_sequences(sequencest, padding='post', truncating='post', maxlen=max_sequence_length)\n",
    "\n",
    "var = lstm_model.predict(XX)\n",
    "\n",
    "print(XX)\n",
    "print(var)\n",
    "print(int(np.argmax(var, axis=1)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
