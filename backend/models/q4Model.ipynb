{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\amine\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow as keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.regularizers import l2\n",
    "from keras import layers\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Embedding, SimpleRNN, Dense, Dropout, LSTM, Bidirectional, Input\n",
    "from keras.layers import Attention, GlobalAveragePooling1D\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import stanza\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import BatchNormalization, Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*<h3>Load Data<h3>*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_id</th>\n",
       "      <th>answer</th>\n",
       "      <th>grade</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>في المدينة المنورة، 570 ميلادي: نبض الإسلام، و...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>قريش، مكة: نبينا الكريم -صلى الله عليه وسلم-، ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>تاريخ مكة يتلألأ بميلاد المصطفى محمد -صلى الله...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>قريش: جذور نبوية، مكة: مهد الإسلام، محمد -صلى ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>جذور قريش وأحضان مكة، شهدتا نور البعثة: مولد س...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>مكة، 572: ميلاد سيد الخلق -صلى الله عليه وسلم-...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4</td>\n",
       "      <td>مكة المكرمة، مهد النبوة الشريفة، شهدت ميلاد خا...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4</td>\n",
       "      <td>مكة، قريش: ميلاد محمد -صلى الله عليه وسلم-، رح...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4</td>\n",
       "      <td>قريش، مكة: محمد -صلى الله عليه وسلم-، نور الإس...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4</td>\n",
       "      <td>صخور مكة تهمس بحكايات النبوة: ميلاد محمد -صلى ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   question_id                                             answer  grade\n",
       "0            4  في المدينة المنورة، 570 ميلادي: نبض الإسلام، و...      0\n",
       "1            4  قريش، مكة: نبينا الكريم -صلى الله عليه وسلم-، ...      2\n",
       "2            4  تاريخ مكة يتلألأ بميلاد المصطفى محمد -صلى الله...      1\n",
       "3            4  قريش: جذور نبوية، مكة: مهد الإسلام، محمد -صلى ...      2\n",
       "4            4  جذور قريش وأحضان مكة، شهدتا نور البعثة: مولد س...      2\n",
       "5            4  مكة، 572: ميلاد سيد الخلق -صلى الله عليه وسلم-...      2\n",
       "6            4  مكة المكرمة، مهد النبوة الشريفة، شهدت ميلاد خا...      2\n",
       "7            4  مكة، قريش: ميلاد محمد -صلى الله عليه وسلم-، رح...      2\n",
       "8            4  قريش، مكة: محمد -صلى الله عليه وسلم-، نور الإس...      2\n",
       "9            4  صخور مكة تهمس بحكايات النبوة: ميلاد محمد -صلى ...      2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read csv file into pandas\n",
    "df = pd.read_csv(\"../datasets/shuffled4.csv\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*<h3>EDA<h3>*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 99 entries, 0 to 98\n",
      "Data columns (total 3 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   question_id  99 non-null     int64 \n",
      " 1   answer       99 non-null     object\n",
      " 2   grade        99 non-null     int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 2.4+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "grade\n",
       "0    28\n",
       "1    32\n",
       "2    39\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('grade').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(data=df, x='grade')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*<h3>Data Cleaning<h3>*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('question_id', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     في المدينة المنورة، 570 ميلادي: نبض الإسلام، و...\n",
      "1     قريش، مكة: نبينا الكريم -صلى الله عليه وسلم-، ...\n",
      "2     تاريخ مكة يتلألأ بميلاد المصطفى محمد -صلى الله...\n",
      "3     قريش: جذور نبوية، مكة: مهد الإسلام، محمد -صلى ...\n",
      "4     جذور قريش وأحضان مكة، شهدتا نور البعثة: مولد س...\n",
      "                            ...                        \n",
      "94    مكة المكرمة شهدت ولادة النبي -صلى الله عليه وس...\n",
      "95    كعبة الله، قريش: أمانة الإسلام، محمد -صلى الله...\n",
      "96        ولد رسول الله عليه الصَّلاة والسَّلام في يثرب\n",
      "97    بلدة قريش العريقة، احتضنت أعظم ميلاد: محمد -صل...\n",
      "98    رسول الله -صلى الله عليه وسلم- وُلِد في مكة ال...\n",
      "Name: answer, Length: 99, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df.drop_duplicates(inplace=True)\n",
    "print(df['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*<h3>Data Pre-Preocessing<h3>*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4de4938e112f47558b827fc1948f82e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.7.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-09 22:49:24 INFO: Downloading default packages for language: ar (Arabic) ...\n",
      "2024-01-09 22:49:25 INFO: File exists: C:\\Users\\amine\\stanza_resources\\ar\\default.zip\n",
      "2024-01-09 22:49:30 INFO: Finished downloading models and saved to C:\\Users\\amine\\stanza_resources.\n",
      "2024-01-09 22:49:30 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "968f1f9a2e224f3abcc9081f564d35f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.7.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-09 22:49:33 INFO: Loading these models for language: ar (Arabic):\n",
      "=============================\n",
      "| Processor | Package       |\n",
      "-----------------------------\n",
      "| tokenize  | padt          |\n",
      "| mwt       | padt          |\n",
      "| pos       | padt_charlm   |\n",
      "| lemma     | padt_nocharlm |\n",
      "| depparse  | padt_charlm   |\n",
      "| ner       | aqmar_charlm  |\n",
      "=============================\n",
      "\n",
      "2024-01-09 22:49:33 INFO: Using device: cpu\n",
      "2024-01-09 22:49:33 INFO: Loading: tokenize\n",
      "2024-01-09 22:49:34 INFO: Loading: mwt\n",
      "2024-01-09 22:49:34 INFO: Loading: pos\n",
      "2024-01-09 22:49:34 INFO: Loading: lemma\n",
      "2024-01-09 22:49:34 INFO: Loading: depparse\n",
      "2024-01-09 22:49:34 INFO: Loading: ner\n",
      "2024-01-09 22:49:35 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "df['grade'] = le.fit_transform(df['grade'])\n",
    "\n",
    "stanza.download('ar')\n",
    "nlp = stanza.Pipeline('ar')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    doc = nlp(text)\n",
    "    tokens = [word.lemma for sent in doc.sentences for word in sent.words if word.upos != 'PUNCT']\n",
    "    return tokens\n",
    "\n",
    "df['answer'] = df['answer'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0 ...   2   1   6]\n",
      " [  0   0   0 ...   6  22 115]\n",
      " [  0   0   0 ...  35  23  36]\n",
      " ...\n",
      " [  0   0   0 ...  15   7 199]\n",
      " [  0   0   0 ...   6  86  35]\n",
      " [  0   0   0 ...   7   8  18]]\n",
      "0     [فِي, مَدِينَة, مُنَوَّر, 570, مِيلَادِي, نَبض...\n",
      "1     [قريش, مكة, نبينا, الكريم, صَلَّى, الله, عَلَى...\n",
      "2     [تَارِيخ, مَكَّة, تَلَأَّأ, بِ, مِيلَاد, المصط...\n",
      "3     [قريش, جِذر, نَبَوِيّ, مَكَّة, مَهَّد, إِسلَام...\n",
      "4     [جذور, قريش, وَ, حُضن, مكة, شَهِد, نور, بَعثَة...\n",
      "                            ...                        \n",
      "94    [مَكَّة, مُكَرَّم, شَهِد, وِلَادَة, نَبِيّ, صَ...\n",
      "95    [كعبة, الله, قريش, أَمَانَة, إِسلَام, محمد, صَ...\n",
      "96    [وَ, لَدّ, رَسُول, الله, عَلَى, هُوَ, صََرَاة,...\n",
      "97    [بَلدَة, قريش, عَرِيق, اِحتَضَن, أَعظَم, مِيلَ...\n",
      "98    [رَسُول, الله, صَلَّى, الله, عَلَى, هُوَ, وَ, ...\n",
      "Name: answer, Length: 99, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Tokenization\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df['answer'])\n",
    "sequences = tokenizer.texts_to_sequences(df['answer'])\n",
    "max_sequence_length = max(len(s) for s in sequences)\n",
    "sequences = pad_sequences(sequences,max_sequence_length)\n",
    "word2idx = tokenizer.word_index\n",
    "vocab_size = len(word2idx) + 1\n",
    "\n",
    "\n",
    "X = pad_sequences(sequences, padding='post', truncating='post', maxlen=max_sequence_length)\n",
    "\n",
    "print(sequences)\n",
    "print(df['answer'])\n",
    "\n",
    "Y = to_categorical(df['grade'], num_classes=3)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.15, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*<h3>build Models<h3>*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*<h4>RNN Model<h4>*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\amine\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:From c:\\Users\\amine\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\amine\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "2/2 [==============================] - 5s 602ms/step - loss: 1.3001 - accuracy: 0.3038 - val_loss: 1.2063 - val_accuracy: 0.4000 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 1.1960 - accuracy: 0.3165 - val_loss: 1.2249 - val_accuracy: 0.4000 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 1.1445 - accuracy: 0.4688\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 1.1422 - accuracy: 0.4684 - val_loss: 1.2144 - val_accuracy: 0.4000 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 1.1194 - accuracy: 0.4430 - val_loss: 1.2117 - val_accuracy: 0.4000 - lr: 2.0000e-04\n",
      "Epoch 5/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 1.1961 - accuracy: 0.3594\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.0001.\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 1.1620 - accuracy: 0.3924 - val_loss: 1.2081 - val_accuracy: 0.4000 - lr: 2.0000e-04\n",
      "Epoch 6/100\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 1.1574 - accuracy: 0.4177 - val_loss: 1.2063 - val_accuracy: 0.4000 - lr: 1.0000e-04\n",
      "Epoch 7/100\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 1.1615 - accuracy: 0.3924 - val_loss: 1.2045 - val_accuracy: 0.4000 - lr: 1.0000e-04\n",
      "Epoch 8/100\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 1.1163 - accuracy: 0.4430 - val_loss: 1.2026 - val_accuracy: 0.4000 - lr: 1.0000e-04\n",
      "Epoch 9/100\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 1.1085 - accuracy: 0.4430 - val_loss: 1.2008 - val_accuracy: 0.4000 - lr: 1.0000e-04\n",
      "Epoch 10/100\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 1.1382 - accuracy: 0.3671 - val_loss: 1.1994 - val_accuracy: 0.4000 - lr: 1.0000e-04\n",
      "Epoch 11/100\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 1.1312 - accuracy: 0.4557 - val_loss: 1.1985 - val_accuracy: 0.4000 - lr: 1.0000e-04\n",
      "Epoch 12/100\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 1.1017 - accuracy: 0.4304 - val_loss: 1.1979 - val_accuracy: 0.4000 - lr: 1.0000e-04\n",
      "Epoch 13/100\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 1.1027 - accuracy: 0.5063 - val_loss: 1.1973 - val_accuracy: 0.4000 - lr: 1.0000e-04\n",
      "Epoch 14/100\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 1.0906 - accuracy: 0.4810 - val_loss: 1.1969 - val_accuracy: 0.4000 - lr: 1.0000e-04\n",
      "Epoch 15/100\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 1.0626 - accuracy: 0.5443 - val_loss: 1.1967 - val_accuracy: 0.4000 - lr: 1.0000e-04\n",
      "Epoch 16/100\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 1.1608 - accuracy: 0.4304 - val_loss: 1.1974 - val_accuracy: 0.4000 - lr: 1.0000e-04\n",
      "Epoch 17/100\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 1.0556 - accuracy: 0.5696 - val_loss: 1.1986 - val_accuracy: 0.4000 - lr: 1.0000e-04\n",
      "Epoch 18/100\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 1.0692 - accuracy: 0.4810 - val_loss: 1.1997 - val_accuracy: 0.4000 - lr: 1.0000e-04\n",
      "Epoch 19/100\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 1.1222 - accuracy: 0.4177 - val_loss: 1.2004 - val_accuracy: 0.4000 - lr: 1.0000e-04\n",
      "Epoch 20/100\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 1.0492 - accuracy: 0.5823 - val_loss: 1.2006 - val_accuracy: 0.4000 - lr: 1.0000e-04\n",
      "Epoch 21/100\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 1.0451 - accuracy: 0.5316 - val_loss: 1.2001 - val_accuracy: 0.4000 - lr: 1.0000e-04\n",
      "Epoch 22/100\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 1.0491 - accuracy: 0.5570 - val_loss: 1.1991 - val_accuracy: 0.4000 - lr: 1.0000e-04\n",
      "Epoch 23/100\n",
      "2/2 [==============================] - 0s 128ms/step - loss: 1.0410 - accuracy: 0.5443 - val_loss: 1.1975 - val_accuracy: 0.4000 - lr: 1.0000e-04\n",
      "Epoch 24/100\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 1.0267 - accuracy: 0.6076 - val_loss: 1.1954 - val_accuracy: 0.4000 - lr: 1.0000e-04\n",
      "Epoch 25/100\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.9940 - accuracy: 0.6203 - val_loss: 1.1932 - val_accuracy: 0.4000 - lr: 1.0000e-04\n",
      "Epoch 26/100\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 1.0083 - accuracy: 0.6203 - val_loss: 1.1915 - val_accuracy: 0.4000 - lr: 1.0000e-04\n",
      "Epoch 27/100\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 1.0096 - accuracy: 0.5696 - val_loss: 1.1905 - val_accuracy: 0.4000 - lr: 1.0000e-04\n",
      "Epoch 28/100\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 1.0406 - accuracy: 0.5696 - val_loss: 1.1897 - val_accuracy: 0.4000 - lr: 1.0000e-04\n",
      "Epoch 29/100\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 1.0001 - accuracy: 0.6582 - val_loss: 1.1893 - val_accuracy: 0.4000 - lr: 1.0000e-04\n",
      "Epoch 30/100\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 1.0229 - accuracy: 0.5570 - val_loss: 1.1884 - val_accuracy: 0.4000 - lr: 1.0000e-04\n",
      "Epoch 31/100\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.9998 - accuracy: 0.6329 - val_loss: 1.1872 - val_accuracy: 0.4000 - lr: 1.0000e-04\n",
      "Epoch 32/100\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.9694 - accuracy: 0.6835 - val_loss: 1.1861 - val_accuracy: 0.4000 - lr: 1.0000e-04\n",
      "Epoch 33/100\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.9680 - accuracy: 0.6076 - val_loss: 1.1855 - val_accuracy: 0.4000 - lr: 1.0000e-04\n",
      "Epoch 34/100\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.9433 - accuracy: 0.7215 - val_loss: 1.1852 - val_accuracy: 0.4000 - lr: 1.0000e-04\n",
      "Epoch 35/100\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 1.0059 - accuracy: 0.6076 - val_loss: 1.1855 - val_accuracy: 0.4000 - lr: 1.0000e-04\n",
      "Epoch 36/100\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.9516 - accuracy: 0.6329 - val_loss: 1.1862 - val_accuracy: 0.4000 - lr: 1.0000e-04\n",
      "Epoch 37/100\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.9838 - accuracy: 0.5949 - val_loss: 1.1869 - val_accuracy: 0.4000 - lr: 1.0000e-04\n",
      "Epoch 38/100\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.9577 - accuracy: 0.6329 - val_loss: 1.1869 - val_accuracy: 0.4000 - lr: 1.0000e-04\n",
      "Epoch 39/100\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.9086 - accuracy: 0.7089 - val_loss: 1.1864 - val_accuracy: 0.4000 - lr: 1.0000e-04\n",
      "Epoch 40/100\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.9343 - accuracy: 0.6709 - val_loss: 1.1857 - val_accuracy: 0.4000 - lr: 1.0000e-04\n",
      "Epoch 41/100\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.9151 - accuracy: 0.6962 - val_loss: 1.1850 - val_accuracy: 0.4000 - lr: 1.0000e-04\n",
      "Epoch 42/100\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.9336 - accuracy: 0.7089 - val_loss: 1.1841 - val_accuracy: 0.4000 - lr: 1.0000e-04\n",
      "Epoch 43/100\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.9083 - accuracy: 0.7089 - val_loss: 1.1834 - val_accuracy: 0.4000 - lr: 1.0000e-04\n",
      "Epoch 44/100\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.8682 - accuracy: 0.7722 - val_loss: 1.1827 - val_accuracy: 0.4000 - lr: 1.0000e-04\n",
      "Epoch 45/100\n",
      "2/2 [==============================] - 0s 110ms/step - loss: 0.8747 - accuracy: 0.7215 - val_loss: 1.1819 - val_accuracy: 0.4000 - lr: 1.0000e-04\n",
      "Epoch 46/100\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.9019 - accuracy: 0.6456 - val_loss: 1.1808 - val_accuracy: 0.4000 - lr: 1.0000e-04\n",
      "Epoch 47/100\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.8958 - accuracy: 0.7089 - val_loss: 1.1792 - val_accuracy: 0.4000 - lr: 1.0000e-04\n",
      "Epoch 48/100\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.8607 - accuracy: 0.7848 - val_loss: 1.1769 - val_accuracy: 0.4000 - lr: 1.0000e-04\n",
      "Epoch 49/100\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.8483 - accuracy: 0.7342 - val_loss: 1.1745 - val_accuracy: 0.4000 - lr: 1.0000e-04\n",
      "Epoch 50/100\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.8459 - accuracy: 0.7342 - val_loss: 1.1717 - val_accuracy: 0.4000 - lr: 1.0000e-04\n",
      "Epoch 51/100\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.8797 - accuracy: 0.6835 - val_loss: 1.1682 - val_accuracy: 0.4000 - lr: 1.0000e-04\n",
      "Epoch 52/100\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.8422 - accuracy: 0.7722 - val_loss: 1.1647 - val_accuracy: 0.4000 - lr: 1.0000e-04\n",
      "Epoch 53/100\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.8555 - accuracy: 0.7089 - val_loss: 1.1618 - val_accuracy: 0.4000 - lr: 1.0000e-04\n",
      "Epoch 54/100\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.8543 - accuracy: 0.7468 - val_loss: 1.1592 - val_accuracy: 0.4000 - lr: 1.0000e-04\n",
      "Epoch 55/100\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.8496 - accuracy: 0.6582 - val_loss: 1.1564 - val_accuracy: 0.4000 - lr: 1.0000e-04\n",
      "Epoch 56/100\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.8270 - accuracy: 0.7975 - val_loss: 1.1532 - val_accuracy: 0.4000 - lr: 1.0000e-04\n",
      "Epoch 57/100\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.7645 - accuracy: 0.8228 - val_loss: 1.1500 - val_accuracy: 0.4000 - lr: 1.0000e-04\n",
      "Epoch 58/100\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.7727 - accuracy: 0.7975 - val_loss: 1.1470 - val_accuracy: 0.4000 - lr: 1.0000e-04\n",
      "Epoch 59/100\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.7923 - accuracy: 0.7342 - val_loss: 1.1442 - val_accuracy: 0.4000 - lr: 1.0000e-04\n",
      "Epoch 60/100\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.7357 - accuracy: 0.8101 - val_loss: 1.1406 - val_accuracy: 0.4000 - lr: 1.0000e-04\n",
      "Epoch 61/100\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.7491 - accuracy: 0.8101 - val_loss: 1.1369 - val_accuracy: 0.4000 - lr: 1.0000e-04\n",
      "Epoch 62/100\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.7635 - accuracy: 0.7975 - val_loss: 1.1340 - val_accuracy: 0.4000 - lr: 1.0000e-04\n",
      "Epoch 63/100\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.7220 - accuracy: 0.8481 - val_loss: 1.1319 - val_accuracy: 0.4000 - lr: 1.0000e-04\n",
      "Epoch 64/100\n",
      "2/2 [==============================] - 0s 111ms/step - loss: 0.7113 - accuracy: 0.8354 - val_loss: 1.1284 - val_accuracy: 0.4000 - lr: 1.0000e-04\n",
      "Epoch 65/100\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.7247 - accuracy: 0.7848 - val_loss: 1.1223 - val_accuracy: 0.4000 - lr: 1.0000e-04\n",
      "Epoch 66/100\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.7009 - accuracy: 0.8481 - val_loss: 1.1166 - val_accuracy: 0.4000 - lr: 1.0000e-04\n",
      "Epoch 67/100\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.6864 - accuracy: 0.8987 - val_loss: 1.1142 - val_accuracy: 0.4000 - lr: 1.0000e-04\n",
      "Epoch 68/100\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.6219 - accuracy: 0.9114 - val_loss: 1.1114 - val_accuracy: 0.4000 - lr: 1.0000e-04\n",
      "Epoch 69/100\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.6621 - accuracy: 0.9114 - val_loss: 1.1067 - val_accuracy: 0.4000 - lr: 1.0000e-04\n",
      "Epoch 70/100\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.5992 - accuracy: 0.9241 - val_loss: 1.1034 - val_accuracy: 0.4000 - lr: 1.0000e-04\n",
      "Epoch 71/100\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.6552 - accuracy: 0.8861 - val_loss: 1.0983 - val_accuracy: 0.4000 - lr: 1.0000e-04\n",
      "Epoch 72/100\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.6129 - accuracy: 0.9241 - val_loss: 1.0945 - val_accuracy: 0.4000 - lr: 1.0000e-04\n",
      "Epoch 73/100\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.6058 - accuracy: 0.9367 - val_loss: 1.0958 - val_accuracy: 0.4000 - lr: 1.0000e-04\n",
      "Epoch 74/100\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.6155 - accuracy: 0.9241 - val_loss: 1.0965 - val_accuracy: 0.4000 - lr: 1.0000e-04\n",
      "Epoch 75/100\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.5968 - accuracy: 0.9367 - val_loss: 1.0897 - val_accuracy: 0.4000 - lr: 1.0000e-04\n",
      "Epoch 76/100\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.5784 - accuracy: 0.9620 - val_loss: 1.0806 - val_accuracy: 0.4000 - lr: 1.0000e-04\n",
      "Epoch 77/100\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.6068 - accuracy: 0.9367 - val_loss: 1.0804 - val_accuracy: 0.4000 - lr: 1.0000e-04\n",
      "Epoch 78/100\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.5606 - accuracy: 0.9494 - val_loss: 1.0772 - val_accuracy: 0.4000 - lr: 1.0000e-04\n",
      "Epoch 79/100\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.5432 - accuracy: 0.9620 - val_loss: 1.0677 - val_accuracy: 0.4000 - lr: 1.0000e-04\n",
      "Epoch 80/100\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.5380 - accuracy: 0.9747 - val_loss: 1.0638 - val_accuracy: 0.4000 - lr: 1.0000e-04\n",
      "Epoch 81/100\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.5695 - accuracy: 0.9367 - val_loss: 1.0652 - val_accuracy: 0.4000 - lr: 1.0000e-04\n",
      "Epoch 82/100\n",
      "2/2 [==============================] - 0s 110ms/step - loss: 0.5363 - accuracy: 0.9620 - val_loss: 1.0620 - val_accuracy: 0.4000 - lr: 1.0000e-04\n",
      "Epoch 83/100\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.5065 - accuracy: 0.9747 - val_loss: 1.0480 - val_accuracy: 0.5500 - lr: 1.0000e-04\n",
      "Epoch 84/100\n",
      "2/2 [==============================] - 0s 169ms/step - loss: 0.5402 - accuracy: 0.9367 - val_loss: 1.0483 - val_accuracy: 0.5000 - lr: 1.0000e-04\n",
      "Epoch 85/100\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.5154 - accuracy: 0.9367 - val_loss: 1.0536 - val_accuracy: 0.4500 - lr: 1.0000e-04\n",
      "Epoch 86/100\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.5280 - accuracy: 0.9494 - val_loss: 1.0458 - val_accuracy: 0.5500 - lr: 1.0000e-04\n",
      "Epoch 87/100\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.5080 - accuracy: 0.9620 - val_loss: 1.0411 - val_accuracy: 0.5500 - lr: 1.0000e-04\n",
      "Epoch 88/100\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 0.4970 - accuracy: 0.9620 - val_loss: 1.0376 - val_accuracy: 0.5500 - lr: 1.0000e-04\n",
      "Epoch 89/100\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 0.4845 - accuracy: 0.9620 - val_loss: 1.0341 - val_accuracy: 0.5500 - lr: 1.0000e-04\n",
      "Epoch 90/100\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.4610 - accuracy: 0.9747 - val_loss: 1.0246 - val_accuracy: 0.6000 - lr: 1.0000e-04\n",
      "Epoch 91/100\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.4570 - accuracy: 0.9747 - val_loss: 1.0078 - val_accuracy: 0.6500 - lr: 1.0000e-04\n",
      "Epoch 92/100\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.4401 - accuracy: 0.9873 - val_loss: 1.0057 - val_accuracy: 0.6000 - lr: 1.0000e-04\n",
      "Epoch 93/100\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.4447 - accuracy: 0.9873 - val_loss: 1.0130 - val_accuracy: 0.6000 - lr: 1.0000e-04\n",
      "Epoch 94/100\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.4344 - accuracy: 1.0000 - val_loss: 1.0060 - val_accuracy: 0.6000 - lr: 1.0000e-04\n",
      "Epoch 95/100\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4290 - accuracy: 1.0000 - val_loss: 0.9973 - val_accuracy: 0.6500 - lr: 1.0000e-04\n",
      "Epoch 96/100\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.4202 - accuracy: 0.9873 - val_loss: 0.9989 - val_accuracy: 0.6000 - lr: 1.0000e-04\n",
      "Epoch 97/100\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.4260 - accuracy: 0.9873 - val_loss: 0.9887 - val_accuracy: 0.7000 - lr: 1.0000e-04\n",
      "Epoch 98/100\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.3995 - accuracy: 1.0000 - val_loss: 0.9833 - val_accuracy: 0.7000 - lr: 1.0000e-04\n",
      "Epoch 99/100\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.3975 - accuracy: 1.0000 - val_loss: 0.9871 - val_accuracy: 0.6000 - lr: 1.0000e-04\n",
      "Epoch 100/100\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.4379 - accuracy: 0.9873 - val_loss: 0.9828 - val_accuracy: 0.6000 - lr: 1.0000e-04\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.8320 - accuracy: 0.9333\n",
      "Evaluation Metrics for RNN:\n",
      "loss: 0.8319979310035706\n",
      "accuracy: 0.9333333373069763\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "\n",
    "X_train_rnn, X_test_rnn, y_train_rnn, y_test_rnn = train_test_split(X, Y, test_size=0.2)\n",
    "\n",
    "def RNN_model(vocab_size, sequence_length):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=vocab_size, output_dim=128, input_length=sequence_length))\n",
    "    model.add(SimpleRNN(units=128, activation='relu', return_sequences=True))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(SimpleRNN(units=64, activation='sigmoid'))\n",
    "    model.add(Dense(256, activation='sigmoid', kernel_regularizer=l2(0.001)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "    optimizer = Adam(learning_rate=0.001)\n",
    "    \n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=0.0001, verbose=1)\n",
    "#EMBEDDING_DIM = 110\n",
    "rnn_model = RNN_model(vocab_size, max_sequence_length)\n",
    "early_stopping_rnn = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "history_updated_rnn = rnn_model.fit(X_train_rnn, y_train_rnn, validation_data=(X_test_rnn, y_test_rnn), epochs=100, batch_size=64, callbacks=[reduce_lr])\n",
    "\n",
    "# Evaluate the RNN model\n",
    "evaluation_metrics_updated_rnn = rnn_model.evaluate(X_test, Y_test)\n",
    "print(\"Evaluation Metrics for RNN:\")\n",
    "for metric_name, metric_value in zip(rnn_model.metrics_names, evaluation_metrics_updated_rnn):\n",
    "    print(f\"{metric_name}: {metric_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*<h4>LSTM Model<h4>*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2/2 [==============================] - 5s 633ms/step - loss: 1.1946 - accuracy: 0.3418 - val_loss: 1.1968 - val_accuracy: 0.4000\n",
      "Epoch 2/100\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 1.1309 - accuracy: 0.4937 - val_loss: 1.1919 - val_accuracy: 0.4000\n",
      "Epoch 3/100\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 1.0586 - accuracy: 0.5823 - val_loss: 1.1856 - val_accuracy: 0.4000\n",
      "Epoch 4/100\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 0.9583 - accuracy: 0.7595 - val_loss: 1.1780 - val_accuracy: 0.4500\n",
      "Epoch 5/100\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 0.8508 - accuracy: 0.7975 - val_loss: 1.1719 - val_accuracy: 0.7000\n",
      "Epoch 6/100\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.7103 - accuracy: 0.8354 - val_loss: 1.1662 - val_accuracy: 0.6500\n",
      "Epoch 7/100\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 0.5312 - accuracy: 0.8861 - val_loss: 1.1573 - val_accuracy: 0.3500\n",
      "Epoch 8/100\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.4045 - accuracy: 0.9241 - val_loss: 1.1526 - val_accuracy: 0.3500\n",
      "Epoch 9/100\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.2937 - accuracy: 0.9494 - val_loss: 1.1425 - val_accuracy: 0.4000\n",
      "Epoch 10/100\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 0.1819 - accuracy: 0.9620 - val_loss: 1.1229 - val_accuracy: 0.6500\n",
      "Epoch 11/100\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.1528 - accuracy: 0.9747 - val_loss: 1.1150 - val_accuracy: 0.6500\n",
      "Epoch 12/100\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.1046 - accuracy: 1.0000 - val_loss: 1.1146 - val_accuracy: 0.6000\n",
      "Epoch 13/100\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.0966 - accuracy: 1.0000 - val_loss: 1.1114 - val_accuracy: 0.5500\n",
      "Epoch 14/100\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.0876 - accuracy: 1.0000 - val_loss: 1.1086 - val_accuracy: 0.5500\n",
      "Epoch 15/100\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.1798 - accuracy: 0.9873 - val_loss: 1.1116 - val_accuracy: 0.5000\n",
      "Epoch 16/100\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.0909 - accuracy: 1.0000 - val_loss: 1.1108 - val_accuracy: 0.4500\n",
      "Epoch 17/100\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.0974 - accuracy: 1.0000 - val_loss: 1.1059 - val_accuracy: 0.5000\n",
      "Epoch 18/100\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.0957 - accuracy: 0.9873 - val_loss: 1.1010 - val_accuracy: 0.5500\n",
      "Epoch 19/100\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.0795 - accuracy: 1.0000 - val_loss: 1.0978 - val_accuracy: 0.5000\n",
      "Epoch 20/100\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.0783 - accuracy: 1.0000 - val_loss: 1.0948 - val_accuracy: 0.5000\n",
      "Epoch 21/100\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.0774 - accuracy: 1.0000 - val_loss: 1.0925 - val_accuracy: 0.4500\n",
      "Epoch 22/100\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.0946 - accuracy: 0.9873 - val_loss: 1.0865 - val_accuracy: 0.5500\n",
      "Epoch 23/100\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.0750 - accuracy: 1.0000 - val_loss: 1.0829 - val_accuracy: 0.5500\n",
      "Epoch 24/100\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.0737 - accuracy: 1.0000 - val_loss: 1.0804 - val_accuracy: 0.5500\n",
      "Epoch 25/100\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.0728 - accuracy: 1.0000 - val_loss: 1.0785 - val_accuracy: 0.6000\n",
      "Epoch 26/100\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 0.0721 - accuracy: 1.0000 - val_loss: 1.0768 - val_accuracy: 0.6000\n",
      "Epoch 27/100\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.0710 - accuracy: 1.0000 - val_loss: 1.0752 - val_accuracy: 0.6000\n",
      "Epoch 28/100\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.0711 - accuracy: 1.0000 - val_loss: 1.0734 - val_accuracy: 0.6000\n",
      "Epoch 29/100\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.0696 - accuracy: 1.0000 - val_loss: 1.0714 - val_accuracy: 0.6000\n",
      "Epoch 30/100\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.0685 - accuracy: 1.0000 - val_loss: 1.0694 - val_accuracy: 0.6000\n",
      "Epoch 31/100\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.0675 - accuracy: 1.0000 - val_loss: 1.0674 - val_accuracy: 0.6000\n",
      "Epoch 32/100\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.0665 - accuracy: 1.0000 - val_loss: 1.0654 - val_accuracy: 0.6000\n",
      "Epoch 33/100\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.0659 - accuracy: 1.0000 - val_loss: 1.0633 - val_accuracy: 0.6000\n",
      "Epoch 34/100\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.0652 - accuracy: 1.0000 - val_loss: 1.0614 - val_accuracy: 0.6000\n",
      "Epoch 35/100\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.0644 - accuracy: 1.0000 - val_loss: 1.0599 - val_accuracy: 0.6000\n",
      "Epoch 36/100\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.0633 - accuracy: 1.0000 - val_loss: 1.0590 - val_accuracy: 0.6000\n",
      "Epoch 37/100\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.0626 - accuracy: 1.0000 - val_loss: 1.0580 - val_accuracy: 0.5500\n",
      "Epoch 38/100\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.0618 - accuracy: 1.0000 - val_loss: 1.0569 - val_accuracy: 0.5500\n",
      "Epoch 39/100\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 0.0610 - accuracy: 1.0000 - val_loss: 1.0556 - val_accuracy: 0.5500\n",
      "Epoch 40/100\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.0604 - accuracy: 1.0000 - val_loss: 1.0544 - val_accuracy: 0.5500\n",
      "Epoch 41/100\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.0596 - accuracy: 1.0000 - val_loss: 1.0533 - val_accuracy: 0.6000\n",
      "Epoch 42/100\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.0589 - accuracy: 1.0000 - val_loss: 1.0519 - val_accuracy: 0.6000\n",
      "Epoch 43/100\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.0583 - accuracy: 1.0000 - val_loss: 1.0504 - val_accuracy: 0.6000\n",
      "Epoch 44/100\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 0.0575 - accuracy: 1.0000 - val_loss: 1.0490 - val_accuracy: 0.6000\n",
      "Epoch 45/100\n",
      "2/2 [==============================] - 0s 95ms/step - loss: 0.0569 - accuracy: 1.0000 - val_loss: 1.0476 - val_accuracy: 0.6000\n",
      "Epoch 46/100\n",
      "2/2 [==============================] - 0s 124ms/step - loss: 0.0565 - accuracy: 1.0000 - val_loss: 1.0459 - val_accuracy: 0.6000\n",
      "Epoch 47/100\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 0.0556 - accuracy: 1.0000 - val_loss: 1.0443 - val_accuracy: 0.6000\n",
      "Epoch 48/100\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.0551 - accuracy: 1.0000 - val_loss: 1.0426 - val_accuracy: 0.6000\n",
      "Epoch 49/100\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.0544 - accuracy: 1.0000 - val_loss: 1.0411 - val_accuracy: 0.6000\n",
      "Epoch 50/100\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.0543 - accuracy: 1.0000 - val_loss: 1.0384 - val_accuracy: 0.6000\n",
      "Epoch 51/100\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.0532 - accuracy: 1.0000 - val_loss: 1.0350 - val_accuracy: 0.6000\n",
      "Epoch 52/100\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.0526 - accuracy: 1.0000 - val_loss: 1.0321 - val_accuracy: 0.5500\n",
      "Epoch 53/100\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.0521 - accuracy: 1.0000 - val_loss: 1.0295 - val_accuracy: 0.5500\n",
      "Epoch 54/100\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.0516 - accuracy: 1.0000 - val_loss: 1.0272 - val_accuracy: 0.6000\n",
      "Epoch 55/100\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.0510 - accuracy: 1.0000 - val_loss: 1.0250 - val_accuracy: 0.6000\n",
      "Epoch 56/100\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.0505 - accuracy: 1.0000 - val_loss: 1.0230 - val_accuracy: 0.6000\n",
      "Epoch 57/100\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.0500 - accuracy: 1.0000 - val_loss: 1.0212 - val_accuracy: 0.6000\n",
      "Epoch 58/100\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.0495 - accuracy: 1.0000 - val_loss: 1.0195 - val_accuracy: 0.6000\n",
      "Epoch 59/100\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.0491 - accuracy: 1.0000 - val_loss: 1.0179 - val_accuracy: 0.6000\n",
      "Epoch 60/100\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.0485 - accuracy: 1.0000 - val_loss: 1.0164 - val_accuracy: 0.6000\n",
      "Epoch 61/100\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.0481 - accuracy: 1.0000 - val_loss: 1.0150 - val_accuracy: 0.6000\n",
      "Epoch 62/100\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.0475 - accuracy: 1.0000 - val_loss: 1.0136 - val_accuracy: 0.6000\n",
      "Epoch 63/100\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.0471 - accuracy: 1.0000 - val_loss: 1.0123 - val_accuracy: 0.6000\n",
      "Epoch 64/100\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.0466 - accuracy: 1.0000 - val_loss: 1.0109 - val_accuracy: 0.6500\n",
      "Epoch 65/100\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.0462 - accuracy: 1.0000 - val_loss: 1.0096 - val_accuracy: 0.6500\n",
      "Epoch 66/100\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.0457 - accuracy: 1.0000 - val_loss: 1.0082 - val_accuracy: 0.6500\n",
      "Epoch 67/100\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.0453 - accuracy: 1.0000 - val_loss: 1.0069 - val_accuracy: 0.6500\n",
      "Epoch 68/100\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.0449 - accuracy: 1.0000 - val_loss: 1.0056 - val_accuracy: 0.6500\n",
      "Epoch 69/100\n",
      "2/2 [==============================] - 0s 95ms/step - loss: 0.0445 - accuracy: 1.0000 - val_loss: 1.0043 - val_accuracy: 0.6500\n",
      "Epoch 70/100\n",
      "2/2 [==============================] - 0s 105ms/step - loss: 0.0441 - accuracy: 1.0000 - val_loss: 1.0029 - val_accuracy: 0.6500\n",
      "Epoch 71/100\n",
      "2/2 [==============================] - 0s 204ms/step - loss: 0.0437 - accuracy: 1.0000 - val_loss: 1.0016 - val_accuracy: 0.6500\n",
      "Epoch 72/100\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 0.0433 - accuracy: 1.0000 - val_loss: 1.0002 - val_accuracy: 0.6500\n",
      "Epoch 73/100\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 0.0429 - accuracy: 1.0000 - val_loss: 0.9990 - val_accuracy: 0.6500\n",
      "Epoch 74/100\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.0425 - accuracy: 1.0000 - val_loss: 0.9978 - val_accuracy: 0.6500\n",
      "Epoch 75/100\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.0422 - accuracy: 1.0000 - val_loss: 0.9965 - val_accuracy: 0.6500\n",
      "Epoch 76/100\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.0417 - accuracy: 1.0000 - val_loss: 0.9953 - val_accuracy: 0.6500\n",
      "Epoch 77/100\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.0414 - accuracy: 1.0000 - val_loss: 0.9939 - val_accuracy: 0.6500\n",
      "Epoch 78/100\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 0.0410 - accuracy: 1.0000 - val_loss: 0.9927 - val_accuracy: 0.7000\n",
      "Epoch 79/100\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.0407 - accuracy: 1.0000 - val_loss: 0.9913 - val_accuracy: 0.7000\n",
      "Epoch 80/100\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.0403 - accuracy: 1.0000 - val_loss: 0.9900 - val_accuracy: 0.7000\n",
      "Epoch 81/100\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.0400 - accuracy: 1.0000 - val_loss: 0.9887 - val_accuracy: 0.6500\n",
      "Epoch 82/100\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.0396 - accuracy: 1.0000 - val_loss: 0.9874 - val_accuracy: 0.6500\n",
      "Epoch 83/100\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.0400 - accuracy: 1.0000 - val_loss: 0.9879 - val_accuracy: 0.7000\n",
      "Epoch 84/100\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.0390 - accuracy: 1.0000 - val_loss: 0.9899 - val_accuracy: 0.7000\n",
      "Epoch 85/100\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.0387 - accuracy: 1.0000 - val_loss: 0.9916 - val_accuracy: 0.7500\n",
      "Epoch 86/100\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.0384 - accuracy: 1.0000 - val_loss: 0.9931 - val_accuracy: 0.7500\n",
      "Epoch 87/100\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.0447 - accuracy: 1.0000 - val_loss: 0.9776 - val_accuracy: 0.6500\n",
      "Epoch 88/100\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.0798 - accuracy: 0.9873 - val_loss: 0.9740 - val_accuracy: 0.6000\n",
      "Epoch 89/100\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 0.0375 - accuracy: 1.0000 - val_loss: 0.9891 - val_accuracy: 0.5500\n",
      "Epoch 90/100\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 0.1854 - accuracy: 0.9747 - val_loss: 1.0009 - val_accuracy: 0.5500\n",
      "Epoch 91/100\n",
      "2/2 [==============================] - 0s 122ms/step - loss: 0.0919 - accuracy: 0.9747 - val_loss: 1.0120 - val_accuracy: 0.5500\n",
      "Epoch 92/100\n",
      "2/2 [==============================] - 0s 114ms/step - loss: 0.0759 - accuracy: 0.9873 - val_loss: 1.0219 - val_accuracy: 0.5500\n",
      "Epoch 93/100\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 0.0649 - accuracy: 0.9873 - val_loss: 1.0225 - val_accuracy: 0.5500\n",
      "Epoch 94/100\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.0775 - accuracy: 0.9873 - val_loss: 1.0164 - val_accuracy: 0.6000\n",
      "Epoch 95/100\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 0.0732 - accuracy: 0.9873 - val_loss: 1.0077 - val_accuracy: 0.6500\n",
      "Epoch 96/100\n",
      "2/2 [==============================] - 0s 126ms/step - loss: 0.0681 - accuracy: 0.9873 - val_loss: 0.9977 - val_accuracy: 0.7500\n",
      "Epoch 97/100\n",
      "2/2 [==============================] - 0s 130ms/step - loss: 0.0796 - accuracy: 0.9873 - val_loss: 0.9876 - val_accuracy: 0.7500\n",
      "Epoch 98/100\n",
      "2/2 [==============================] - 0s 105ms/step - loss: 0.0697 - accuracy: 0.9747 - val_loss: 0.9786 - val_accuracy: 0.8000\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.8266 - accuracy: 0.9333\n",
      "Evaluation Metrics for LSTM:\n",
      "loss: 0.8265913724899292\n",
      "accuracy: 0.9333333373069763\n"
     ]
    }
   ],
   "source": [
    "def LSTM_model(vocab_size, sequence_length):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=vocab_size, output_dim=128, input_length=sequence_length))\n",
    "    model.add(LSTM(units=128, activation='relu', return_sequences=True))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LSTM(units=64, activation='relu'))\n",
    "    model.add(Dense(256, activation='relu', kernel_regularizer=l2(0.001)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "    optimizer = Adam(learning_rate=0.001)\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "#EMBEDDING_DIM = 110\n",
    "lstm_model = LSTM_model(vocab_size, max_sequence_length)\n",
    "early_stopping_rnn = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "history_updated_rnn = lstm_model.fit(X_train_rnn, y_train_rnn, validation_data=(X_test_rnn, y_test_rnn), epochs=100, batch_size=64, callbacks=[early_stopping_rnn])\n",
    "\n",
    "\n",
    "# Evaluate the lstm model\n",
    "evaluation_metrics_updated_lstm = lstm_model.evaluate(X_test, Y_test)\n",
    "print(\"Evaluation Metrics for LSTM:\")\n",
    "for metric_name, metric_value in zip(lstm_model.metrics_names, evaluation_metrics_updated_lstm):\n",
    "    print(f\"{metric_name}: {metric_value}\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*<h4>TRANSFORMER Model<h4>*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, density, rate=0.1, l2_reg=0.01):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            layers.Dense(density, activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(l2_reg)),\n",
    "            layers.Dense(embed_dim, kernel_regularizer=tf.keras.regularizers.l2(l2_reg)),\n",
    "        ])\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "\n",
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions\n",
    "\n",
    "def build_transformer_model(maxlen, vocab_size, embed_dim, num_heads, density, transformer_units, mlp_units, dropout_rate, num_classes):\n",
    "    inputs = layers.Input(shape=(maxlen,))\n",
    "    embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
    "    x = embedding_layer(inputs)\n",
    "    for _ in range(transformer_units):\n",
    "        x = TransformerBlock(embed_dim, num_heads, density, rate=dropout_rate)(x)\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    for dim in mlp_units:\n",
    "        x = layers.Dense(dim, activation=\"relu\")(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "    outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = max_sequence_length\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "embed_dim = 120\n",
    "num_heads = 2\n",
    "density = 3\n",
    "transformer_units = 4\n",
    "mlp_units = [128]\n",
    "dropout_rate = 0.5\n",
    "num_classes = len(df['grade'].unique())\n",
    "\n",
    "transformer_model = build_transformer_model(maxlen, vocab_size, embed_dim, num_heads, density, transformer_units, mlp_units, dropout_rate, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\amine\\anaconda3\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Epoch 1/30\n",
      "3/3 [==============================] - 11s 573ms/step - loss: 1.7482 - accuracy: 0.2976 - val_loss: 1.9421 - val_accuracy: 0.0667\n",
      "Epoch 2/30\n",
      "3/3 [==============================] - 0s 69ms/step - loss: 1.6357 - accuracy: 0.3214 - val_loss: 1.4400 - val_accuracy: 0.4000\n",
      "Epoch 3/30\n",
      "3/3 [==============================] - 0s 69ms/step - loss: 1.4431 - accuracy: 0.5238 - val_loss: 1.5761 - val_accuracy: 0.3333\n",
      "Epoch 4/30\n",
      "3/3 [==============================] - 0s 65ms/step - loss: 1.3723 - accuracy: 0.5000 - val_loss: 1.2915 - val_accuracy: 0.6667\n",
      "Epoch 5/30\n",
      "3/3 [==============================] - 0s 68ms/step - loss: 1.2790 - accuracy: 0.5714 - val_loss: 1.0535 - val_accuracy: 0.8667\n",
      "Epoch 6/30\n",
      "3/3 [==============================] - 0s 63ms/step - loss: 1.1111 - accuracy: 0.6786 - val_loss: 1.2253 - val_accuracy: 0.7333\n",
      "Epoch 7/30\n",
      "3/3 [==============================] - 0s 62ms/step - loss: 1.0149 - accuracy: 0.6786 - val_loss: 0.9922 - val_accuracy: 0.8000\n",
      "Epoch 8/30\n",
      "3/3 [==============================] - 0s 63ms/step - loss: 0.8708 - accuracy: 0.7500 - val_loss: 0.8317 - val_accuracy: 0.8667\n",
      "Epoch 9/30\n",
      "3/3 [==============================] - 0s 67ms/step - loss: 0.7915 - accuracy: 0.8095 - val_loss: 0.9516 - val_accuracy: 0.8000\n",
      "Epoch 10/30\n",
      "3/3 [==============================] - 0s 70ms/step - loss: 0.6606 - accuracy: 0.8690 - val_loss: 0.7970 - val_accuracy: 0.8000\n",
      "Epoch 11/30\n",
      "3/3 [==============================] - 0s 69ms/step - loss: 0.5436 - accuracy: 0.9524 - val_loss: 0.6402 - val_accuracy: 0.8667\n",
      "Epoch 12/30\n",
      "3/3 [==============================] - 0s 77ms/step - loss: 0.5345 - accuracy: 0.9405 - val_loss: 0.8661 - val_accuracy: 0.9333\n",
      "Epoch 13/30\n",
      "3/3 [==============================] - 0s 71ms/step - loss: 0.4970 - accuracy: 0.9762 - val_loss: 1.0101 - val_accuracy: 0.8667\n",
      "Epoch 14/30\n",
      "3/3 [==============================] - 0s 65ms/step - loss: 0.4275 - accuracy: 0.9881 - val_loss: 0.9837 - val_accuracy: 0.8000\n",
      "Epoch 15/30\n",
      "3/3 [==============================] - 0s 64ms/step - loss: 0.4259 - accuracy: 0.9762 - val_loss: 0.8393 - val_accuracy: 0.9333\n",
      "Epoch 16/30\n",
      "3/3 [==============================] - 0s 96ms/step - loss: 0.3866 - accuracy: 1.0000 - val_loss: 0.7321 - val_accuracy: 0.8667\n",
      "Epoch 17/30\n",
      "3/3 [==============================] - 0s 64ms/step - loss: 0.3832 - accuracy: 0.9881 - val_loss: 0.5532 - val_accuracy: 0.9333\n",
      "Epoch 18/30\n",
      "3/3 [==============================] - 0s 63ms/step - loss: 0.3586 - accuracy: 1.0000 - val_loss: 0.5650 - val_accuracy: 0.8667\n",
      "Epoch 19/30\n",
      "3/3 [==============================] - 0s 91ms/step - loss: 0.3517 - accuracy: 1.0000 - val_loss: 0.6400 - val_accuracy: 0.8667\n",
      "Epoch 20/30\n",
      "3/3 [==============================] - 0s 67ms/step - loss: 0.3558 - accuracy: 0.9881 - val_loss: 0.7678 - val_accuracy: 0.8000\n",
      "Epoch 21/30\n",
      "3/3 [==============================] - 0s 78ms/step - loss: 0.3363 - accuracy: 1.0000 - val_loss: 1.0019 - val_accuracy: 0.8667\n",
      "Epoch 22/30\n",
      "3/3 [==============================] - 0s 134ms/step - loss: 0.3314 - accuracy: 1.0000 - val_loss: 1.0015 - val_accuracy: 0.8667\n",
      "Epoch 23/30\n",
      "3/3 [==============================] - 1s 228ms/step - loss: 0.3254 - accuracy: 1.0000 - val_loss: 0.8696 - val_accuracy: 0.8667\n",
      "Epoch 24/30\n",
      "3/3 [==============================] - 0s 130ms/step - loss: 0.3139 - accuracy: 1.0000 - val_loss: 1.0602 - val_accuracy: 0.8000\n",
      "Epoch 25/30\n",
      "3/3 [==============================] - 1s 232ms/step - loss: 0.3068 - accuracy: 1.0000 - val_loss: 1.1619 - val_accuracy: 0.8000\n",
      "Epoch 26/30\n",
      "3/3 [==============================] - 0s 210ms/step - loss: 0.3015 - accuracy: 1.0000 - val_loss: 1.2058 - val_accuracy: 0.8000\n",
      "Epoch 27/30\n",
      "3/3 [==============================] - 0s 124ms/step - loss: 0.2938 - accuracy: 1.0000 - val_loss: 1.2475 - val_accuracy: 0.8667\n",
      "Epoch 28/30\n",
      "3/3 [==============================] - 0s 79ms/step - loss: 0.2883 - accuracy: 1.0000 - val_loss: 1.2744 - val_accuracy: 0.8667\n",
      "Epoch 29/30\n",
      "3/3 [==============================] - 0s 103ms/step - loss: 0.2818 - accuracy: 1.0000 - val_loss: 1.3033 - val_accuracy: 0.8000\n",
      "Epoch 30/30\n",
      "3/3 [==============================] - 0s 91ms/step - loss: 0.2752 - accuracy: 1.0000 - val_loss: 1.3384 - val_accuracy: 0.8000\n",
      "Epoch 1/30\n",
      "3/3 [==============================] - 1s 157ms/step - loss: 0.2699 - accuracy: 1.0000 - val_loss: 1.3381 - val_accuracy: 0.8000\n",
      "Epoch 2/30\n",
      "3/3 [==============================] - 0s 95ms/step - loss: 0.2633 - accuracy: 1.0000 - val_loss: 1.3240 - val_accuracy: 0.8667\n",
      "Epoch 3/30\n",
      "3/3 [==============================] - 0s 105ms/step - loss: 0.2571 - accuracy: 1.0000 - val_loss: 1.2547 - val_accuracy: 0.8667\n",
      "Epoch 4/30\n",
      "3/3 [==============================] - 1s 202ms/step - loss: 0.2504 - accuracy: 1.0000 - val_loss: 1.2042 - val_accuracy: 0.8667\n",
      "Epoch 5/30\n",
      "3/3 [==============================] - 0s 144ms/step - loss: 0.2462 - accuracy: 1.0000 - val_loss: 1.1845 - val_accuracy: 0.8667\n",
      "Epoch 6/30\n",
      "3/3 [==============================] - 0s 108ms/step - loss: 0.2393 - accuracy: 1.0000 - val_loss: 1.1756 - val_accuracy: 0.8667\n",
      "Epoch 7/30\n",
      "3/3 [==============================] - 0s 117ms/step - loss: 0.2332 - accuracy: 1.0000 - val_loss: 1.1767 - val_accuracy: 0.8000\n",
      "Epoch 8/30\n",
      "3/3 [==============================] - 0s 208ms/step - loss: 0.2270 - accuracy: 1.0000 - val_loss: 1.1826 - val_accuracy: 0.8000\n",
      "Epoch 9/30\n",
      "3/3 [==============================] - 0s 104ms/step - loss: 0.2218 - accuracy: 1.0000 - val_loss: 1.1905 - val_accuracy: 0.8000\n",
      "Epoch 10/30\n",
      "3/3 [==============================] - 0s 95ms/step - loss: 0.2165 - accuracy: 1.0000 - val_loss: 1.1994 - val_accuracy: 0.8000\n",
      "Epoch 11/30\n",
      "3/3 [==============================] - 0s 77ms/step - loss: 0.2111 - accuracy: 1.0000 - val_loss: 1.2067 - val_accuracy: 0.8000\n",
      "Epoch 12/30\n",
      "3/3 [==============================] - 0s 70ms/step - loss: 0.2059 - accuracy: 1.0000 - val_loss: 1.2139 - val_accuracy: 0.8000\n",
      "Epoch 13/30\n",
      "3/3 [==============================] - 0s 73ms/step - loss: 0.2027 - accuracy: 1.0000 - val_loss: 1.2400 - val_accuracy: 0.8000\n",
      "Epoch 14/30\n",
      "3/3 [==============================] - 0s 65ms/step - loss: 0.1960 - accuracy: 1.0000 - val_loss: 1.2662 - val_accuracy: 0.8000\n",
      "Epoch 15/30\n",
      "3/3 [==============================] - 0s 66ms/step - loss: 0.1912 - accuracy: 1.0000 - val_loss: 1.2846 - val_accuracy: 0.8000\n",
      "Epoch 16/30\n",
      "3/3 [==============================] - 0s 62ms/step - loss: 0.1864 - accuracy: 1.0000 - val_loss: 1.2927 - val_accuracy: 0.8000\n",
      "Epoch 17/30\n",
      "3/3 [==============================] - 0s 62ms/step - loss: 0.1819 - accuracy: 1.0000 - val_loss: 1.2994 - val_accuracy: 0.8000\n",
      "Epoch 18/30\n",
      "3/3 [==============================] - 0s 62ms/step - loss: 0.1776 - accuracy: 1.0000 - val_loss: 1.3060 - val_accuracy: 0.8000\n",
      "Epoch 19/30\n",
      "3/3 [==============================] - 0s 61ms/step - loss: 0.1728 - accuracy: 1.0000 - val_loss: 1.3142 - val_accuracy: 0.8000\n",
      "Epoch 20/30\n",
      "3/3 [==============================] - 0s 65ms/step - loss: 0.1686 - accuracy: 1.0000 - val_loss: 1.3222 - val_accuracy: 0.8000\n",
      "Epoch 21/30\n",
      "3/3 [==============================] - 0s 66ms/step - loss: 0.1643 - accuracy: 1.0000 - val_loss: 1.3287 - val_accuracy: 0.8000\n",
      "Epoch 22/30\n",
      "3/3 [==============================] - 0s 60ms/step - loss: 0.1600 - accuracy: 1.0000 - val_loss: 1.3350 - val_accuracy: 0.8000\n",
      "Epoch 23/30\n",
      "3/3 [==============================] - 0s 66ms/step - loss: 0.1557 - accuracy: 1.0000 - val_loss: 1.3381 - val_accuracy: 0.8000\n",
      "Epoch 24/30\n",
      "3/3 [==============================] - 0s 70ms/step - loss: 0.1521 - accuracy: 1.0000 - val_loss: 1.3389 - val_accuracy: 0.8000\n",
      "Epoch 25/30\n",
      "3/3 [==============================] - 0s 69ms/step - loss: 0.1479 - accuracy: 1.0000 - val_loss: 1.3400 - val_accuracy: 0.8000\n",
      "Epoch 26/30\n",
      "3/3 [==============================] - 0s 62ms/step - loss: 0.1438 - accuracy: 1.0000 - val_loss: 1.3421 - val_accuracy: 0.8000\n",
      "Epoch 27/30\n",
      "3/3 [==============================] - 0s 64ms/step - loss: 0.1401 - accuracy: 1.0000 - val_loss: 1.3448 - val_accuracy: 0.8000\n",
      "Epoch 28/30\n",
      "3/3 [==============================] - 0s 64ms/step - loss: 0.1364 - accuracy: 1.0000 - val_loss: 1.3484 - val_accuracy: 0.8000\n",
      "Epoch 29/30\n",
      "3/3 [==============================] - 0s 59ms/step - loss: 0.1328 - accuracy: 1.0000 - val_loss: 1.3502 - val_accuracy: 0.8000\n",
      "Epoch 30/30\n",
      "3/3 [==============================] - 0s 72ms/step - loss: 0.1294 - accuracy: 1.0000 - val_loss: 1.3428 - val_accuracy: 0.8000\n"
     ]
    }
   ],
   "source": [
    "transformer_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "transformer_model.fit(X_train, Y_train, epochs=30, batch_size=32, validation_data=(X_test, Y_test))\n",
    "history = transformer_model.fit(X_train, Y_train, epochs=30, batch_size=32, validation_data=(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 51ms/step - loss: 1.3428 - accuracy: 0.8000\n",
      "Evaluation Metrics Transformer:\n",
      "loss: 1.3427586555480957\n",
      "accuracy: 0.800000011920929\n"
     ]
    }
   ],
   "source": [
    "evaluation_metrics_transformer = transformer_model.evaluate(X_test, Y_test)\n",
    "print(\"Evaluation Metrics Transformer:\")\n",
    "for metric_name, metric_value in zip(transformer_model.metrics_names, evaluation_metrics_transformer):\n",
    "    print(f\"{metric_name}: {metric_value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 52ms/step - loss: 1.3428 - accuracy: 0.8000\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "[[  0   0   0   0   0   0   0   0   0 162 163  10  12   3   4   2  16   1\n",
      "   15 164 165  28  10   2 166]\n",
      " [  0   0   0   0   0  83  17   2  63  11  18  33  45  10  27  25  23  97\n",
      "   98   5   3   4   2   1   6]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0 114   3  13  34  14\n",
      "    9   5   3   4   2   1   6]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  61 127  34  14\n",
      "   58   5   3   4   2   1   6]\n",
      " [  0   0   0   0   0   0   0   0   0   0 200  13  48  70 113  10   9   5\n",
      "    3   4   2   1   6  86  35]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   9   5   3   4\n",
      "    2   1   6  13  11  10  24]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   1  19  12   3   4   2  16\n",
      "    1  15   7  28  59  89 170]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0  99 153  28   2]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   28  10   2 120   7  80  24]\n",
      " [  0   0   0   0   0   0   0   7  41  42  44  45  29  14   1  43  71  72\n",
      "   53   5   3   4   2   1   6]\n",
      " [  0   0   0   0   0   0   1  19  12   9   5   3   4   2   1   6   7   8\n",
      "   18   7  21  13  65  33 139]\n",
      " [  0   0   0 177  13 178  17  10  38  39   4   2  16   1  15  51 179   2\n",
      "  112  25  23  36   7   8  67]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  24  11  29  35\n",
      "    9   5   3   4   2   1   6]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  11  13  20  76\n",
      "    9   5   3   4   2   1   6]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0  28  10   2 156   2 100]]\n",
      "\n",
      "Real and Predicted Values:\n",
      "    Real  Predicted\n",
      "0      0          0\n",
      "1      2          2\n",
      "2      2          2\n",
      "3      1          0\n",
      "4      2          2\n",
      "5      2          2\n",
      "6      0          0\n",
      "7      0          0\n",
      "8      0          1\n",
      "9      0          0\n",
      "10     2          2\n",
      "11     2          2\n",
      "12     2          1\n",
      "13     2          2\n",
      "14     0          0\n"
     ]
    }
   ],
   "source": [
    "evaluation_metrics_transformer = transformer_model.evaluate(X_test, Y_test)\n",
    "\n",
    "predictions = transformer_model.predict(X_test)\n",
    "print(X_test)\n",
    "\n",
    "# Convert one-hot encoded labels to class indices\n",
    "y_true = np.argmax(Y_test, axis=1)\n",
    "\n",
    "# Convert predicted probabilities to class indices\n",
    "y_pred = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Create DataFrame\n",
    "results_df = pd.DataFrame({\"Real\": y_true, \"Predicted\": y_pred})\n",
    "\n",
    "# Display DataFrame\n",
    "print(\"\\nReal and Predicted Values:\")\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy RNN: 0.9333333373069763\n",
      "Accuracy LSTM: 0.9333333373069763\n",
      "Accuracy Transformer: 0.800000011920929\n"
     ]
    }
   ],
   "source": [
    "# Evaluate RNN model\n",
    "rnn_accuracy = rnn_model.evaluate(X_test, Y_test, verbose=0)[1]\n",
    "print(\"Accuracy RNN:\", rnn_accuracy)\n",
    "\n",
    "# Evaluate LSTM model\n",
    "lstm_accuracy = lstm_model.evaluate(X_test, Y_test, verbose=0)[1]\n",
    "print(\"Accuracy LSTM:\", lstm_accuracy)\n",
    "\n",
    "# Evaluate Transformer model\n",
    "transformer_accuracy = transformer_model.evaluate(X_test, Y_test, verbose=0)[1]\n",
    "print(\"Accuracy Transformer:\", transformer_accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model (RNN) with accuracy 0.9333333373069763 has been saved to './savedModels/q4_model.h5'\n"
     ]
    }
   ],
   "source": [
    "# Determine the best model\n",
    "best_model_name, best_model_accuracy = max([('RNN', rnn_accuracy), ('LSTM', lstm_accuracy), ('Transformer', transformer_accuracy)], key=lambda x: x[1])\n",
    "\n",
    "save_path = './savedModels/q4_model.h5'\n",
    "# Save the best model\n",
    "if best_model_name == 'RNN':\n",
    "    rnn_model.save(save_path)\n",
    "elif best_model_name == 'LSTM':\n",
    "    lstm_model.save(save_path)\n",
    "elif best_model_name == 'Transformer':\n",
    "    transformer_model.save(save_path)\n",
    "\n",
    "print(f\"The best model ({best_model_name}) with accuracy {best_model_accuracy} has been saved to '{save_path}'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
