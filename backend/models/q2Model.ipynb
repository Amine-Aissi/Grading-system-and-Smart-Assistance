{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow as keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.regularizers import l2\n",
    "from keras import layers\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Embedding, SimpleRNN, Dense, Dropout, LSTM, Bidirectional, Input\n",
    "from keras.layers import Attention, GlobalAveragePooling1D\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import stanza\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import BatchNormalization, Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*<h3>Load Data<h3>*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_id</th>\n",
       "      <th>answer</th>\n",
       "      <th>grade</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>نزل ميكائيل عليه السلام على الرسول في أول وحي ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>في عتمة الغار، تلقى الرسول نفحاتِ الوحي، كلمات...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>في غارِ حراء، ظهر للرسول ملاكٌ يُغيّرُ مجرىَ ا...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>من وراء حجاب النور، تكلّم ملاكٌ بكلام الله، فح...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>في رحلةٍ من التأمل والعزلة، اختصَ اللهُ عزَّ و...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>عهدٍ إلهيٍ محمولٍ على أجنحة الملائكة، تلقى الر...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>ظهر للرسول في أول وحي له نوح عليه السلام.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>ظهر ميكائيل للنبي محمد في الوحي الأول.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>لحظةٌ مقدسة، نورٌ يهبطُ من عليٍ، ملاكٌ يحملُ أ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>ظهر جبريل عليه السلام للنبي محمد في الوحي الأول.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   question_id                                             answer  grade\n",
       "0            2  نزل ميكائيل عليه السلام على الرسول في أول وحي ...      0\n",
       "1            2  في عتمة الغار، تلقى الرسول نفحاتِ الوحي، كلمات...      1\n",
       "2            2  في غارِ حراء، ظهر للرسول ملاكٌ يُغيّرُ مجرىَ ا...      1\n",
       "3            2  من وراء حجاب النور، تكلّم ملاكٌ بكلام الله، فح...      1\n",
       "4            2  في رحلةٍ من التأمل والعزلة، اختصَ اللهُ عزَّ و...      2\n",
       "5            2  عهدٍ إلهيٍ محمولٍ على أجنحة الملائكة، تلقى الر...      1\n",
       "6            2          ظهر للرسول في أول وحي له نوح عليه السلام.      0\n",
       "7            2             ظهر ميكائيل للنبي محمد في الوحي الأول.      0\n",
       "8            2  لحظةٌ مقدسة، نورٌ يهبطُ من عليٍ، ملاكٌ يحملُ أ...      1\n",
       "9            2   ظهر جبريل عليه السلام للنبي محمد في الوحي الأول.      2"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read csv file into pandas\n",
    "df = pd.read_csv(\"../datasets/shuffled2.csv\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*<h3>EDA<h3>*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 90 entries, 0 to 89\n",
      "Data columns (total 3 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   question_id  90 non-null     int64 \n",
      " 1   answer       90 non-null     object\n",
      " 2   grade        90 non-null     int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 2.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "grade\n",
       "0    29\n",
       "1    28\n",
       "2    33\n",
       "dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('grade').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(data=df, x='grade')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*<h3>Data Cleaning<h3>*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('question_id', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     نزل ميكائيل عليه السلام على الرسول في أول وحي ...\n",
      "1     في عتمة الغار، تلقى الرسول نفحاتِ الوحي، كلمات...\n",
      "2     في غارِ حراء، ظهر للرسول ملاكٌ يُغيّرُ مجرىَ ا...\n",
      "3     من وراء حجاب النور، تكلّم ملاكٌ بكلام الله، فح...\n",
      "4     في رحلةٍ من التأمل والعزلة، اختصَ اللهُ عزَّ و...\n",
      "                            ...                        \n",
      "85    مع نزول جبريل عليه السلام على الرسول في أول وح...\n",
      "86               ظهر إسرافيل للنبي محمد في الوحي الأول.\n",
      "87        أوصل أحد الملائكة رسالة الله تعالى إلى الرسول\n",
      "88                            الملاك جبريل عليه السلام \n",
      "89    في ليلة القدر، ظهر للرسول في أول وحي له جبريل ...\n",
      "Name: answer, Length: 90, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df.drop_duplicates(inplace=True)\n",
    "print(df['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*<h3>Data Pre-Preocessing<h3>*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f0d0c4767714f708d585227ef3a8078",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.7.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-09 22:25:02 INFO: Downloading default packages for language: ar (Arabic) ...\n",
      "2024-01-09 22:25:03 INFO: File exists: C:\\Users\\amine\\stanza_resources\\ar\\default.zip\n",
      "2024-01-09 22:25:07 INFO: Finished downloading models and saved to C:\\Users\\amine\\stanza_resources.\n",
      "2024-01-09 22:25:07 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d15494c337146c79c57e5f81452d538",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.7.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-09 22:25:09 INFO: Loading these models for language: ar (Arabic):\n",
      "=============================\n",
      "| Processor | Package       |\n",
      "-----------------------------\n",
      "| tokenize  | padt          |\n",
      "| mwt       | padt          |\n",
      "| pos       | padt_charlm   |\n",
      "| lemma     | padt_nocharlm |\n",
      "| depparse  | padt_charlm   |\n",
      "| ner       | aqmar_charlm  |\n",
      "=============================\n",
      "\n",
      "2024-01-09 22:25:09 INFO: Using device: cpu\n",
      "2024-01-09 22:25:09 INFO: Loading: tokenize\n",
      "2024-01-09 22:25:09 INFO: Loading: mwt\n",
      "2024-01-09 22:25:09 INFO: Loading: pos\n",
      "2024-01-09 22:25:10 INFO: Loading: lemma\n",
      "2024-01-09 22:25:10 INFO: Loading: depparse\n",
      "2024-01-09 22:25:10 INFO: Loading: ner\n",
      "2024-01-09 22:25:11 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "df['grade'] = le.fit_transform(df['grade'])\n",
    "\n",
    "stanza.download('ar')\n",
    "nlp = stanza.Pipeline('ar')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    doc = nlp(text)\n",
    "    tokens = [word.lemma for sent in doc.sentences for word in sent.words if word.upos != 'PUNCT']\n",
    "    return tokens\n",
    "\n",
    "df['answer'] = df['answer'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0 ...  69 101  46]\n",
      " [  0   0   0 ...   3 107  70]\n",
      " [  0   0   0 ... 109 110 111]\n",
      " ...\n",
      " [  0   0   0 ...  37  35   7]\n",
      " [  0   0   0 ...   3   1   6]\n",
      " [  0   0   0 ...   1  10  15]]\n",
      "0     [نَزَل, مِيكَائِيل, عَلَى, هُوَ, سَلَام, عَلَى...\n",
      "1     [فِي, عتمة, غَار, تَلَقَّى, رَسُول, نفحاتِ, وَ...\n",
      "2     [فِي, غارِ, حراء, ظَهَر, لِ, رَسُول, ملاكٌ, أَ...\n",
      "3     [مِن, وَرَاءَ, حِجَاب, النور, كَلَّم, ملاكٌ, ب...\n",
      "4     [فِي, رِحلَة, ٍِ, مِن, تَأَمُّل, وَ, عُزلَة, ا...\n",
      "                            ...                        \n",
      "85    [مَعَ, نُزُول, جبريل, عَلَى, هُوَ, سَلَام, عَل...\n",
      "86    [ظَهَر, إسرافيل, لِ, نَبِيّ, محمد, فِي, وَحي, ...\n",
      "87    [أَوصَل, أَحَد, مَلأَك, رِسَالَة, اَللّٰه, تَع...\n",
      "88                 [مَالِك, جبريل, عَلَى, هُوَ, سَلَام]\n",
      "89    [فِي, لَيلَة, قَدر, ظَهَر, لِ, رَسُول, فِي, أَ...\n",
      "Name: answer, Length: 90, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Tokenization\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df['answer'])\n",
    "sequences = tokenizer.texts_to_sequences(df['answer'])\n",
    "max_sequence_length = max(len(s) for s in sequences)\n",
    "sequences = pad_sequences(sequences,max_sequence_length)\n",
    "word2idx = tokenizer.word_index\n",
    "vocab_size = len(word2idx) + 1\n",
    "\n",
    "\n",
    "X = pad_sequences(sequences, padding='post', truncating='post', maxlen=max_sequence_length)\n",
    "\n",
    "print(sequences)\n",
    "print(df['answer'])\n",
    "\n",
    "Y = to_categorical(df['grade'], num_classes=3)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.15, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*<h3>build Models<h3>*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*<h4>RNN Model<h4>*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2/2 [==============================] - 4s 594ms/step - loss: 1.3454 - accuracy: 0.3194 - val_loss: 1.2022 - val_accuracy: 0.2778 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 1.2001 - accuracy: 0.3750 - val_loss: 1.1753 - val_accuracy: 0.4444 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 1.2602 - accuracy: 0.3611 - val_loss: 1.1857 - val_accuracy: 0.4444 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 1.2513 - accuracy: 0.4375\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 1.2318 - accuracy: 0.4583 - val_loss: 1.1960 - val_accuracy: 0.4444 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 1.1600 - accuracy: 0.4861 - val_loss: 1.1995 - val_accuracy: 0.2778 - lr: 2.0000e-04\n",
      "Epoch 6/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 1.1487 - accuracy: 0.5000\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0001.\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 1.1661 - accuracy: 0.4861 - val_loss: 1.2001 - val_accuracy: 0.2778 - lr: 2.0000e-04\n",
      "Epoch 7/100\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 1.1906 - accuracy: 0.4583 - val_loss: 1.1993 - val_accuracy: 0.2778 - lr: 1.0000e-04\n",
      "Epoch 8/100\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 1.1627 - accuracy: 0.4167 - val_loss: 1.1979 - val_accuracy: 0.2778 - lr: 1.0000e-04\n",
      "Epoch 9/100\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 1.1539 - accuracy: 0.4167 - val_loss: 1.1967 - val_accuracy: 0.2778 - lr: 1.0000e-04\n",
      "Epoch 10/100\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 1.2279 - accuracy: 0.3472 - val_loss: 1.1961 - val_accuracy: 0.2778 - lr: 1.0000e-04\n",
      "Epoch 11/100\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 1.1709 - accuracy: 0.4167 - val_loss: 1.1954 - val_accuracy: 0.2778 - lr: 1.0000e-04\n",
      "Epoch 12/100\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 1.1274 - accuracy: 0.4167 - val_loss: 1.1945 - val_accuracy: 0.2778 - lr: 1.0000e-04\n",
      "Epoch 13/100\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 1.1145 - accuracy: 0.5417 - val_loss: 1.1936 - val_accuracy: 0.2778 - lr: 1.0000e-04\n",
      "Epoch 14/100\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 1.1454 - accuracy: 0.4861 - val_loss: 1.1933 - val_accuracy: 0.2778 - lr: 1.0000e-04\n",
      "Epoch 15/100\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 1.1523 - accuracy: 0.4444 - val_loss: 1.1930 - val_accuracy: 0.2778 - lr: 1.0000e-04\n",
      "Epoch 16/100\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 1.1167 - accuracy: 0.5000 - val_loss: 1.1919 - val_accuracy: 0.2778 - lr: 1.0000e-04\n",
      "Epoch 17/100\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 1.0906 - accuracy: 0.5139 - val_loss: 1.1894 - val_accuracy: 0.5000 - lr: 1.0000e-04\n",
      "Epoch 18/100\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 1.1092 - accuracy: 0.5556 - val_loss: 1.1869 - val_accuracy: 0.4444 - lr: 1.0000e-04\n",
      "Epoch 19/100\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 1.1110 - accuracy: 0.5139 - val_loss: 1.1856 - val_accuracy: 0.4444 - lr: 1.0000e-04\n",
      "Epoch 20/100\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 1.1436 - accuracy: 0.4861 - val_loss: 1.1846 - val_accuracy: 0.4444 - lr: 1.0000e-04\n",
      "Epoch 21/100\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 1.1129 - accuracy: 0.5000 - val_loss: 1.1834 - val_accuracy: 0.4444 - lr: 1.0000e-04\n",
      "Epoch 22/100\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 1.0588 - accuracy: 0.5694 - val_loss: 1.1823 - val_accuracy: 0.4444 - lr: 1.0000e-04\n",
      "Epoch 23/100\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 1.1584 - accuracy: 0.4444 - val_loss: 1.1815 - val_accuracy: 0.4444 - lr: 1.0000e-04\n",
      "Epoch 24/100\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 1.1076 - accuracy: 0.4306 - val_loss: 1.1808 - val_accuracy: 0.4444 - lr: 1.0000e-04\n",
      "Epoch 25/100\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 1.0876 - accuracy: 0.5417 - val_loss: 1.1800 - val_accuracy: 0.4444 - lr: 1.0000e-04\n",
      "Epoch 26/100\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 1.0941 - accuracy: 0.5139 - val_loss: 1.1796 - val_accuracy: 0.4444 - lr: 1.0000e-04\n",
      "Epoch 27/100\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 1.0671 - accuracy: 0.5139 - val_loss: 1.1794 - val_accuracy: 0.4444 - lr: 1.0000e-04\n",
      "Epoch 28/100\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 1.1102 - accuracy: 0.4444 - val_loss: 1.1791 - val_accuracy: 0.4444 - lr: 1.0000e-04\n",
      "Epoch 29/100\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 1.0814 - accuracy: 0.5000 - val_loss: 1.1787 - val_accuracy: 0.4444 - lr: 1.0000e-04\n",
      "Epoch 30/100\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 1.0600 - accuracy: 0.5833 - val_loss: 1.1778 - val_accuracy: 0.4444 - lr: 1.0000e-04\n",
      "Epoch 31/100\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 1.1362 - accuracy: 0.4722 - val_loss: 1.1765 - val_accuracy: 0.4444 - lr: 1.0000e-04\n",
      "Epoch 32/100\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 1.0273 - accuracy: 0.6667 - val_loss: 1.1753 - val_accuracy: 0.4444 - lr: 1.0000e-04\n",
      "Epoch 33/100\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 1.0514 - accuracy: 0.5417 - val_loss: 1.1741 - val_accuracy: 0.4444 - lr: 1.0000e-04\n",
      "Epoch 34/100\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 1.0346 - accuracy: 0.5556 - val_loss: 1.1729 - val_accuracy: 0.4444 - lr: 1.0000e-04\n",
      "Epoch 35/100\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 1.0482 - accuracy: 0.5833 - val_loss: 1.1719 - val_accuracy: 0.4444 - lr: 1.0000e-04\n",
      "Epoch 36/100\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.9731 - accuracy: 0.6806 - val_loss: 1.1712 - val_accuracy: 0.4444 - lr: 1.0000e-04\n",
      "Epoch 37/100\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 1.0400 - accuracy: 0.6111 - val_loss: 1.1710 - val_accuracy: 0.4444 - lr: 1.0000e-04\n",
      "Epoch 38/100\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 1.0061 - accuracy: 0.6389 - val_loss: 1.1710 - val_accuracy: 0.4444 - lr: 1.0000e-04\n",
      "Epoch 39/100\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.9819 - accuracy: 0.6389 - val_loss: 1.1715 - val_accuracy: 0.4444 - lr: 1.0000e-04\n",
      "Epoch 40/100\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 1.0277 - accuracy: 0.5972 - val_loss: 1.1722 - val_accuracy: 0.4444 - lr: 1.0000e-04\n",
      "Epoch 41/100\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.9948 - accuracy: 0.5556 - val_loss: 1.1732 - val_accuracy: 0.4444 - lr: 1.0000e-04\n",
      "Epoch 42/100\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.9498 - accuracy: 0.7083 - val_loss: 1.1742 - val_accuracy: 0.4444 - lr: 1.0000e-04\n",
      "Epoch 43/100\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.9992 - accuracy: 0.6389 - val_loss: 1.1749 - val_accuracy: 0.4444 - lr: 1.0000e-04\n",
      "Epoch 44/100\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.9634 - accuracy: 0.6944 - val_loss: 1.1752 - val_accuracy: 0.4444 - lr: 1.0000e-04\n",
      "Epoch 45/100\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.9855 - accuracy: 0.5833 - val_loss: 1.1750 - val_accuracy: 0.4444 - lr: 1.0000e-04\n",
      "Epoch 46/100\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.9712 - accuracy: 0.6389 - val_loss: 1.1752 - val_accuracy: 0.4444 - lr: 1.0000e-04\n",
      "Epoch 47/100\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.9995 - accuracy: 0.6250 - val_loss: 1.1756 - val_accuracy: 0.4444 - lr: 1.0000e-04\n",
      "Epoch 48/100\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.9705 - accuracy: 0.7083 - val_loss: 1.1759 - val_accuracy: 0.4444 - lr: 1.0000e-04\n",
      "Epoch 49/100\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.8722 - accuracy: 0.7361 - val_loss: 1.1762 - val_accuracy: 0.4444 - lr: 1.0000e-04\n",
      "Epoch 50/100\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.9400 - accuracy: 0.7222 - val_loss: 1.1764 - val_accuracy: 0.4444 - lr: 1.0000e-04\n",
      "Epoch 51/100\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.9393 - accuracy: 0.7083 - val_loss: 1.1764 - val_accuracy: 0.4444 - lr: 1.0000e-04\n",
      "Epoch 52/100\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 0.8959 - accuracy: 0.7222 - val_loss: 1.1762 - val_accuracy: 0.4444 - lr: 1.0000e-04\n",
      "Epoch 53/100\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.8746 - accuracy: 0.7639 - val_loss: 1.1758 - val_accuracy: 0.4444 - lr: 1.0000e-04\n",
      "Epoch 54/100\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.8767 - accuracy: 0.7778 - val_loss: 1.1748 - val_accuracy: 0.4444 - lr: 1.0000e-04\n",
      "Epoch 55/100\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.8806 - accuracy: 0.8056 - val_loss: 1.1738 - val_accuracy: 0.4444 - lr: 1.0000e-04\n",
      "Epoch 56/100\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.9161 - accuracy: 0.6806 - val_loss: 1.1729 - val_accuracy: 0.4444 - lr: 1.0000e-04\n",
      "Epoch 57/100\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.8652 - accuracy: 0.7222 - val_loss: 1.1718 - val_accuracy: 0.4444 - lr: 1.0000e-04\n",
      "Epoch 58/100\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.8693 - accuracy: 0.7500 - val_loss: 1.1705 - val_accuracy: 0.4444 - lr: 1.0000e-04\n",
      "Epoch 59/100\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.8812 - accuracy: 0.7222 - val_loss: 1.1694 - val_accuracy: 0.4444 - lr: 1.0000e-04\n",
      "Epoch 60/100\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.9059 - accuracy: 0.7500 - val_loss: 1.1686 - val_accuracy: 0.4444 - lr: 1.0000e-04\n",
      "Epoch 61/100\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.8842 - accuracy: 0.7361 - val_loss: 1.1679 - val_accuracy: 0.4444 - lr: 1.0000e-04\n",
      "Epoch 62/100\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.8736 - accuracy: 0.7361 - val_loss: 1.1668 - val_accuracy: 0.4444 - lr: 1.0000e-04\n",
      "Epoch 63/100\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.8017 - accuracy: 0.8194 - val_loss: 1.1652 - val_accuracy: 0.4444 - lr: 1.0000e-04\n",
      "Epoch 64/100\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.8411 - accuracy: 0.8333 - val_loss: 1.1633 - val_accuracy: 0.4444 - lr: 1.0000e-04\n",
      "Epoch 65/100\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.8790 - accuracy: 0.7361 - val_loss: 1.1606 - val_accuracy: 0.4444 - lr: 1.0000e-04\n",
      "Epoch 66/100\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.8258 - accuracy: 0.7917 - val_loss: 1.1578 - val_accuracy: 0.4444 - lr: 1.0000e-04\n",
      "Epoch 67/100\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.8314 - accuracy: 0.7361 - val_loss: 1.1551 - val_accuracy: 0.4444 - lr: 1.0000e-04\n",
      "Epoch 68/100\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.8468 - accuracy: 0.7361 - val_loss: 1.1528 - val_accuracy: 0.4444 - lr: 1.0000e-04\n",
      "Epoch 69/100\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.8004 - accuracy: 0.7917 - val_loss: 1.1508 - val_accuracy: 0.4444 - lr: 1.0000e-04\n",
      "Epoch 70/100\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.8427 - accuracy: 0.7500 - val_loss: 1.1489 - val_accuracy: 0.4444 - lr: 1.0000e-04\n",
      "Epoch 71/100\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.8181 - accuracy: 0.7778 - val_loss: 1.1470 - val_accuracy: 0.4444 - lr: 1.0000e-04\n",
      "Epoch 72/100\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.7933 - accuracy: 0.7917 - val_loss: 1.1448 - val_accuracy: 0.4444 - lr: 1.0000e-04\n",
      "Epoch 73/100\n",
      "2/2 [==============================] - 0s 110ms/step - loss: 0.7641 - accuracy: 0.7778 - val_loss: 1.1431 - val_accuracy: 0.4444 - lr: 1.0000e-04\n",
      "Epoch 74/100\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.7930 - accuracy: 0.7778 - val_loss: 1.1423 - val_accuracy: 0.4444 - lr: 1.0000e-04\n",
      "Epoch 75/100\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.7128 - accuracy: 0.8472 - val_loss: 1.1415 - val_accuracy: 0.4444 - lr: 1.0000e-04\n",
      "Epoch 76/100\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.7558 - accuracy: 0.8611 - val_loss: 1.1402 - val_accuracy: 0.4444 - lr: 1.0000e-04\n",
      "Epoch 77/100\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.7072 - accuracy: 0.8889 - val_loss: 1.1384 - val_accuracy: 0.4444 - lr: 1.0000e-04\n",
      "Epoch 78/100\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.7369 - accuracy: 0.8472 - val_loss: 1.1370 - val_accuracy: 0.4444 - lr: 1.0000e-04\n",
      "Epoch 79/100\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.7300 - accuracy: 0.8194 - val_loss: 1.1363 - val_accuracy: 0.5000 - lr: 1.0000e-04\n",
      "Epoch 80/100\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.6443 - accuracy: 0.9028 - val_loss: 1.1352 - val_accuracy: 0.5000 - lr: 1.0000e-04\n",
      "Epoch 81/100\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.6849 - accuracy: 0.9167 - val_loss: 1.1337 - val_accuracy: 0.5000 - lr: 1.0000e-04\n",
      "Epoch 82/100\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.6910 - accuracy: 0.8750 - val_loss: 1.1315 - val_accuracy: 0.5000 - lr: 1.0000e-04\n",
      "Epoch 83/100\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.6660 - accuracy: 0.9306 - val_loss: 1.1292 - val_accuracy: 0.5000 - lr: 1.0000e-04\n",
      "Epoch 84/100\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.6934 - accuracy: 0.8750 - val_loss: 1.1265 - val_accuracy: 0.5000 - lr: 1.0000e-04\n",
      "Epoch 85/100\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.6649 - accuracy: 0.9167 - val_loss: 1.1234 - val_accuracy: 0.5000 - lr: 1.0000e-04\n",
      "Epoch 86/100\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.6634 - accuracy: 0.8194 - val_loss: 1.1205 - val_accuracy: 0.5000 - lr: 1.0000e-04\n",
      "Epoch 87/100\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.6532 - accuracy: 0.8750 - val_loss: 1.1177 - val_accuracy: 0.5000 - lr: 1.0000e-04\n",
      "Epoch 88/100\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.6464 - accuracy: 0.9167 - val_loss: 1.1155 - val_accuracy: 0.5000 - lr: 1.0000e-04\n",
      "Epoch 89/100\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.5916 - accuracy: 0.9722 - val_loss: 1.1120 - val_accuracy: 0.5000 - lr: 1.0000e-04\n",
      "Epoch 90/100\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.6351 - accuracy: 0.9028 - val_loss: 1.1090 - val_accuracy: 0.5000 - lr: 1.0000e-04\n",
      "Epoch 91/100\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.6290 - accuracy: 0.9167 - val_loss: 1.1081 - val_accuracy: 0.4444 - lr: 1.0000e-04\n",
      "Epoch 92/100\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.5989 - accuracy: 0.9444 - val_loss: 1.1059 - val_accuracy: 0.5000 - lr: 1.0000e-04\n",
      "Epoch 93/100\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.6161 - accuracy: 0.9306 - val_loss: 1.1010 - val_accuracy: 0.5000 - lr: 1.0000e-04\n",
      "Epoch 94/100\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.6395 - accuracy: 0.9028 - val_loss: 1.0966 - val_accuracy: 0.6111 - lr: 1.0000e-04\n",
      "Epoch 95/100\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.6306 - accuracy: 0.9028 - val_loss: 1.0953 - val_accuracy: 0.6111 - lr: 1.0000e-04\n",
      "Epoch 96/100\n",
      "2/2 [==============================] - 0s 95ms/step - loss: 0.5989 - accuracy: 0.9306 - val_loss: 1.0961 - val_accuracy: 0.6111 - lr: 1.0000e-04\n",
      "Epoch 97/100\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.5956 - accuracy: 0.9028 - val_loss: 1.0948 - val_accuracy: 0.6111 - lr: 1.0000e-04\n",
      "Epoch 98/100\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.5831 - accuracy: 0.9444 - val_loss: 1.0896 - val_accuracy: 0.6111 - lr: 1.0000e-04\n",
      "Epoch 99/100\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.5840 - accuracy: 0.9444 - val_loss: 1.0876 - val_accuracy: 0.6111 - lr: 1.0000e-04\n",
      "Epoch 100/100\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.5582 - accuracy: 0.9306 - val_loss: 1.0849 - val_accuracy: 0.6111 - lr: 1.0000e-04\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 1.0663 - accuracy: 0.6429\n",
      "Evaluation Metrics for RNN:\n",
      "loss: 1.0663355588912964\n",
      "accuracy: 0.6428571343421936\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "\n",
    "X_train_rnn, X_test_rnn, y_train_rnn, y_test_rnn = train_test_split(X, Y, test_size=0.2)\n",
    "\n",
    "def RNN_model(vocab_size, sequence_length):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=vocab_size, output_dim=128, input_length=sequence_length))\n",
    "    model.add(SimpleRNN(units=128, activation='relu', return_sequences=True))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(SimpleRNN(units=64, activation='sigmoid'))\n",
    "    model.add(Dense(256, activation='sigmoid', kernel_regularizer=l2(0.001)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "    optimizer = Adam(learning_rate=0.001)\n",
    "    \n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=0.0001, verbose=1)\n",
    "#EMBEDDING_DIM = 110\n",
    "rnn_model = RNN_model(vocab_size, max_sequence_length)\n",
    "early_stopping_rnn = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "history_updated_rnn = rnn_model.fit(X_train_rnn, y_train_rnn, validation_data=(X_test_rnn, y_test_rnn), epochs=100, batch_size=64, callbacks=[reduce_lr])\n",
    "\n",
    "# Evaluate the RNN model\n",
    "evaluation_metrics_updated_rnn = rnn_model.evaluate(X_test, Y_test)\n",
    "print(\"Evaluation Metrics for RNN:\")\n",
    "for metric_name, metric_value in zip(rnn_model.metrics_names, evaluation_metrics_updated_rnn):\n",
    "    print(f\"{metric_name}: {metric_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*<h4>LSTM Model<h4>*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2/2 [==============================] - 6s 835ms/step - loss: 1.1993 - accuracy: 0.2778 - val_loss: 1.1975 - val_accuracy: 0.4444\n",
      "Epoch 2/100\n",
      "2/2 [==============================] - 0s 106ms/step - loss: 1.1639 - accuracy: 0.5833 - val_loss: 1.1946 - val_accuracy: 0.3889\n",
      "Epoch 3/100\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 1.1198 - accuracy: 0.6944 - val_loss: 1.1911 - val_accuracy: 0.6111\n",
      "Epoch 4/100\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 1.0427 - accuracy: 0.7778 - val_loss: 1.1878 - val_accuracy: 0.5000\n",
      "Epoch 5/100\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 0.9532 - accuracy: 0.6667 - val_loss: 1.1823 - val_accuracy: 0.6667\n",
      "Epoch 6/100\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 0.8070 - accuracy: 0.7500 - val_loss: 1.1745 - val_accuracy: 0.6667\n",
      "Epoch 7/100\n",
      "2/2 [==============================] - 0s 115ms/step - loss: 0.6941 - accuracy: 0.7917 - val_loss: 1.1663 - val_accuracy: 0.6667\n",
      "Epoch 8/100\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 0.5807 - accuracy: 0.8611 - val_loss: 1.1598 - val_accuracy: 0.8333\n",
      "Epoch 9/100\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.4760 - accuracy: 0.8750 - val_loss: 1.1533 - val_accuracy: 0.8333\n",
      "Epoch 10/100\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.4040 - accuracy: 0.8889 - val_loss: 1.1467 - val_accuracy: 0.8333\n",
      "Epoch 11/100\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.3604 - accuracy: 0.9028 - val_loss: 1.1427 - val_accuracy: 0.7222\n",
      "Epoch 12/100\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 1.2260 - accuracy: 0.8889 - val_loss: 1.1395 - val_accuracy: 0.6667\n",
      "Epoch 13/100\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.2531 - accuracy: 0.9583 - val_loss: 1.1382 - val_accuracy: 0.6111\n",
      "Epoch 14/100\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.2370 - accuracy: 0.9444 - val_loss: 1.1362 - val_accuracy: 0.6111\n",
      "Epoch 15/100\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.4556 - accuracy: 0.9444 - val_loss: 1.1347 - val_accuracy: 0.6111\n",
      "Epoch 16/100\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.1890 - accuracy: 0.9722 - val_loss: 1.1328 - val_accuracy: 0.6111\n",
      "Epoch 17/100\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 0.1805 - accuracy: 0.9722 - val_loss: 1.1309 - val_accuracy: 0.6111\n",
      "Epoch 18/100\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 0.1720 - accuracy: 0.9861 - val_loss: 1.1287 - val_accuracy: 0.6667\n",
      "Epoch 19/100\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 0.1823 - accuracy: 0.9722 - val_loss: 1.1271 - val_accuracy: 0.6111\n",
      "Epoch 20/100\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 0.1623 - accuracy: 0.9722 - val_loss: 1.1262 - val_accuracy: 0.5556\n",
      "Epoch 21/100\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.1311 - accuracy: 0.9861 - val_loss: 1.1254 - val_accuracy: 0.6111\n",
      "Epoch 22/100\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 0.1168 - accuracy: 1.0000 - val_loss: 1.1242 - val_accuracy: 0.5556\n",
      "Epoch 23/100\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 0.1034 - accuracy: 1.0000 - val_loss: 1.1222 - val_accuracy: 0.5556\n",
      "Epoch 24/100\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.1442 - accuracy: 0.9861 - val_loss: 1.1157 - val_accuracy: 0.6111\n",
      "Epoch 25/100\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.1050 - accuracy: 0.9861 - val_loss: 1.1087 - val_accuracy: 0.5556\n",
      "Epoch 26/100\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.1396 - accuracy: 0.9722 - val_loss: 1.1054 - val_accuracy: 0.6111\n",
      "Epoch 27/100\n",
      "2/2 [==============================] - 0s 124ms/step - loss: 0.1275 - accuracy: 0.9722 - val_loss: 1.1046 - val_accuracy: 0.6667\n",
      "Epoch 28/100\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.1179 - accuracy: 0.9722 - val_loss: 1.1046 - val_accuracy: 0.6111\n",
      "Epoch 29/100\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.1029 - accuracy: 1.0000 - val_loss: 1.1047 - val_accuracy: 0.6111\n",
      "Epoch 30/100\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.1051 - accuracy: 0.9861 - val_loss: 1.1053 - val_accuracy: 0.6111\n",
      "Epoch 31/100\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.0986 - accuracy: 1.0000 - val_loss: 1.1060 - val_accuracy: 0.6111\n",
      "Epoch 32/100\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.1157 - accuracy: 0.9861 - val_loss: 1.0976 - val_accuracy: 0.6111\n",
      "Epoch 33/100\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.1000 - accuracy: 1.0000 - val_loss: 1.0856 - val_accuracy: 0.7778\n",
      "Epoch 34/100\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.6742 - accuracy: 0.9028 - val_loss: 1.0893 - val_accuracy: 0.6667\n",
      "Epoch 35/100\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.0934 - accuracy: 1.0000 - val_loss: 1.0981 - val_accuracy: 0.6111\n",
      "Epoch 36/100\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.1009 - accuracy: 1.0000 - val_loss: 1.1072 - val_accuracy: 0.6111\n",
      "Epoch 37/100\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.2211 - accuracy: 0.9583 - val_loss: 1.1122 - val_accuracy: 0.6111\n",
      "Epoch 38/100\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.4525 - accuracy: 0.9306 - val_loss: 1.1084 - val_accuracy: 0.6111\n",
      "Epoch 39/100\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.1177 - accuracy: 1.0000 - val_loss: 1.1049 - val_accuracy: 0.6111\n",
      "Epoch 40/100\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.1018 - accuracy: 1.0000 - val_loss: 1.1016 - val_accuracy: 0.6111\n",
      "Epoch 41/100\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.1026 - accuracy: 1.0000 - val_loss: 1.0983 - val_accuracy: 0.6111\n",
      "Epoch 42/100\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.0994 - accuracy: 1.0000 - val_loss: 1.0944 - val_accuracy: 0.6667\n",
      "Epoch 43/100\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 0.0963 - accuracy: 1.0000 - val_loss: 1.0904 - val_accuracy: 0.6667\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 1.0527 - accuracy: 0.9286\n",
      "Evaluation Metrics for LSTM:\n",
      "loss: 1.052721381187439\n",
      "accuracy: 0.9285714030265808\n"
     ]
    }
   ],
   "source": [
    "def LSTM_model(vocab_size, sequence_length):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=vocab_size, output_dim=128, input_length=sequence_length))\n",
    "    model.add(LSTM(units=128, activation='relu', return_sequences=True))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LSTM(units=64, activation='relu'))\n",
    "    model.add(Dense(256, activation='relu', kernel_regularizer=l2(0.001)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "    optimizer = Adam(learning_rate=0.001)\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "#EMBEDDING_DIM = 110\n",
    "lstm_model = LSTM_model(vocab_size, max_sequence_length)\n",
    "early_stopping_rnn = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "history_updated_rnn = lstm_model.fit(X_train_rnn, y_train_rnn, validation_data=(X_test_rnn, y_test_rnn), epochs=100, batch_size=64, callbacks=[early_stopping_rnn])\n",
    "\n",
    "\n",
    "# Evaluate the lstm model\n",
    "evaluation_metrics_updated_lstm = lstm_model.evaluate(X_test, Y_test)\n",
    "print(\"Evaluation Metrics for LSTM:\")\n",
    "for metric_name, metric_value in zip(lstm_model.metrics_names, evaluation_metrics_updated_lstm):\n",
    "    print(f\"{metric_name}: {metric_value}\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*<h4>TRANSFORMER Model<h4>*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, density, rate=0.1, l2_reg=0.01):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            layers.Dense(density, activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(l2_reg)),\n",
    "            layers.Dense(embed_dim, kernel_regularizer=tf.keras.regularizers.l2(l2_reg)),\n",
    "        ])\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "\n",
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions\n",
    "\n",
    "def build_transformer_model(maxlen, vocab_size, embed_dim, num_heads, density, transformer_units, mlp_units, dropout_rate, num_classes):\n",
    "    inputs = layers.Input(shape=(maxlen,))\n",
    "    embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
    "    x = embedding_layer(inputs)\n",
    "    for _ in range(transformer_units):\n",
    "        x = TransformerBlock(embed_dim, num_heads, density, rate=dropout_rate)(x)\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    for dim in mlp_units:\n",
    "        x = layers.Dense(dim, activation=\"relu\")(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "    outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = max_sequence_length\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "embed_dim = 120\n",
    "num_heads = 2\n",
    "density = 3\n",
    "transformer_units = 4\n",
    "mlp_units = [128]\n",
    "dropout_rate = 0.5\n",
    "num_classes = len(df['grade'].unique())\n",
    "\n",
    "transformer_model = build_transformer_model(maxlen, vocab_size, embed_dim, num_heads, density, transformer_units, mlp_units, dropout_rate, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "3/3 [==============================] - 12s 604ms/step - loss: 1.8518 - accuracy: 0.2368 - val_loss: 1.8231 - val_accuracy: 0.2857\n",
      "Epoch 2/30\n",
      "3/3 [==============================] - 0s 80ms/step - loss: 1.6921 - accuracy: 0.3421 - val_loss: 1.6051 - val_accuracy: 0.3571\n",
      "Epoch 3/30\n",
      "3/3 [==============================] - 0s 80ms/step - loss: 1.7026 - accuracy: 0.3553 - val_loss: 1.5282 - val_accuracy: 0.4286\n",
      "Epoch 4/30\n",
      "3/3 [==============================] - 0s 81ms/step - loss: 1.6155 - accuracy: 0.3158 - val_loss: 1.5552 - val_accuracy: 0.3571\n",
      "Epoch 5/30\n",
      "3/3 [==============================] - 0s 78ms/step - loss: 1.5776 - accuracy: 0.3684 - val_loss: 1.5288 - val_accuracy: 0.4286\n",
      "Epoch 6/30\n",
      "3/3 [==============================] - 0s 84ms/step - loss: 1.5036 - accuracy: 0.4868 - val_loss: 1.4804 - val_accuracy: 0.3571\n",
      "Epoch 7/30\n",
      "3/3 [==============================] - 0s 81ms/step - loss: 1.4267 - accuracy: 0.4211 - val_loss: 1.3777 - val_accuracy: 0.4286\n",
      "Epoch 8/30\n",
      "3/3 [==============================] - 0s 80ms/step - loss: 1.2962 - accuracy: 0.5658 - val_loss: 1.2717 - val_accuracy: 0.5000\n",
      "Epoch 9/30\n",
      "3/3 [==============================] - 0s 82ms/step - loss: 1.2620 - accuracy: 0.5789 - val_loss: 1.1092 - val_accuracy: 0.6429\n",
      "Epoch 10/30\n",
      "3/3 [==============================] - 0s 79ms/step - loss: 1.0830 - accuracy: 0.6974 - val_loss: 1.3006 - val_accuracy: 0.6429\n",
      "Epoch 11/30\n",
      "3/3 [==============================] - 0s 84ms/step - loss: 0.9189 - accuracy: 0.8158 - val_loss: 1.3150 - val_accuracy: 0.5000\n",
      "Epoch 12/30\n",
      "3/3 [==============================] - 0s 78ms/step - loss: 0.7807 - accuracy: 0.8289 - val_loss: 1.3189 - val_accuracy: 0.7143\n",
      "Epoch 13/30\n",
      "3/3 [==============================] - 0s 76ms/step - loss: 0.5917 - accuracy: 0.9474 - val_loss: 0.8328 - val_accuracy: 0.8571\n",
      "Epoch 14/30\n",
      "3/3 [==============================] - 0s 81ms/step - loss: 0.5304 - accuracy: 0.9737 - val_loss: 0.6913 - val_accuracy: 0.7857\n",
      "Epoch 15/30\n",
      "3/3 [==============================] - 0s 81ms/step - loss: 0.4761 - accuracy: 0.9868 - val_loss: 1.0610 - val_accuracy: 0.7857\n",
      "Epoch 16/30\n",
      "3/3 [==============================] - 0s 79ms/step - loss: 0.4605 - accuracy: 0.9737 - val_loss: 0.8499 - val_accuracy: 0.8571\n",
      "Epoch 17/30\n",
      "3/3 [==============================] - 0s 105ms/step - loss: 0.3869 - accuracy: 1.0000 - val_loss: 0.7415 - val_accuracy: 0.9286\n",
      "Epoch 18/30\n",
      "3/3 [==============================] - 0s 80ms/step - loss: 0.3937 - accuracy: 0.9868 - val_loss: 0.8654 - val_accuracy: 0.8571\n",
      "Epoch 19/30\n",
      "3/3 [==============================] - 0s 79ms/step - loss: 0.3675 - accuracy: 1.0000 - val_loss: 1.0904 - val_accuracy: 0.8571\n",
      "Epoch 20/30\n",
      "3/3 [==============================] - 0s 91ms/step - loss: 0.3667 - accuracy: 1.0000 - val_loss: 1.2830 - val_accuracy: 0.8571\n",
      "Epoch 21/30\n",
      "3/3 [==============================] - 0s 88ms/step - loss: 0.3570 - accuracy: 1.0000 - val_loss: 1.3293 - val_accuracy: 0.8571\n",
      "Epoch 22/30\n",
      "3/3 [==============================] - 0s 83ms/step - loss: 0.3452 - accuracy: 1.0000 - val_loss: 1.3419 - val_accuracy: 0.8571\n",
      "Epoch 23/30\n",
      "3/3 [==============================] - 0s 80ms/step - loss: 0.3386 - accuracy: 1.0000 - val_loss: 1.3312 - val_accuracy: 0.8571\n",
      "Epoch 24/30\n",
      "3/3 [==============================] - 0s 78ms/step - loss: 0.3334 - accuracy: 1.0000 - val_loss: 1.2792 - val_accuracy: 0.8571\n",
      "Epoch 25/30\n",
      "3/3 [==============================] - 0s 78ms/step - loss: 0.3277 - accuracy: 1.0000 - val_loss: 1.1535 - val_accuracy: 0.8571\n",
      "Epoch 26/30\n",
      "3/3 [==============================] - 0s 77ms/step - loss: 0.3190 - accuracy: 1.0000 - val_loss: 1.0289 - val_accuracy: 0.8571\n",
      "Epoch 27/30\n",
      "3/3 [==============================] - 0s 79ms/step - loss: 0.3129 - accuracy: 1.0000 - val_loss: 0.9073 - val_accuracy: 0.8571\n",
      "Epoch 28/30\n",
      "3/3 [==============================] - 0s 78ms/step - loss: 0.3069 - accuracy: 1.0000 - val_loss: 0.8031 - val_accuracy: 0.8571\n",
      "Epoch 29/30\n",
      "3/3 [==============================] - 0s 75ms/step - loss: 0.3013 - accuracy: 1.0000 - val_loss: 0.7287 - val_accuracy: 0.8571\n",
      "Epoch 30/30\n",
      "3/3 [==============================] - 0s 76ms/step - loss: 0.2964 - accuracy: 1.0000 - val_loss: 0.6748 - val_accuracy: 0.8571\n",
      "Epoch 1/30\n",
      "3/3 [==============================] - 0s 108ms/step - loss: 0.2892 - accuracy: 1.0000 - val_loss: 0.6346 - val_accuracy: 0.9286\n",
      "Epoch 2/30\n",
      "3/3 [==============================] - 0s 77ms/step - loss: 0.2841 - accuracy: 1.0000 - val_loss: 0.6041 - val_accuracy: 0.9286\n",
      "Epoch 3/30\n",
      "3/3 [==============================] - 0s 77ms/step - loss: 0.2779 - accuracy: 1.0000 - val_loss: 0.5814 - val_accuracy: 0.9286\n",
      "Epoch 4/30\n",
      "3/3 [==============================] - 0s 78ms/step - loss: 0.2725 - accuracy: 1.0000 - val_loss: 0.5648 - val_accuracy: 0.9286\n",
      "Epoch 5/30\n",
      "3/3 [==============================] - 0s 75ms/step - loss: 0.2666 - accuracy: 1.0000 - val_loss: 0.5506 - val_accuracy: 0.9286\n",
      "Epoch 6/30\n",
      "3/3 [==============================] - 0s 81ms/step - loss: 0.2622 - accuracy: 1.0000 - val_loss: 0.5343 - val_accuracy: 0.9286\n",
      "Epoch 7/30\n",
      "3/3 [==============================] - 0s 80ms/step - loss: 0.2561 - accuracy: 1.0000 - val_loss: 0.5164 - val_accuracy: 0.9286\n",
      "Epoch 8/30\n",
      "3/3 [==============================] - 0s 75ms/step - loss: 0.2509 - accuracy: 1.0000 - val_loss: 0.5057 - val_accuracy: 0.9286\n",
      "Epoch 9/30\n",
      "3/3 [==============================] - 0s 81ms/step - loss: 0.2461 - accuracy: 1.0000 - val_loss: 0.5247 - val_accuracy: 0.9286\n",
      "Epoch 10/30\n",
      "3/3 [==============================] - 0s 87ms/step - loss: 0.2412 - accuracy: 1.0000 - val_loss: 0.5460 - val_accuracy: 0.8571\n",
      "Epoch 11/30\n",
      "3/3 [==============================] - 0s 84ms/step - loss: 0.2362 - accuracy: 1.0000 - val_loss: 0.5592 - val_accuracy: 0.8571\n",
      "Epoch 12/30\n",
      "3/3 [==============================] - 0s 79ms/step - loss: 0.2308 - accuracy: 1.0000 - val_loss: 0.5787 - val_accuracy: 0.8571\n",
      "Epoch 13/30\n",
      "3/3 [==============================] - 0s 80ms/step - loss: 0.2259 - accuracy: 1.0000 - val_loss: 0.5929 - val_accuracy: 0.8571\n",
      "Epoch 14/30\n",
      "3/3 [==============================] - 0s 79ms/step - loss: 0.2220 - accuracy: 1.0000 - val_loss: 0.5913 - val_accuracy: 0.8571\n",
      "Epoch 15/30\n",
      "3/3 [==============================] - 0s 83ms/step - loss: 0.2164 - accuracy: 1.0000 - val_loss: 0.5807 - val_accuracy: 0.8571\n",
      "Epoch 16/30\n",
      "3/3 [==============================] - 0s 79ms/step - loss: 0.2117 - accuracy: 1.0000 - val_loss: 0.5691 - val_accuracy: 0.8571\n",
      "Epoch 17/30\n",
      "3/3 [==============================] - 0s 89ms/step - loss: 0.2071 - accuracy: 1.0000 - val_loss: 0.5518 - val_accuracy: 0.8571\n",
      "Epoch 18/30\n",
      "3/3 [==============================] - 0s 84ms/step - loss: 0.2027 - accuracy: 1.0000 - val_loss: 0.5348 - val_accuracy: 0.8571\n",
      "Epoch 19/30\n",
      "3/3 [==============================] - 0s 86ms/step - loss: 0.1981 - accuracy: 1.0000 - val_loss: 0.5237 - val_accuracy: 0.8571\n",
      "Epoch 20/30\n",
      "3/3 [==============================] - 0s 76ms/step - loss: 0.1937 - accuracy: 1.0000 - val_loss: 0.5191 - val_accuracy: 0.8571\n",
      "Epoch 21/30\n",
      "3/3 [==============================] - 0s 77ms/step - loss: 0.1895 - accuracy: 1.0000 - val_loss: 0.5136 - val_accuracy: 0.8571\n",
      "Epoch 22/30\n",
      "3/3 [==============================] - 0s 77ms/step - loss: 0.1855 - accuracy: 1.0000 - val_loss: 0.5185 - val_accuracy: 0.8571\n",
      "Epoch 23/30\n",
      "3/3 [==============================] - 0s 76ms/step - loss: 0.1813 - accuracy: 1.0000 - val_loss: 0.5203 - val_accuracy: 0.8571\n",
      "Epoch 24/30\n",
      "3/3 [==============================] - 0s 76ms/step - loss: 0.1779 - accuracy: 1.0000 - val_loss: 0.5237 - val_accuracy: 0.8571\n",
      "Epoch 25/30\n",
      "3/3 [==============================] - 0s 89ms/step - loss: 0.1735 - accuracy: 1.0000 - val_loss: 0.5253 - val_accuracy: 0.8571\n",
      "Epoch 26/30\n",
      "3/3 [==============================] - 0s 74ms/step - loss: 0.1699 - accuracy: 1.0000 - val_loss: 0.5144 - val_accuracy: 0.8571\n",
      "Epoch 27/30\n",
      "3/3 [==============================] - 0s 75ms/step - loss: 0.1665 - accuracy: 1.0000 - val_loss: 0.5040 - val_accuracy: 0.8571\n",
      "Epoch 28/30\n",
      "3/3 [==============================] - 0s 76ms/step - loss: 0.1621 - accuracy: 1.0000 - val_loss: 0.4980 - val_accuracy: 0.8571\n",
      "Epoch 29/30\n",
      "3/3 [==============================] - 0s 78ms/step - loss: 0.1584 - accuracy: 1.0000 - val_loss: 0.4927 - val_accuracy: 0.8571\n",
      "Epoch 30/30\n",
      "3/3 [==============================] - 0s 75ms/step - loss: 0.1550 - accuracy: 1.0000 - val_loss: 0.4849 - val_accuracy: 0.8571\n"
     ]
    }
   ],
   "source": [
    "transformer_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "transformer_model.fit(X_train, Y_train, epochs=30, batch_size=32, validation_data=(X_test, Y_test))\n",
    "history = transformer_model.fit(X_train, Y_train, epochs=30, batch_size=32, validation_data=(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 45ms/step - loss: 0.4849 - accuracy: 0.8571\n",
      "Evaluation Metrics Transformer:\n",
      "loss: 0.4848890006542206\n",
      "accuracy: 0.8571428656578064\n"
     ]
    }
   ],
   "source": [
    "evaluation_metrics_transformer = transformer_model.evaluate(X_test, Y_test)\n",
    "print(\"Evaluation Metrics Transformer:\")\n",
    "for metric_name, metric_value in zip(transformer_model.metrics_names, evaluation_metrics_transformer):\n",
    "    print(f\"{metric_name}: {metric_value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 57ms/step - loss: 0.4849 - accuracy: 0.8571\n",
      "1/1 [==============================] - 1s 877ms/step\n",
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0 160  23   3   1   6   3   7  36   3  61   1 161  15]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0  10 144 145  78  23   3   1   6   3   7]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0  28   8  52  31  30  12   9   2   7  33  45  10 186 187   3 188]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  95\n",
      "   13   3   1   6   2   7   4   8   5  11   2   1   2 202   2 203 204  56]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  19\n",
      "   18   3   1   6   3   7   4   8   5  11   2   1   2 100   1  69 101  46]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0  45  18   3   1   6   7   4   8   5  11   2   1 150  34   1  84]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   9  79   2  14  16   4  12   8]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   9   2   7   4   8   5  11   2   1  18   3   1   6  36  34  12]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   9  44  14]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   9  13   3   1   6   2  14  16   4   8   5  11   2   1]\n",
      " [  0   0   0   0   0   0   0  10 225  25  32   3   1   6  35  54  27  21\n",
      "   17   3   1   5  24   2  66  57   1  10  12   5 226   3   1  10 227  15]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0  28  55  41  15  19  23   3   1   6   3   7]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0  10 189  25  13  36  34  15  35  14  16]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   9  23   2  14  16   4  12   8]]\n",
      "\n",
      "Real and Predicted Values:\n",
      "    Real  Predicted\n",
      "0      0          0\n",
      "1      0          1\n",
      "2      1          1\n",
      "3      2          2\n",
      "4      0          0\n",
      "5      0          0\n",
      "6      1          1\n",
      "7      0          0\n",
      "8      1          1\n",
      "9      2          2\n",
      "10     2          2\n",
      "11     0          0\n",
      "12     2          2\n",
      "13     0          1\n"
     ]
    }
   ],
   "source": [
    "evaluation_metrics_transformer = transformer_model.evaluate(X_test, Y_test)\n",
    "\n",
    "predictions = transformer_model.predict(X_test)\n",
    "print(X_test)\n",
    "\n",
    "# Convert one-hot encoded labels to class indices\n",
    "y_true = np.argmax(Y_test, axis=1)\n",
    "\n",
    "# Convert predicted probabilities to class indices\n",
    "y_pred = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Create DataFrame\n",
    "results_df = pd.DataFrame({\"Real\": y_true, \"Predicted\": y_pred})\n",
    "\n",
    "# Display DataFrame\n",
    "print(\"\\nReal and Predicted Values:\")\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy RNN: 0.6428571343421936\n",
      "Accuracy LSTM: 0.9285714030265808\n",
      "Accuracy Transformer: 0.8571428656578064\n"
     ]
    }
   ],
   "source": [
    "# Evaluate RNN model\n",
    "rnn_accuracy = rnn_model.evaluate(X_test, Y_test, verbose=0)[1]\n",
    "print(\"Accuracy RNN:\", rnn_accuracy)\n",
    "\n",
    "# Evaluate LSTM model\n",
    "lstm_accuracy = lstm_model.evaluate(X_test, Y_test, verbose=0)[1]\n",
    "print(\"Accuracy LSTM:\", lstm_accuracy)\n",
    "\n",
    "# Evaluate Transformer model\n",
    "transformer_accuracy = transformer_model.evaluate(X_test, Y_test, verbose=0)[1]\n",
    "print(\"Accuracy Transformer:\", transformer_accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model (LSTM) with accuracy 0.9285714030265808 has been saved to './savedModels/q2_model.h5'\n"
     ]
    }
   ],
   "source": [
    "# Determine the best model\n",
    "best_model_name, best_model_accuracy = max([('RNN', rnn_accuracy), ('LSTM', lstm_accuracy), ('Transformer', transformer_accuracy)], key=lambda x: x[1])\n",
    "\n",
    "save_path = './savedModels/q2_model.h5'\n",
    "# Save the best model\n",
    "if best_model_name == 'RNN':\n",
    "    rnn_model.save(save_path)\n",
    "elif best_model_name == 'LSTM':\n",
    "    lstm_model.save(save_path)\n",
    "elif best_model_name == 'Transformer':\n",
    "    transformer_model.save(save_path)\n",
    "\n",
    "print(f\"The best model ({best_model_name}) with accuracy {best_model_accuracy} has been saved to '{save_path}'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
