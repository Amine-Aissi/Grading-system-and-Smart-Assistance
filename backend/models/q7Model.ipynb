{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\amine\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow as keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.regularizers import l2\n",
    "from keras import layers\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Embedding, SimpleRNN, Dense, Dropout, LSTM, Bidirectional, Input\n",
    "from keras.layers import Attention, GlobalAveragePooling1D\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import stanza\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import BatchNormalization, Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*<h3>Load Data<h3>*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_id</th>\n",
       "      <th>answer</th>\n",
       "      <th>grade</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>في مكانٍ هادئٍ على جبل النور، حيث يمكن التأمل ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>الوحي نزل في غار حراء.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>في جبل النور.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>غار حراء - جبل النور موقع نزول الوحي.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>نزل الوحي في مكان واحد وهو غار حراء الكائن في ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7</td>\n",
       "      <td>منطقة جبل النور .</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>نزل الوحي على الرسول صلَّى الله عليه وسلَّم في...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>غار حراء - جبل النور هما الموقع حيث نزل الوحي.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7</td>\n",
       "      <td>نزل الوحي في غار حراء - جبل النور بمكة المدينة.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>7</td>\n",
       "      <td>الوحي نزل في غار حراء وجبل النور بمكة المدينة.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   question_id                                             answer  grade\n",
       "0            7  في مكانٍ هادئٍ على جبل النور، حيث يمكن التأمل ...      0\n",
       "1            7                             الوحي نزل في غار حراء.      2\n",
       "2            7                                      في جبل النور.      1\n",
       "3            7              غار حراء - جبل النور موقع نزول الوحي.      2\n",
       "4            7  نزل الوحي في مكان واحد وهو غار حراء الكائن في ...      2\n",
       "5            7                                  منطقة جبل النور .      1\n",
       "6            7  نزل الوحي على الرسول صلَّى الله عليه وسلَّم في...      2\n",
       "7            7     غار حراء - جبل النور هما الموقع حيث نزل الوحي.      2\n",
       "8            7    نزل الوحي في غار حراء - جبل النور بمكة المدينة.      2\n",
       "9            7     الوحي نزل في غار حراء وجبل النور بمكة المدينة.      2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read csv file into pandas\n",
    "df = pd.read_csv(\"../datasets/shuffled7.csv\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*<h3>EDA<h3>*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100 entries, 0 to 99\n",
      "Data columns (total 3 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   question_id  100 non-null    int64 \n",
      " 1   answer       100 non-null    object\n",
      " 2   grade        100 non-null    int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 2.5+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "grade\n",
       "0    35\n",
       "1    29\n",
       "2    36\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('grade').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(data=df, x='grade')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*<h3>Data Cleaning<h3>*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('question_id', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     في مكانٍ هادئٍ على جبل النور، حيث يمكن التأمل ...\n",
      "1                                الوحي نزل في غار حراء.\n",
      "2                                         في جبل النور.\n",
      "3                 غار حراء - جبل النور موقع نزول الوحي.\n",
      "4     نزل الوحي في مكان واحد وهو غار حراء الكائن في ...\n",
      "                            ...                        \n",
      "95     جبل النور هو المكان الذي استقبل فيه النبي الوحي.\n",
      "96                   نزل الوحي في غار حراء - جبل النور.\n",
      "97    في مكان منعزل ومقدس ظهر نور الوحي للنبي محمد ل...\n",
      "98    الوحي نزل في مكان واحد وهو غار حراء في جبل النور.\n",
      "99                  نزل الوحي في غار حراء بمكة الشريفة.\n",
      "Name: answer, Length: 100, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df.drop_duplicates(inplace=True)\n",
    "print(df['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*<h3>Data Pre-Preocessing<h3>*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "174c1038b92545929e1cbb5013eca61c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.7.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-09 22:54:27 INFO: Downloading default packages for language: ar (Arabic) ...\n",
      "2024-01-09 22:54:29 INFO: File exists: C:\\Users\\amine\\stanza_resources\\ar\\default.zip\n",
      "2024-01-09 22:54:34 INFO: Finished downloading models and saved to C:\\Users\\amine\\stanza_resources.\n",
      "2024-01-09 22:54:34 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5f8bbf1a9ed498d88f4df4601a3acf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.7.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-09 22:54:36 INFO: Loading these models for language: ar (Arabic):\n",
      "=============================\n",
      "| Processor | Package       |\n",
      "-----------------------------\n",
      "| tokenize  | padt          |\n",
      "| mwt       | padt          |\n",
      "| pos       | padt_charlm   |\n",
      "| lemma     | padt_nocharlm |\n",
      "| depparse  | padt_charlm   |\n",
      "| ner       | aqmar_charlm  |\n",
      "=============================\n",
      "\n",
      "2024-01-09 22:54:36 INFO: Using device: cpu\n",
      "2024-01-09 22:54:36 INFO: Loading: tokenize\n",
      "2024-01-09 22:54:38 INFO: Loading: mwt\n",
      "2024-01-09 22:54:38 INFO: Loading: pos\n",
      "2024-01-09 22:54:38 INFO: Loading: lemma\n",
      "2024-01-09 22:54:38 INFO: Loading: depparse\n",
      "2024-01-09 22:54:39 INFO: Loading: ner\n",
      "2024-01-09 22:54:40 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "df['grade'] = le.fit_transform(df['grade'])\n",
    "\n",
    "stanza.download('ar')\n",
    "nlp = stanza.Pipeline('ar')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    doc = nlp(text)\n",
    "    tokens = [word.lemma for sent in doc.sentences for word in sent.words if word.upos != 'PUNCT']\n",
    "    return tokens\n",
    "\n",
    "df['answer'] = df['answer'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0 ...  36  80  81]\n",
      " [  0   0   0 ...   1  11   8]\n",
      " [  0   0   0 ...   1   4   3]\n",
      " ...\n",
      " [  0   0   0 ...  10 188   7]\n",
      " [  0   0   0 ...   1   4   3]\n",
      " [  0   0   0 ...  12   9  74]]\n",
      "0     [فِي, مَكَان, هَادِئ, عَلَى, جَبَل, النور, حَي...\n",
      "1                        [وَحي, نَزَل, فِي, غَار, حراء]\n",
      "2                                   [فِي, جَبَل, النور]\n",
      "3       [غار, حراء, جَبَل, النور, مَوقِع, نُزُول, وَحي]\n",
      "4     [نَزَل, وَحي, فِي, مَكَان, وَاحِد, وَ, هُوَ, غ...\n",
      "                            ...                        \n",
      "95    [جَبَل, النور, هُوَ, مَكَان, اَلَّذِي, اِستَقب...\n",
      "96          [نَزَل, وَحي, فِي, غار, حراء, جَبَل, النور]\n",
      "97    [فِي, مَكَان, مُنعَزِل, وَ, مُقَدَّس, ظَهَر, ن...\n",
      "98    [وَحي, نَزَل, فِي, مَكَان, وَاحِد, وَ, هُوَ, غ...\n",
      "99    [نَزَل, وَحي, فِي, غَار, حراء, بِ, مَكَّة, شَر...\n",
      "Name: answer, Length: 100, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Tokenization\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df['answer'])\n",
    "sequences = tokenizer.texts_to_sequences(df['answer'])\n",
    "max_sequence_length = max(len(s) for s in sequences)\n",
    "sequences = pad_sequences(sequences,max_sequence_length)\n",
    "word2idx = tokenizer.word_index\n",
    "vocab_size = len(word2idx) + 1\n",
    "\n",
    "\n",
    "X = pad_sequences(sequences, padding='post', truncating='post', maxlen=max_sequence_length)\n",
    "\n",
    "print(sequences)\n",
    "print(df['answer'])\n",
    "\n",
    "Y = to_categorical(df['grade'], num_classes=3)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.15, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*<h3>build Models<h3>*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*<h4>RNN Model<h4>*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\amine\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:From c:\\Users\\amine\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\amine\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "2/2 [==============================] - 6s 1s/step - loss: 1.6812 - accuracy: 0.3500 - val_loss: 1.3583 - val_accuracy: 0.3500 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 1.2773 - accuracy: 0.3750 - val_loss: 1.2310 - val_accuracy: 0.3500 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 1.1591 - accuracy: 0.4125 - val_loss: 1.2169 - val_accuracy: 0.3000 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 1.2202 - accuracy: 0.4250 - val_loss: 1.2390 - val_accuracy: 0.3000 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 1.1613 - accuracy: 0.4688\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 1.1768 - accuracy: 0.4375 - val_loss: 1.2472 - val_accuracy: 0.3000 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 1.1861 - accuracy: 0.3125 - val_loss: 1.2433 - val_accuracy: 0.3000 - lr: 2.0000e-04\n",
      "Epoch 7/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 1.1408 - accuracy: 0.4844\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0001.\n",
      "2/2 [==============================] - 0s 114ms/step - loss: 1.1728 - accuracy: 0.4750 - val_loss: 1.2370 - val_accuracy: 0.3000 - lr: 2.0000e-04\n",
      "Epoch 8/100\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 1.1181 - accuracy: 0.4375 - val_loss: 1.2328 - val_accuracy: 0.3000 - lr: 1.0000e-04\n",
      "Epoch 9/100\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 1.1675 - accuracy: 0.4875 - val_loss: 1.2285 - val_accuracy: 0.3000 - lr: 1.0000e-04\n",
      "Epoch 10/100\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 1.1843 - accuracy: 0.3750 - val_loss: 1.2243 - val_accuracy: 0.3000 - lr: 1.0000e-04\n",
      "Epoch 11/100\n",
      "2/2 [==============================] - 0s 114ms/step - loss: 1.1497 - accuracy: 0.4125 - val_loss: 1.2202 - val_accuracy: 0.3000 - lr: 1.0000e-04\n",
      "Epoch 12/100\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 1.1312 - accuracy: 0.4500 - val_loss: 1.2163 - val_accuracy: 0.3000 - lr: 1.0000e-04\n",
      "Epoch 13/100\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 1.1493 - accuracy: 0.4875 - val_loss: 1.2130 - val_accuracy: 0.3000 - lr: 1.0000e-04\n",
      "Epoch 14/100\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 1.1133 - accuracy: 0.4375 - val_loss: 1.2102 - val_accuracy: 0.3000 - lr: 1.0000e-04\n",
      "Epoch 15/100\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 1.0811 - accuracy: 0.4750 - val_loss: 1.2073 - val_accuracy: 0.3000 - lr: 1.0000e-04\n",
      "Epoch 16/100\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 1.0829 - accuracy: 0.5500 - val_loss: 1.2050 - val_accuracy: 0.3000 - lr: 1.0000e-04\n",
      "Epoch 17/100\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 1.0665 - accuracy: 0.5000 - val_loss: 1.2033 - val_accuracy: 0.3000 - lr: 1.0000e-04\n",
      "Epoch 18/100\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 1.0572 - accuracy: 0.4875 - val_loss: 1.2024 - val_accuracy: 0.3000 - lr: 1.0000e-04\n",
      "Epoch 19/100\n",
      "2/2 [==============================] - 0s 157ms/step - loss: 1.0796 - accuracy: 0.5625 - val_loss: 1.2017 - val_accuracy: 0.3000 - lr: 1.0000e-04\n",
      "Epoch 20/100\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 1.0294 - accuracy: 0.6000 - val_loss: 1.2009 - val_accuracy: 0.3000 - lr: 1.0000e-04\n",
      "Epoch 21/100\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 1.0550 - accuracy: 0.5375 - val_loss: 1.2001 - val_accuracy: 0.3000 - lr: 1.0000e-04\n",
      "Epoch 22/100\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 1.0610 - accuracy: 0.5625 - val_loss: 1.1996 - val_accuracy: 0.3500 - lr: 1.0000e-04\n",
      "Epoch 23/100\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 1.0341 - accuracy: 0.6125 - val_loss: 1.1995 - val_accuracy: 0.5500 - lr: 1.0000e-04\n",
      "Epoch 24/100\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 1.0082 - accuracy: 0.5750 - val_loss: 1.1995 - val_accuracy: 0.6000 - lr: 1.0000e-04\n",
      "Epoch 25/100\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 1.0108 - accuracy: 0.6750 - val_loss: 1.1996 - val_accuracy: 0.6000 - lr: 1.0000e-04\n",
      "Epoch 26/100\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 1.0492 - accuracy: 0.5875 - val_loss: 1.1995 - val_accuracy: 0.5000 - lr: 1.0000e-04\n",
      "Epoch 27/100\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 1.0268 - accuracy: 0.6125 - val_loss: 1.1993 - val_accuracy: 0.4500 - lr: 1.0000e-04\n",
      "Epoch 28/100\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 1.0005 - accuracy: 0.6625 - val_loss: 1.1992 - val_accuracy: 0.4000 - lr: 1.0000e-04\n",
      "Epoch 29/100\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.9555 - accuracy: 0.7000 - val_loss: 1.1992 - val_accuracy: 0.4500 - lr: 1.0000e-04\n",
      "Epoch 30/100\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 0.9719 - accuracy: 0.7125 - val_loss: 1.1991 - val_accuracy: 0.4500 - lr: 1.0000e-04\n",
      "Epoch 31/100\n",
      "2/2 [==============================] - 0s 312ms/step - loss: 0.9727 - accuracy: 0.6500 - val_loss: 1.1991 - val_accuracy: 0.4500 - lr: 1.0000e-04\n",
      "Epoch 32/100\n",
      "2/2 [==============================] - 0s 271ms/step - loss: 1.0014 - accuracy: 0.6250 - val_loss: 1.1992 - val_accuracy: 0.4500 - lr: 1.0000e-04\n",
      "Epoch 33/100\n",
      "2/2 [==============================] - 0s 176ms/step - loss: 0.9473 - accuracy: 0.6750 - val_loss: 1.1994 - val_accuracy: 0.5000 - lr: 1.0000e-04\n",
      "Epoch 34/100\n",
      "2/2 [==============================] - 0s 210ms/step - loss: 0.9231 - accuracy: 0.7000 - val_loss: 1.1992 - val_accuracy: 0.5000 - lr: 1.0000e-04\n",
      "Epoch 35/100\n",
      "2/2 [==============================] - 0s 109ms/step - loss: 0.9534 - accuracy: 0.7250 - val_loss: 1.1990 - val_accuracy: 0.5000 - lr: 1.0000e-04\n",
      "Epoch 36/100\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.9469 - accuracy: 0.6875 - val_loss: 1.1986 - val_accuracy: 0.6000 - lr: 1.0000e-04\n",
      "Epoch 37/100\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.9262 - accuracy: 0.7125 - val_loss: 1.1977 - val_accuracy: 0.6000 - lr: 1.0000e-04\n",
      "Epoch 38/100\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.9311 - accuracy: 0.6625 - val_loss: 1.1966 - val_accuracy: 0.6000 - lr: 1.0000e-04\n",
      "Epoch 39/100\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.9650 - accuracy: 0.7125 - val_loss: 1.1953 - val_accuracy: 0.6000 - lr: 1.0000e-04\n",
      "Epoch 40/100\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.9123 - accuracy: 0.7250 - val_loss: 1.1939 - val_accuracy: 0.6000 - lr: 1.0000e-04\n",
      "Epoch 41/100\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.9168 - accuracy: 0.7375 - val_loss: 1.1926 - val_accuracy: 0.6000 - lr: 1.0000e-04\n",
      "Epoch 42/100\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.8653 - accuracy: 0.8375 - val_loss: 1.1914 - val_accuracy: 0.6000 - lr: 1.0000e-04\n",
      "Epoch 43/100\n",
      "2/2 [==============================] - 0s 114ms/step - loss: 0.8503 - accuracy: 0.7625 - val_loss: 1.1899 - val_accuracy: 0.6000 - lr: 1.0000e-04\n",
      "Epoch 44/100\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 0.8677 - accuracy: 0.7625 - val_loss: 1.1882 - val_accuracy: 0.6000 - lr: 1.0000e-04\n",
      "Epoch 45/100\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.8350 - accuracy: 0.8000 - val_loss: 1.1866 - val_accuracy: 0.6000 - lr: 1.0000e-04\n",
      "Epoch 46/100\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.8680 - accuracy: 0.7250 - val_loss: 1.1847 - val_accuracy: 0.6000 - lr: 1.0000e-04\n",
      "Epoch 47/100\n",
      "2/2 [==============================] - 0s 112ms/step - loss: 0.9080 - accuracy: 0.6875 - val_loss: 1.1825 - val_accuracy: 0.6000 - lr: 1.0000e-04\n",
      "Epoch 48/100\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.8616 - accuracy: 0.7875 - val_loss: 1.1802 - val_accuracy: 0.6000 - lr: 1.0000e-04\n",
      "Epoch 49/100\n",
      "2/2 [==============================] - 0s 114ms/step - loss: 0.8344 - accuracy: 0.7500 - val_loss: 1.1782 - val_accuracy: 0.6500 - lr: 1.0000e-04\n",
      "Epoch 50/100\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.8387 - accuracy: 0.7625 - val_loss: 1.1765 - val_accuracy: 0.6500 - lr: 1.0000e-04\n",
      "Epoch 51/100\n",
      "2/2 [==============================] - 0s 156ms/step - loss: 0.8170 - accuracy: 0.8125 - val_loss: 1.1747 - val_accuracy: 0.6500 - lr: 1.0000e-04\n",
      "Epoch 52/100\n",
      "2/2 [==============================] - 0s 114ms/step - loss: 0.8194 - accuracy: 0.8000 - val_loss: 1.1727 - val_accuracy: 0.6500 - lr: 1.0000e-04\n",
      "Epoch 53/100\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.7697 - accuracy: 0.8125 - val_loss: 1.1702 - val_accuracy: 0.6500 - lr: 1.0000e-04\n",
      "Epoch 54/100\n",
      "2/2 [==============================] - 0s 107ms/step - loss: 0.8420 - accuracy: 0.7625 - val_loss: 1.1676 - val_accuracy: 0.6000 - lr: 1.0000e-04\n",
      "Epoch 55/100\n",
      "2/2 [==============================] - 0s 117ms/step - loss: 0.7628 - accuracy: 0.8125 - val_loss: 1.1650 - val_accuracy: 0.6000 - lr: 1.0000e-04\n",
      "Epoch 56/100\n",
      "2/2 [==============================] - 0s 318ms/step - loss: 0.7815 - accuracy: 0.7625 - val_loss: 1.1619 - val_accuracy: 0.6000 - lr: 1.0000e-04\n",
      "Epoch 57/100\n",
      "2/2 [==============================] - 0s 186ms/step - loss: 0.7222 - accuracy: 0.8500 - val_loss: 1.1587 - val_accuracy: 0.6000 - lr: 1.0000e-04\n",
      "Epoch 58/100\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 0.7673 - accuracy: 0.7875 - val_loss: 1.1554 - val_accuracy: 0.6000 - lr: 1.0000e-04\n",
      "Epoch 59/100\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.7112 - accuracy: 0.8375 - val_loss: 1.1519 - val_accuracy: 0.6500 - lr: 1.0000e-04\n",
      "Epoch 60/100\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.7457 - accuracy: 0.7875 - val_loss: 1.1478 - val_accuracy: 0.6000 - lr: 1.0000e-04\n",
      "Epoch 61/100\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 0.7137 - accuracy: 0.8500 - val_loss: 1.1439 - val_accuracy: 0.6000 - lr: 1.0000e-04\n",
      "Epoch 62/100\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 0.7180 - accuracy: 0.8625 - val_loss: 1.1406 - val_accuracy: 0.6000 - lr: 1.0000e-04\n",
      "Epoch 63/100\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.6948 - accuracy: 0.8500 - val_loss: 1.1375 - val_accuracy: 0.6000 - lr: 1.0000e-04\n",
      "Epoch 64/100\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.6693 - accuracy: 0.9000 - val_loss: 1.1347 - val_accuracy: 0.6000 - lr: 1.0000e-04\n",
      "Epoch 65/100\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.7167 - accuracy: 0.8375 - val_loss: 1.1313 - val_accuracy: 0.6000 - lr: 1.0000e-04\n",
      "Epoch 66/100\n",
      "2/2 [==============================] - 0s 472ms/step - loss: 0.6823 - accuracy: 0.8500 - val_loss: 1.1277 - val_accuracy: 0.6000 - lr: 1.0000e-04\n",
      "Epoch 67/100\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.6338 - accuracy: 0.9250 - val_loss: 1.1240 - val_accuracy: 0.6000 - lr: 1.0000e-04\n",
      "Epoch 68/100\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 0.6493 - accuracy: 0.9000 - val_loss: 1.1198 - val_accuracy: 0.6000 - lr: 1.0000e-04\n",
      "Epoch 69/100\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.5922 - accuracy: 0.9375 - val_loss: 1.1153 - val_accuracy: 0.6000 - lr: 1.0000e-04\n",
      "Epoch 70/100\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.6413 - accuracy: 0.8875 - val_loss: 1.1108 - val_accuracy: 0.6000 - lr: 1.0000e-04\n",
      "Epoch 71/100\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.6316 - accuracy: 0.9250 - val_loss: 1.1062 - val_accuracy: 0.6000 - lr: 1.0000e-04\n",
      "Epoch 72/100\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.5886 - accuracy: 0.9250 - val_loss: 1.1018 - val_accuracy: 0.6000 - lr: 1.0000e-04\n",
      "Epoch 73/100\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 0.5956 - accuracy: 0.9250 - val_loss: 1.0974 - val_accuracy: 0.6000 - lr: 1.0000e-04\n",
      "Epoch 74/100\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 0.6324 - accuracy: 0.8750 - val_loss: 1.0935 - val_accuracy: 0.6000 - lr: 1.0000e-04\n",
      "Epoch 75/100\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 0.6114 - accuracy: 0.9125 - val_loss: 1.0906 - val_accuracy: 0.6000 - lr: 1.0000e-04\n",
      "Epoch 76/100\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 0.6050 - accuracy: 0.9125 - val_loss: 1.0877 - val_accuracy: 0.6500 - lr: 1.0000e-04\n",
      "Epoch 77/100\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.5803 - accuracy: 0.9250 - val_loss: 1.0840 - val_accuracy: 0.6500 - lr: 1.0000e-04\n",
      "Epoch 78/100\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.5683 - accuracy: 0.9500 - val_loss: 1.0804 - val_accuracy: 0.6000 - lr: 1.0000e-04\n",
      "Epoch 79/100\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.5659 - accuracy: 0.9125 - val_loss: 1.0757 - val_accuracy: 0.6000 - lr: 1.0000e-04\n",
      "Epoch 80/100\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 0.5460 - accuracy: 0.9125 - val_loss: 1.0710 - val_accuracy: 0.6500 - lr: 1.0000e-04\n",
      "Epoch 81/100\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 0.5317 - accuracy: 0.9500 - val_loss: 1.0677 - val_accuracy: 0.6500 - lr: 1.0000e-04\n",
      "Epoch 82/100\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.5405 - accuracy: 0.9625 - val_loss: 1.0641 - val_accuracy: 0.6000 - lr: 1.0000e-04\n",
      "Epoch 83/100\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.5175 - accuracy: 0.9750 - val_loss: 1.0613 - val_accuracy: 0.6000 - lr: 1.0000e-04\n",
      "Epoch 84/100\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.5629 - accuracy: 0.9375 - val_loss: 1.0607 - val_accuracy: 0.6000 - lr: 1.0000e-04\n",
      "Epoch 85/100\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 0.5166 - accuracy: 1.0000 - val_loss: 1.0590 - val_accuracy: 0.6000 - lr: 1.0000e-04\n",
      "Epoch 86/100\n",
      "2/2 [==============================] - 0s 190ms/step - loss: 0.5396 - accuracy: 0.9625 - val_loss: 1.0571 - val_accuracy: 0.6000 - lr: 1.0000e-04\n",
      "Epoch 87/100\n",
      "2/2 [==============================] - 0s 280ms/step - loss: 0.5028 - accuracy: 0.9875 - val_loss: 1.0548 - val_accuracy: 0.6000 - lr: 1.0000e-04\n",
      "Epoch 88/100\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 0.4853 - accuracy: 0.9500 - val_loss: 1.0484 - val_accuracy: 0.6000 - lr: 1.0000e-04\n",
      "Epoch 89/100\n",
      "2/2 [==============================] - 0s 148ms/step - loss: 0.4988 - accuracy: 0.9750 - val_loss: 1.0385 - val_accuracy: 0.6000 - lr: 1.0000e-04\n",
      "Epoch 90/100\n",
      "2/2 [==============================] - 0s 115ms/step - loss: 0.4842 - accuracy: 0.9875 - val_loss: 1.0335 - val_accuracy: 0.6000 - lr: 1.0000e-04\n",
      "Epoch 91/100\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 0.4973 - accuracy: 0.9875 - val_loss: 1.0268 - val_accuracy: 0.6000 - lr: 1.0000e-04\n",
      "Epoch 92/100\n",
      "2/2 [==============================] - 0s 109ms/step - loss: 0.4641 - accuracy: 0.9875 - val_loss: 1.0189 - val_accuracy: 0.6500 - lr: 1.0000e-04\n",
      "Epoch 93/100\n",
      "2/2 [==============================] - 0s 126ms/step - loss: 0.4718 - accuracy: 0.9875 - val_loss: 1.0165 - val_accuracy: 0.6500 - lr: 1.0000e-04\n",
      "Epoch 94/100\n",
      "2/2 [==============================] - 0s 156ms/step - loss: 0.4730 - accuracy: 0.9875 - val_loss: 1.0210 - val_accuracy: 0.6000 - lr: 1.0000e-04\n",
      "Epoch 95/100\n",
      "2/2 [==============================] - 0s 311ms/step - loss: 0.4480 - accuracy: 1.0000 - val_loss: 1.0170 - val_accuracy: 0.6000 - lr: 1.0000e-04\n",
      "Epoch 96/100\n",
      "2/2 [==============================] - 0s 244ms/step - loss: 0.4562 - accuracy: 0.9750 - val_loss: 1.0089 - val_accuracy: 0.7000 - lr: 1.0000e-04\n",
      "Epoch 97/100\n",
      "2/2 [==============================] - 0s 201ms/step - loss: 0.4652 - accuracy: 0.9875 - val_loss: 1.0068 - val_accuracy: 0.6500 - lr: 1.0000e-04\n",
      "Epoch 98/100\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.4670 - accuracy: 0.9500 - val_loss: 1.0089 - val_accuracy: 0.6000 - lr: 1.0000e-04\n",
      "Epoch 99/100\n",
      "2/2 [==============================] - 0s 112ms/step - loss: 0.4240 - accuracy: 1.0000 - val_loss: 1.0101 - val_accuracy: 0.6000 - lr: 1.0000e-04\n",
      "Epoch 100/100\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 0.4646 - accuracy: 0.9750 - val_loss: 0.9997 - val_accuracy: 0.6000 - lr: 1.0000e-04\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.9009 - accuracy: 0.9333\n",
      "Evaluation Metrics for RNN:\n",
      "loss: 0.9009271264076233\n",
      "accuracy: 0.9333333373069763\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "\n",
    "X_train_rnn, X_test_rnn, y_train_rnn, y_test_rnn = train_test_split(X, Y, test_size=0.2)\n",
    "\n",
    "def RNN_model(vocab_size, sequence_length):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=vocab_size, output_dim=128, input_length=sequence_length))\n",
    "    model.add(SimpleRNN(units=128, activation='relu', return_sequences=True))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(SimpleRNN(units=64, activation='sigmoid'))\n",
    "    model.add(Dense(256, activation='sigmoid', kernel_regularizer=l2(0.001)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "    optimizer = Adam(learning_rate=0.001)\n",
    "    \n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=0.0001, verbose=1)\n",
    "#EMBEDDING_DIM = 110\n",
    "rnn_model = RNN_model(vocab_size, max_sequence_length)\n",
    "early_stopping_rnn = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "history_updated_rnn = rnn_model.fit(X_train_rnn, y_train_rnn, validation_data=(X_test_rnn, y_test_rnn), epochs=100, batch_size=64, callbacks=[reduce_lr])\n",
    "\n",
    "# Evaluate the RNN model\n",
    "evaluation_metrics_updated_rnn = rnn_model.evaluate(X_test, Y_test)\n",
    "print(\"Evaluation Metrics for RNN:\")\n",
    "for metric_name, metric_value in zip(rnn_model.metrics_names, evaluation_metrics_updated_rnn):\n",
    "    print(f\"{metric_name}: {metric_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*<h4>LSTM Model<h4>*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2/2 [==============================] - 11s 1000ms/step - loss: 1.1871 - accuracy: 0.4500 - val_loss: 1.1988 - val_accuracy: 0.4500\n",
      "Epoch 2/100\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 1.1375 - accuracy: 0.5250 - val_loss: 1.1966 - val_accuracy: 0.6000\n",
      "Epoch 3/100\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 1.0666 - accuracy: 0.6125 - val_loss: 1.1938 - val_accuracy: 0.5000\n",
      "Epoch 4/100\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 0.9685 - accuracy: 0.6750 - val_loss: 1.1906 - val_accuracy: 0.3500\n",
      "Epoch 5/100\n",
      "2/2 [==============================] - 0s 117ms/step - loss: 0.8568 - accuracy: 0.7625 - val_loss: 1.1872 - val_accuracy: 0.3500\n",
      "Epoch 6/100\n",
      "2/2 [==============================] - 0s 126ms/step - loss: 0.7058 - accuracy: 0.7750 - val_loss: 1.1840 - val_accuracy: 0.3500\n",
      "Epoch 7/100\n",
      "2/2 [==============================] - 0s 122ms/step - loss: 0.5603 - accuracy: 0.8500 - val_loss: 1.1831 - val_accuracy: 0.3500\n",
      "Epoch 8/100\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.3875 - accuracy: 0.9375 - val_loss: 1.1821 - val_accuracy: 0.3500\n",
      "Epoch 9/100\n",
      "2/2 [==============================] - 0s 95ms/step - loss: 0.3213 - accuracy: 0.9375 - val_loss: 1.1699 - val_accuracy: 0.3500\n",
      "Epoch 10/100\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 0.2314 - accuracy: 0.9500 - val_loss: 1.1554 - val_accuracy: 0.4500\n",
      "Epoch 11/100\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 0.2291 - accuracy: 0.9625 - val_loss: 1.1536 - val_accuracy: 0.3500\n",
      "Epoch 12/100\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 0.1524 - accuracy: 0.9750 - val_loss: 1.1571 - val_accuracy: 0.3500\n",
      "Epoch 13/100\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.1787 - accuracy: 0.9500 - val_loss: 1.1538 - val_accuracy: 0.3500\n",
      "Epoch 14/100\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 0.1486 - accuracy: 0.9750 - val_loss: 1.1411 - val_accuracy: 0.4000\n",
      "Epoch 15/100\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 0.1779 - accuracy: 0.9625 - val_loss: 1.1324 - val_accuracy: 0.4500\n",
      "Epoch 16/100\n",
      "2/2 [==============================] - 0s 103ms/step - loss: 0.1809 - accuracy: 0.9750 - val_loss: 1.1275 - val_accuracy: 0.5500\n",
      "Epoch 17/100\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 0.1427 - accuracy: 0.9750 - val_loss: 1.1247 - val_accuracy: 0.5500\n",
      "Epoch 18/100\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.1366 - accuracy: 0.9875 - val_loss: 1.1237 - val_accuracy: 0.5500\n",
      "Epoch 19/100\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 0.1248 - accuracy: 1.0000 - val_loss: 1.1245 - val_accuracy: 0.4500\n",
      "Epoch 20/100\n",
      "2/2 [==============================] - 0s 122ms/step - loss: 0.1161 - accuracy: 0.9875 - val_loss: 1.1251 - val_accuracy: 0.4000\n",
      "Epoch 21/100\n",
      "2/2 [==============================] - 0s 147ms/step - loss: 0.1208 - accuracy: 0.9875 - val_loss: 1.1247 - val_accuracy: 0.4000\n",
      "Epoch 22/100\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 0.2047 - accuracy: 0.9625 - val_loss: 1.1271 - val_accuracy: 0.3500\n",
      "Epoch 23/100\n",
      "2/2 [==============================] - 0s 120ms/step - loss: 0.1345 - accuracy: 0.9875 - val_loss: 1.1231 - val_accuracy: 0.3500\n",
      "Epoch 24/100\n",
      "2/2 [==============================] - 0s 108ms/step - loss: 0.1054 - accuracy: 1.0000 - val_loss: 1.1193 - val_accuracy: 0.3500\n",
      "Epoch 25/100\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 0.1070 - accuracy: 1.0000 - val_loss: 1.1158 - val_accuracy: 0.4000\n",
      "Epoch 26/100\n",
      "2/2 [==============================] - 0s 111ms/step - loss: 0.1015 - accuracy: 1.0000 - val_loss: 1.1127 - val_accuracy: 0.4000\n",
      "Epoch 27/100\n",
      "2/2 [==============================] - 0s 219ms/step - loss: 0.1006 - accuracy: 1.0000 - val_loss: 1.1099 - val_accuracy: 0.4000\n",
      "Epoch 28/100\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.0904 - accuracy: 1.0000 - val_loss: 1.1076 - val_accuracy: 0.4000\n",
      "Epoch 29/100\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.1107 - accuracy: 0.9750 - val_loss: 1.1020 - val_accuracy: 0.4500\n",
      "Epoch 30/100\n",
      "2/2 [==============================] - 0s 109ms/step - loss: 0.0861 - accuracy: 1.0000 - val_loss: 1.0941 - val_accuracy: 0.5500\n",
      "Epoch 31/100\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 0.0881 - accuracy: 1.0000 - val_loss: 1.0879 - val_accuracy: 0.5500\n",
      "Epoch 32/100\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 0.0914 - accuracy: 1.0000 - val_loss: 1.0826 - val_accuracy: 0.6000\n",
      "Epoch 33/100\n",
      "2/2 [==============================] - 0s 106ms/step - loss: 0.0890 - accuracy: 1.0000 - val_loss: 1.0781 - val_accuracy: 0.6000\n",
      "Epoch 34/100\n",
      "2/2 [==============================] - 0s 113ms/step - loss: 0.0864 - accuracy: 1.0000 - val_loss: 1.0745 - val_accuracy: 0.6000\n",
      "Epoch 35/100\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 0.0881 - accuracy: 1.0000 - val_loss: 1.0725 - val_accuracy: 0.6000\n",
      "Epoch 36/100\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 0.0777 - accuracy: 1.0000 - val_loss: 1.0726 - val_accuracy: 0.6000\n",
      "Epoch 37/100\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 0.0777 - accuracy: 1.0000 - val_loss: 1.0729 - val_accuracy: 0.5500\n",
      "Epoch 38/100\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 0.0771 - accuracy: 1.0000 - val_loss: 1.0730 - val_accuracy: 0.5500\n",
      "Epoch 39/100\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.0757 - accuracy: 1.0000 - val_loss: 1.0725 - val_accuracy: 0.5500\n",
      "Epoch 40/100\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.0742 - accuracy: 1.0000 - val_loss: 1.0712 - val_accuracy: 0.5500\n",
      "Epoch 41/100\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.0853 - accuracy: 0.9875 - val_loss: 1.0667 - val_accuracy: 0.5500\n",
      "Epoch 42/100\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 0.0731 - accuracy: 1.0000 - val_loss: 1.0616 - val_accuracy: 0.6000\n",
      "Epoch 43/100\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.0702 - accuracy: 1.0000 - val_loss: 1.0561 - val_accuracy: 0.6000\n",
      "Epoch 44/100\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 0.0699 - accuracy: 1.0000 - val_loss: 1.0517 - val_accuracy: 0.6000\n",
      "Epoch 45/100\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 0.0712 - accuracy: 1.0000 - val_loss: 1.0482 - val_accuracy: 0.6000\n",
      "Epoch 46/100\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 0.0724 - accuracy: 1.0000 - val_loss: 1.0455 - val_accuracy: 0.6000\n",
      "Epoch 47/100\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 0.0702 - accuracy: 1.0000 - val_loss: 1.0432 - val_accuracy: 0.6000\n",
      "Epoch 48/100\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 0.0707 - accuracy: 1.0000 - val_loss: 1.0415 - val_accuracy: 0.6000\n",
      "Epoch 49/100\n",
      "2/2 [==============================] - 0s 95ms/step - loss: 0.0688 - accuracy: 1.0000 - val_loss: 1.0401 - val_accuracy: 0.6000\n",
      "Epoch 50/100\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.0666 - accuracy: 1.0000 - val_loss: 1.0389 - val_accuracy: 0.6000\n",
      "Epoch 51/100\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 0.0662 - accuracy: 1.0000 - val_loss: 1.0377 - val_accuracy: 0.6000\n",
      "Epoch 52/100\n",
      "2/2 [==============================] - 0s 104ms/step - loss: 0.0655 - accuracy: 1.0000 - val_loss: 1.0365 - val_accuracy: 0.6000\n",
      "Epoch 53/100\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 0.0651 - accuracy: 1.0000 - val_loss: 1.0354 - val_accuracy: 0.6000\n",
      "Epoch 54/100\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 0.0649 - accuracy: 1.0000 - val_loss: 1.0342 - val_accuracy: 0.6000\n",
      "Epoch 55/100\n",
      "2/2 [==============================] - 0s 183ms/step - loss: 0.0643 - accuracy: 1.0000 - val_loss: 1.0330 - val_accuracy: 0.6000\n",
      "Epoch 56/100\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 0.0641 - accuracy: 1.0000 - val_loss: 1.0318 - val_accuracy: 0.6000\n",
      "Epoch 57/100\n",
      "2/2 [==============================] - 0s 104ms/step - loss: 0.0634 - accuracy: 1.0000 - val_loss: 1.0308 - val_accuracy: 0.6000\n",
      "Epoch 58/100\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 0.0631 - accuracy: 1.0000 - val_loss: 1.0296 - val_accuracy: 0.6000\n",
      "Epoch 59/100\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.0627 - accuracy: 1.0000 - val_loss: 1.0282 - val_accuracy: 0.6000\n",
      "Epoch 60/100\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 0.0629 - accuracy: 1.0000 - val_loss: 1.0262 - val_accuracy: 0.6000\n",
      "Epoch 61/100\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 0.0621 - accuracy: 1.0000 - val_loss: 1.0241 - val_accuracy: 0.6000\n",
      "Epoch 62/100\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 0.0616 - accuracy: 1.0000 - val_loss: 1.0220 - val_accuracy: 0.6000\n",
      "Epoch 63/100\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.0613 - accuracy: 1.0000 - val_loss: 1.0198 - val_accuracy: 0.6000\n",
      "Epoch 64/100\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 0.0608 - accuracy: 1.0000 - val_loss: 1.0176 - val_accuracy: 0.6000\n",
      "Epoch 65/100\n",
      "2/2 [==============================] - 0s 113ms/step - loss: 0.0605 - accuracy: 1.0000 - val_loss: 1.0155 - val_accuracy: 0.6000\n",
      "Epoch 66/100\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 0.0601 - accuracy: 1.0000 - val_loss: 1.0136 - val_accuracy: 0.6000\n",
      "Epoch 67/100\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 0.0597 - accuracy: 1.0000 - val_loss: 1.0116 - val_accuracy: 0.6000\n",
      "Epoch 68/100\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.0595 - accuracy: 1.0000 - val_loss: 1.0095 - val_accuracy: 0.6000\n",
      "Epoch 69/100\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 0.0590 - accuracy: 1.0000 - val_loss: 1.0074 - val_accuracy: 0.6000\n",
      "Epoch 70/100\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.0587 - accuracy: 1.0000 - val_loss: 1.0052 - val_accuracy: 0.6000\n",
      "Epoch 71/100\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.0583 - accuracy: 1.0000 - val_loss: 1.0030 - val_accuracy: 0.6000\n",
      "Epoch 72/100\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 0.0579 - accuracy: 1.0000 - val_loss: 1.0009 - val_accuracy: 0.6000\n",
      "Epoch 73/100\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.0578 - accuracy: 1.0000 - val_loss: 0.9984 - val_accuracy: 0.6000\n",
      "Epoch 74/100\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.0574 - accuracy: 1.0000 - val_loss: 0.9956 - val_accuracy: 0.6000\n",
      "Epoch 75/100\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 0.0572 - accuracy: 1.0000 - val_loss: 0.9930 - val_accuracy: 0.6000\n",
      "Epoch 76/100\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.0566 - accuracy: 1.0000 - val_loss: 0.9905 - val_accuracy: 0.6000\n",
      "Epoch 77/100\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.0568 - accuracy: 1.0000 - val_loss: 0.9882 - val_accuracy: 0.6000\n",
      "Epoch 78/100\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 0.0560 - accuracy: 1.0000 - val_loss: 0.9860 - val_accuracy: 0.6000\n",
      "Epoch 79/100\n",
      "2/2 [==============================] - 0s 129ms/step - loss: 0.0557 - accuracy: 1.0000 - val_loss: 0.9838 - val_accuracy: 0.6000\n",
      "Epoch 80/100\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 0.0554 - accuracy: 1.0000 - val_loss: 0.9818 - val_accuracy: 0.6000\n",
      "Epoch 81/100\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 0.0551 - accuracy: 1.0000 - val_loss: 0.9798 - val_accuracy: 0.6000\n",
      "Epoch 82/100\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.0549 - accuracy: 1.0000 - val_loss: 0.9779 - val_accuracy: 0.6500\n",
      "Epoch 83/100\n",
      "2/2 [==============================] - 0s 107ms/step - loss: 0.0545 - accuracy: 1.0000 - val_loss: 0.9758 - val_accuracy: 0.6500\n",
      "Epoch 84/100\n",
      "2/2 [==============================] - 0s 95ms/step - loss: 0.0547 - accuracy: 1.0000 - val_loss: 0.9741 - val_accuracy: 0.6500\n",
      "Epoch 85/100\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 0.0542 - accuracy: 1.0000 - val_loss: 0.9724 - val_accuracy: 0.6000\n",
      "Epoch 86/100\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 0.0536 - accuracy: 1.0000 - val_loss: 0.9706 - val_accuracy: 0.6000\n",
      "Epoch 87/100\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 0.0534 - accuracy: 1.0000 - val_loss: 0.9689 - val_accuracy: 0.6000\n",
      "Epoch 88/100\n",
      "2/2 [==============================] - 0s 98ms/step - loss: 0.0531 - accuracy: 1.0000 - val_loss: 0.9672 - val_accuracy: 0.6000\n",
      "Epoch 89/100\n",
      "2/2 [==============================] - 0s 105ms/step - loss: 0.0528 - accuracy: 1.0000 - val_loss: 0.9654 - val_accuracy: 0.6000\n",
      "Epoch 90/100\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.0525 - accuracy: 1.0000 - val_loss: 0.9636 - val_accuracy: 0.6000\n",
      "Epoch 91/100\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 0.0522 - accuracy: 1.0000 - val_loss: 0.9617 - val_accuracy: 0.6000\n",
      "Epoch 92/100\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 0.0520 - accuracy: 1.0000 - val_loss: 0.9598 - val_accuracy: 0.6000\n",
      "Epoch 93/100\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 0.0517 - accuracy: 1.0000 - val_loss: 0.9580 - val_accuracy: 0.6000\n",
      "Epoch 94/100\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 0.0514 - accuracy: 1.0000 - val_loss: 0.9561 - val_accuracy: 0.6000\n",
      "Epoch 95/100\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.0511 - accuracy: 1.0000 - val_loss: 0.9542 - val_accuracy: 0.6000\n",
      "Epoch 96/100\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 0.0509 - accuracy: 1.0000 - val_loss: 0.9522 - val_accuracy: 0.6000\n",
      "Epoch 97/100\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 0.0507 - accuracy: 1.0000 - val_loss: 0.9500 - val_accuracy: 0.6000\n",
      "Epoch 98/100\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.0504 - accuracy: 1.0000 - val_loss: 0.9479 - val_accuracy: 0.6000\n",
      "Epoch 99/100\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 0.0501 - accuracy: 1.0000 - val_loss: 0.9458 - val_accuracy: 0.6000\n",
      "Epoch 100/100\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.0498 - accuracy: 1.0000 - val_loss: 0.9437 - val_accuracy: 0.6000\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.7368 - accuracy: 0.9333\n",
      "Evaluation Metrics for LSTM:\n",
      "loss: 0.7368327975273132\n",
      "accuracy: 0.9333333373069763\n"
     ]
    }
   ],
   "source": [
    "def LSTM_model(vocab_size, sequence_length):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=vocab_size, output_dim=128, input_length=sequence_length))\n",
    "    model.add(LSTM(units=128, activation='relu', return_sequences=True))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LSTM(units=64, activation='relu'))\n",
    "    model.add(Dense(256, activation='relu', kernel_regularizer=l2(0.001)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "    optimizer = Adam(learning_rate=0.001)\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "#EMBEDDING_DIM = 110\n",
    "lstm_model = LSTM_model(vocab_size, max_sequence_length)\n",
    "early_stopping_rnn = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "history_updated_rnn = lstm_model.fit(X_train_rnn, y_train_rnn, validation_data=(X_test_rnn, y_test_rnn), epochs=100, batch_size=64, callbacks=[early_stopping_rnn])\n",
    "\n",
    "\n",
    "# Evaluate the lstm model\n",
    "evaluation_metrics_updated_lstm = lstm_model.evaluate(X_test, Y_test)\n",
    "print(\"Evaluation Metrics for LSTM:\")\n",
    "for metric_name, metric_value in zip(lstm_model.metrics_names, evaluation_metrics_updated_lstm):\n",
    "    print(f\"{metric_name}: {metric_value}\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*<h4>TRANSFORMER Model<h4>*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, density, rate=0.1, l2_reg=0.01):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            layers.Dense(density, activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(l2_reg)),\n",
    "            layers.Dense(embed_dim, kernel_regularizer=tf.keras.regularizers.l2(l2_reg)),\n",
    "        ])\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "\n",
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions\n",
    "\n",
    "def build_transformer_model(maxlen, vocab_size, embed_dim, num_heads, density, transformer_units, mlp_units, dropout_rate, num_classes):\n",
    "    inputs = layers.Input(shape=(maxlen,))\n",
    "    embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
    "    x = embedding_layer(inputs)\n",
    "    for _ in range(transformer_units):\n",
    "        x = TransformerBlock(embed_dim, num_heads, density, rate=dropout_rate)(x)\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    for dim in mlp_units:\n",
    "        x = layers.Dense(dim, activation=\"relu\")(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "    outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = max_sequence_length\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "embed_dim = 120\n",
    "num_heads = 2\n",
    "density = 3\n",
    "transformer_units = 4\n",
    "mlp_units = [128]\n",
    "dropout_rate = 0.5\n",
    "num_classes = len(df['grade'].unique())\n",
    "\n",
    "transformer_model = build_transformer_model(maxlen, vocab_size, embed_dim, num_heads, density, transformer_units, mlp_units, dropout_rate, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\amine\\anaconda3\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Epoch 1/30\n",
      "3/3 [==============================] - 20s 1s/step - loss: 1.7473 - accuracy: 0.3882 - val_loss: 1.8682 - val_accuracy: 0.2667\n",
      "Epoch 2/30\n",
      "3/3 [==============================] - 0s 83ms/step - loss: 1.9142 - accuracy: 0.3529 - val_loss: 1.8021 - val_accuracy: 0.2000\n",
      "Epoch 3/30\n",
      "3/3 [==============================] - 0s 99ms/step - loss: 1.6629 - accuracy: 0.4471 - val_loss: 1.4072 - val_accuracy: 0.6667\n",
      "Epoch 4/30\n",
      "3/3 [==============================] - 0s 84ms/step - loss: 1.5285 - accuracy: 0.4706 - val_loss: 1.3127 - val_accuracy: 0.8000\n",
      "Epoch 5/30\n",
      "3/3 [==============================] - 0s 88ms/step - loss: 1.4059 - accuracy: 0.5412 - val_loss: 1.2533 - val_accuracy: 0.4000\n",
      "Epoch 6/30\n",
      "3/3 [==============================] - 0s 88ms/step - loss: 1.1766 - accuracy: 0.6471 - val_loss: 1.1673 - val_accuracy: 0.4000\n",
      "Epoch 7/30\n",
      "3/3 [==============================] - 0s 98ms/step - loss: 1.0414 - accuracy: 0.6235 - val_loss: 1.0548 - val_accuracy: 0.5333\n",
      "Epoch 8/30\n",
      "3/3 [==============================] - 0s 78ms/step - loss: 0.8568 - accuracy: 0.7647 - val_loss: 0.8559 - val_accuracy: 0.8000\n",
      "Epoch 9/30\n",
      "3/3 [==============================] - 0s 94ms/step - loss: 0.9227 - accuracy: 0.7176 - val_loss: 0.8438 - val_accuracy: 0.8667\n",
      "Epoch 10/30\n",
      "3/3 [==============================] - 0s 81ms/step - loss: 0.8559 - accuracy: 0.7176 - val_loss: 1.0062 - val_accuracy: 0.6000\n",
      "Epoch 11/30\n",
      "3/3 [==============================] - 0s 89ms/step - loss: 0.8457 - accuracy: 0.7412 - val_loss: 0.9819 - val_accuracy: 0.6000\n",
      "Epoch 12/30\n",
      "3/3 [==============================] - 0s 91ms/step - loss: 0.8443 - accuracy: 0.7529 - val_loss: 0.7135 - val_accuracy: 0.8667\n",
      "Epoch 13/30\n",
      "3/3 [==============================] - 0s 87ms/step - loss: 0.7121 - accuracy: 0.8235 - val_loss: 1.0285 - val_accuracy: 0.6667\n",
      "Epoch 14/30\n",
      "3/3 [==============================] - 0s 91ms/step - loss: 0.6216 - accuracy: 0.8941 - val_loss: 0.9786 - val_accuracy: 0.7333\n",
      "Epoch 15/30\n",
      "3/3 [==============================] - 0s 95ms/step - loss: 0.6137 - accuracy: 0.9176 - val_loss: 0.8097 - val_accuracy: 0.8000\n",
      "Epoch 16/30\n",
      "3/3 [==============================] - 0s 99ms/step - loss: 0.4980 - accuracy: 0.9529 - val_loss: 1.0183 - val_accuracy: 0.6667\n",
      "Epoch 17/30\n",
      "3/3 [==============================] - 0s 92ms/step - loss: 0.4756 - accuracy: 0.9529 - val_loss: 0.9767 - val_accuracy: 0.6667\n",
      "Epoch 18/30\n",
      "3/3 [==============================] - 0s 101ms/step - loss: 0.4010 - accuracy: 0.9882 - val_loss: 1.6770 - val_accuracy: 0.6667\n",
      "Epoch 19/30\n",
      "3/3 [==============================] - 0s 92ms/step - loss: 0.3768 - accuracy: 1.0000 - val_loss: 1.4078 - val_accuracy: 0.6000\n",
      "Epoch 20/30\n",
      "3/3 [==============================] - 0s 88ms/step - loss: 0.3621 - accuracy: 1.0000 - val_loss: 1.0650 - val_accuracy: 0.8000\n",
      "Epoch 21/30\n",
      "3/3 [==============================] - 0s 89ms/step - loss: 0.3570 - accuracy: 1.0000 - val_loss: 0.9653 - val_accuracy: 0.8000\n",
      "Epoch 22/30\n",
      "3/3 [==============================] - 0s 94ms/step - loss: 0.3995 - accuracy: 0.9882 - val_loss: 2.6422 - val_accuracy: 0.6667\n",
      "Epoch 23/30\n",
      "3/3 [==============================] - 0s 91ms/step - loss: 0.3639 - accuracy: 1.0000 - val_loss: 2.3198 - val_accuracy: 0.6667\n",
      "Epoch 24/30\n",
      "3/3 [==============================] - 0s 83ms/step - loss: 0.3310 - accuracy: 1.0000 - val_loss: 1.2832 - val_accuracy: 0.7333\n",
      "Epoch 25/30\n",
      "3/3 [==============================] - 0s 77ms/step - loss: 0.3437 - accuracy: 0.9882 - val_loss: 2.3236 - val_accuracy: 0.6000\n",
      "Epoch 26/30\n",
      "3/3 [==============================] - 0s 108ms/step - loss: 0.3462 - accuracy: 0.9882 - val_loss: 3.1621 - val_accuracy: 0.6000\n",
      "Epoch 27/30\n",
      "3/3 [==============================] - 0s 89ms/step - loss: 0.3065 - accuracy: 1.0000 - val_loss: 2.7218 - val_accuracy: 0.6000\n",
      "Epoch 28/30\n",
      "3/3 [==============================] - 0s 82ms/step - loss: 0.2994 - accuracy: 1.0000 - val_loss: 2.2306 - val_accuracy: 0.6667\n",
      "Epoch 29/30\n",
      "3/3 [==============================] - 0s 87ms/step - loss: 0.2938 - accuracy: 1.0000 - val_loss: 1.9571 - val_accuracy: 0.6667\n",
      "Epoch 30/30\n",
      "3/3 [==============================] - 0s 96ms/step - loss: 0.2877 - accuracy: 1.0000 - val_loss: 1.7778 - val_accuracy: 0.6667\n",
      "Epoch 1/30\n",
      "3/3 [==============================] - 0s 135ms/step - loss: 0.2896 - accuracy: 1.0000 - val_loss: 1.9822 - val_accuracy: 0.7333\n",
      "Epoch 2/30\n",
      "3/3 [==============================] - 0s 137ms/step - loss: 0.3097 - accuracy: 0.9765 - val_loss: 3.3152 - val_accuracy: 0.6667\n",
      "Epoch 3/30\n",
      "3/3 [==============================] - 0s 87ms/step - loss: 0.2863 - accuracy: 1.0000 - val_loss: 1.4814 - val_accuracy: 0.7333\n",
      "Epoch 4/30\n",
      "3/3 [==============================] - 0s 88ms/step - loss: 0.2659 - accuracy: 1.0000 - val_loss: 0.9679 - val_accuracy: 0.8667\n",
      "Epoch 5/30\n",
      "3/3 [==============================] - 0s 102ms/step - loss: 0.2945 - accuracy: 0.9882 - val_loss: 2.2497 - val_accuracy: 0.6667\n",
      "Epoch 6/30\n",
      "3/3 [==============================] - 0s 92ms/step - loss: 0.2545 - accuracy: 1.0000 - val_loss: 3.2771 - val_accuracy: 0.6667\n",
      "Epoch 7/30\n",
      "3/3 [==============================] - 0s 103ms/step - loss: 0.2620 - accuracy: 0.9882 - val_loss: 3.4938 - val_accuracy: 0.6667\n",
      "Epoch 8/30\n",
      "3/3 [==============================] - 0s 148ms/step - loss: 0.3264 - accuracy: 0.9882 - val_loss: 3.1429 - val_accuracy: 0.6667\n",
      "Epoch 9/30\n",
      "3/3 [==============================] - 0s 89ms/step - loss: 0.2439 - accuracy: 1.0000 - val_loss: 2.5431 - val_accuracy: 0.6667\n",
      "Epoch 10/30\n",
      "3/3 [==============================] - 0s 80ms/step - loss: 0.2348 - accuracy: 1.0000 - val_loss: 1.8185 - val_accuracy: 0.7333\n",
      "Epoch 11/30\n",
      "3/3 [==============================] - 0s 104ms/step - loss: 0.2380 - accuracy: 1.0000 - val_loss: 1.5175 - val_accuracy: 0.7333\n",
      "Epoch 12/30\n",
      "3/3 [==============================] - 0s 91ms/step - loss: 0.2264 - accuracy: 1.0000 - val_loss: 1.4069 - val_accuracy: 0.7333\n",
      "Epoch 13/30\n",
      "3/3 [==============================] - 0s 85ms/step - loss: 0.2213 - accuracy: 1.0000 - val_loss: 1.4347 - val_accuracy: 0.6667\n",
      "Epoch 14/30\n",
      "3/3 [==============================] - 0s 92ms/step - loss: 0.2161 - accuracy: 1.0000 - val_loss: 1.5064 - val_accuracy: 0.6667\n",
      "Epoch 15/30\n",
      "3/3 [==============================] - 0s 89ms/step - loss: 0.2129 - accuracy: 1.0000 - val_loss: 1.6709 - val_accuracy: 0.6667\n",
      "Epoch 16/30\n",
      "3/3 [==============================] - 0s 100ms/step - loss: 0.2108 - accuracy: 1.0000 - val_loss: 1.8815 - val_accuracy: 0.6667\n",
      "Epoch 17/30\n",
      "3/3 [==============================] - 0s 85ms/step - loss: 0.2081 - accuracy: 1.0000 - val_loss: 1.2419 - val_accuracy: 0.6667\n",
      "Epoch 18/30\n",
      "3/3 [==============================] - 0s 79ms/step - loss: 0.1993 - accuracy: 1.0000 - val_loss: 0.8048 - val_accuracy: 0.7333\n",
      "Epoch 19/30\n",
      "3/3 [==============================] - 0s 84ms/step - loss: 0.1943 - accuracy: 1.0000 - val_loss: 0.6204 - val_accuracy: 0.8667\n",
      "Epoch 20/30\n",
      "3/3 [==============================] - 0s 86ms/step - loss: 0.1920 - accuracy: 1.0000 - val_loss: 0.7649 - val_accuracy: 0.8000\n",
      "Epoch 21/30\n",
      "3/3 [==============================] - 0s 86ms/step - loss: 0.1862 - accuracy: 1.0000 - val_loss: 0.9603 - val_accuracy: 0.8000\n",
      "Epoch 22/30\n",
      "3/3 [==============================] - 0s 77ms/step - loss: 0.1828 - accuracy: 1.0000 - val_loss: 1.3645 - val_accuracy: 0.6667\n",
      "Epoch 23/30\n",
      "3/3 [==============================] - 0s 79ms/step - loss: 0.1783 - accuracy: 1.0000 - val_loss: 1.7945 - val_accuracy: 0.6667\n",
      "Epoch 24/30\n",
      "3/3 [==============================] - 0s 75ms/step - loss: 0.1740 - accuracy: 1.0000 - val_loss: 2.1120 - val_accuracy: 0.6667\n",
      "Epoch 25/30\n",
      "3/3 [==============================] - 0s 92ms/step - loss: 0.1716 - accuracy: 1.0000 - val_loss: 2.3321 - val_accuracy: 0.6667\n",
      "Epoch 26/30\n",
      "3/3 [==============================] - 0s 117ms/step - loss: 0.1673 - accuracy: 1.0000 - val_loss: 2.4829 - val_accuracy: 0.6667\n",
      "Epoch 27/30\n",
      "3/3 [==============================] - 0s 113ms/step - loss: 0.1636 - accuracy: 1.0000 - val_loss: 2.5884 - val_accuracy: 0.6667\n",
      "Epoch 28/30\n",
      "3/3 [==============================] - 0s 87ms/step - loss: 0.1596 - accuracy: 1.0000 - val_loss: 2.5959 - val_accuracy: 0.6667\n",
      "Epoch 29/30\n",
      "3/3 [==============================] - 0s 87ms/step - loss: 0.1568 - accuracy: 1.0000 - val_loss: 2.6078 - val_accuracy: 0.6667\n",
      "Epoch 30/30\n",
      "3/3 [==============================] - 0s 75ms/step - loss: 0.1526 - accuracy: 1.0000 - val_loss: 2.6292 - val_accuracy: 0.6667\n"
     ]
    }
   ],
   "source": [
    "transformer_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "transformer_model.fit(X_train, Y_train, epochs=30, batch_size=32, validation_data=(X_test, Y_test))\n",
    "history = transformer_model.fit(X_train, Y_train, epochs=30, batch_size=32, validation_data=(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 223ms/step - loss: 2.6292 - accuracy: 0.6667\n",
      "Evaluation Metrics Transformer:\n",
      "loss: 2.6291909217834473\n",
      "accuracy: 0.6666666865348816\n"
     ]
    }
   ],
   "source": [
    "evaluation_metrics_transformer = transformer_model.evaluate(X_test, Y_test)\n",
    "print(\"Evaluation Metrics Transformer:\")\n",
    "for metric_name, metric_value in zip(transformer_model.metrics_names, evaluation_metrics_transformer):\n",
    "    print(f\"{metric_name}: {metric_value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 109ms/step - loss: 2.6292 - accuracy: 0.6667\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   5   2   1  11   8\n",
      "   12   9  21]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   4\n",
      "    3  12   9]\n",
      " [  0   0   0   0   0   0   0   0   1 160   7 161  42   6  13  15  56  34\n",
      "   12  26   2]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   5   2   1  11\n",
      "    8  12   9]\n",
      " [  0   0   0   0   1 126  30   7  32  61  14  17  14   6   7  10  20  38\n",
      "   39   2  62]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   2   5   1  11   8\n",
      "   12   4   3]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 100   1  40\n",
      "   41   9  22]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   5   2   1  11   8\n",
      "   10   4   3]\n",
      " [  0   0   0   0   0  87  88  47   6  19  14  17  14   6   7  10  20   1\n",
      "   89  30   7]\n",
      " [  0   0   0   0   0   0   0   1  16  76   6   4   3  25  77  78  10  79\n",
      "   36  80  81]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1  50\n",
      "   95  12   9]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1  50\n",
      "  111 112 113]\n",
      " [  0   0   0   0   0   0   0   0   0  71  36  72  21  10 168 169   5   2\n",
      "    6  13  15]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   2   5   1  29   4\n",
      "    3  12   9]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   4   3   7  16  27  44\n",
      "    2   1   7]]\n",
      "\n",
      "Real and Predicted Values:\n",
      "    Real  Predicted\n",
      "0      2          2\n",
      "1      1          1\n",
      "2      0          1\n",
      "3      2          2\n",
      "4      0          0\n",
      "5      2          2\n",
      "6      0          0\n",
      "7      2          2\n",
      "8      0          0\n",
      "9      0          1\n",
      "10     0          1\n",
      "11     0          1\n",
      "12     0          1\n",
      "13     1          1\n",
      "14     1          1\n"
     ]
    }
   ],
   "source": [
    "evaluation_metrics_transformer = transformer_model.evaluate(X_test, Y_test)\n",
    "\n",
    "predictions = transformer_model.predict(X_test)\n",
    "print(X_test)\n",
    "\n",
    "# Convert one-hot encoded labels to class indices\n",
    "y_true = np.argmax(Y_test, axis=1)\n",
    "\n",
    "# Convert predicted probabilities to class indices\n",
    "y_pred = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Create DataFrame\n",
    "results_df = pd.DataFrame({\"Real\": y_true, \"Predicted\": y_pred})\n",
    "\n",
    "# Display DataFrame\n",
    "print(\"\\nReal and Predicted Values:\")\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy RNN: 0.9333333373069763\n",
      "Accuracy LSTM: 0.9333333373069763\n",
      "Accuracy Transformer: 0.6666666865348816\n"
     ]
    }
   ],
   "source": [
    "# Evaluate RNN model\n",
    "rnn_accuracy = rnn_model.evaluate(X_test, Y_test, verbose=0)[1]\n",
    "print(\"Accuracy RNN:\", rnn_accuracy)\n",
    "\n",
    "# Evaluate LSTM model\n",
    "lstm_accuracy = lstm_model.evaluate(X_test, Y_test, verbose=0)[1]\n",
    "print(\"Accuracy LSTM:\", lstm_accuracy)\n",
    "\n",
    "# Evaluate Transformer model\n",
    "transformer_accuracy = transformer_model.evaluate(X_test, Y_test, verbose=0)[1]\n",
    "print(\"Accuracy Transformer:\", transformer_accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model (RNN) with accuracy 0.9333333373069763 has been saved to './savedModels/q7_model.h5'\n"
     ]
    }
   ],
   "source": [
    "# Determine the best model\n",
    "best_model_name, best_model_accuracy = max([('RNN', rnn_accuracy), ('LSTM', lstm_accuracy), ('Transformer', transformer_accuracy)], key=lambda x: x[1])\n",
    "\n",
    "save_path = './savedModels/q7_model.h5'\n",
    "# Save the best model\n",
    "if best_model_name == 'RNN':\n",
    "    rnn_model.save(save_path)\n",
    "elif best_model_name == 'LSTM':\n",
    "    lstm_model.save(save_path)\n",
    "elif best_model_name == 'Transformer':\n",
    "    transformer_model.save(save_path)\n",
    "\n",
    "print(f\"The best model ({best_model_name}) with accuracy {best_model_accuracy} has been saved to '{save_path}'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
